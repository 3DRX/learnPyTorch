{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBDR70oSFwSp"
   },
   "source": [
    "# batch_size 对训练效果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0evLB0i_C-1H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MBC3q6zDKt_",
    "outputId": "84ad3bf6-a540-460e-d651-862d46eafc6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AQy6kjGDVkj"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vXAxqWi_DYBh"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLUVMhxPDeTC"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uTEQZeigDhei"
   },
   "outputs": [],
   "source": [
    "def data_loader(batch_size: int):\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    testing_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=batch_size)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Phd2iPkDj1j"
   },
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JoKh46fgDmBU"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            pass\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            pass\n",
    "        pass\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbUvl6OnDqe-"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J_l9GWVZmWYp"
   },
   "outputs": [],
   "source": [
    "epoches = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = MyModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqyr8O4UOs7O"
   },
   "source": [
    "### batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9_4z6eCOsUJ",
    "outputId": "3a94b47b-5fa8-42f1-e83c-427efb902d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 2.333284  [    5/60000]\n",
      "loss: 2.317782  [  505/60000]\n",
      "loss: 2.260652  [ 1005/60000]\n",
      "loss: 2.200168  [ 1505/60000]\n",
      "loss: 2.239794  [ 2005/60000]\n",
      "loss: 2.229897  [ 2505/60000]\n",
      "loss: 2.226673  [ 3005/60000]\n",
      "loss: 2.200083  [ 3505/60000]\n",
      "loss: 2.153225  [ 4005/60000]\n",
      "loss: 2.234951  [ 4505/60000]\n",
      "loss: 2.106346  [ 5005/60000]\n",
      "loss: 2.186150  [ 5505/60000]\n",
      "loss: 2.119397  [ 6005/60000]\n",
      "loss: 2.053933  [ 6505/60000]\n",
      "loss: 2.050398  [ 7005/60000]\n",
      "loss: 1.959623  [ 7505/60000]\n",
      "loss: 2.030324  [ 8005/60000]\n",
      "loss: 1.923928  [ 8505/60000]\n",
      "loss: 1.957605  [ 9005/60000]\n",
      "loss: 1.875095  [ 9505/60000]\n",
      "loss: 1.912050  [10005/60000]\n",
      "loss: 1.636605  [10505/60000]\n",
      "loss: 1.814382  [11005/60000]\n",
      "loss: 1.758196  [11505/60000]\n",
      "loss: 1.630694  [12005/60000]\n",
      "loss: 1.657760  [12505/60000]\n",
      "loss: 1.727346  [13005/60000]\n",
      "loss: 1.878179  [13505/60000]\n",
      "loss: 1.384589  [14005/60000]\n",
      "loss: 1.647344  [14505/60000]\n",
      "loss: 1.210850  [15005/60000]\n",
      "loss: 1.146078  [15505/60000]\n",
      "loss: 1.695258  [16005/60000]\n",
      "loss: 0.904212  [16505/60000]\n",
      "loss: 1.169519  [17005/60000]\n",
      "loss: 1.403392  [17505/60000]\n",
      "loss: 1.494034  [18005/60000]\n",
      "loss: 1.022180  [18505/60000]\n",
      "loss: 1.136001  [19005/60000]\n",
      "loss: 1.258381  [19505/60000]\n",
      "loss: 1.392555  [20005/60000]\n",
      "loss: 1.076198  [20505/60000]\n",
      "loss: 0.806431  [21005/60000]\n",
      "loss: 1.146093  [21505/60000]\n",
      "loss: 1.130584  [22005/60000]\n",
      "loss: 1.033176  [22505/60000]\n",
      "loss: 1.209815  [23005/60000]\n",
      "loss: 1.011826  [23505/60000]\n",
      "loss: 1.182313  [24005/60000]\n",
      "loss: 0.683040  [24505/60000]\n",
      "loss: 0.991913  [25005/60000]\n",
      "loss: 1.283368  [25505/60000]\n",
      "loss: 1.257040  [26005/60000]\n",
      "loss: 1.287655  [26505/60000]\n",
      "loss: 0.573776  [27005/60000]\n",
      "loss: 1.282339  [27505/60000]\n",
      "loss: 1.013498  [28005/60000]\n",
      "loss: 1.181808  [28505/60000]\n",
      "loss: 0.747714  [29005/60000]\n",
      "loss: 1.545628  [29505/60000]\n",
      "loss: 0.951962  [30005/60000]\n",
      "loss: 0.670463  [30505/60000]\n",
      "loss: 0.686047  [31005/60000]\n",
      "loss: 0.643559  [31505/60000]\n",
      "loss: 0.991170  [32005/60000]\n",
      "loss: 0.625340  [32505/60000]\n",
      "loss: 0.384479  [33005/60000]\n",
      "loss: 0.800876  [33505/60000]\n",
      "loss: 0.723318  [34005/60000]\n",
      "loss: 0.309931  [34505/60000]\n",
      "loss: 0.772973  [35005/60000]\n",
      "loss: 0.589625  [35505/60000]\n",
      "loss: 0.590727  [36005/60000]\n",
      "loss: 1.019533  [36505/60000]\n",
      "loss: 0.536399  [37005/60000]\n",
      "loss: 1.061494  [37505/60000]\n",
      "loss: 1.056763  [38005/60000]\n",
      "loss: 0.419473  [38505/60000]\n",
      "loss: 1.069681  [39005/60000]\n",
      "loss: 0.946611  [39505/60000]\n",
      "loss: 0.532994  [40005/60000]\n",
      "loss: 0.793624  [40505/60000]\n",
      "loss: 0.603112  [41005/60000]\n",
      "loss: 0.653939  [41505/60000]\n",
      "loss: 0.481627  [42005/60000]\n",
      "loss: 1.464521  [42505/60000]\n",
      "loss: 0.949092  [43005/60000]\n",
      "loss: 0.630210  [43505/60000]\n",
      "loss: 0.494672  [44005/60000]\n",
      "loss: 0.461726  [44505/60000]\n",
      "loss: 0.846040  [45005/60000]\n",
      "loss: 0.484982  [45505/60000]\n",
      "loss: 0.871446  [46005/60000]\n",
      "loss: 0.341552  [46505/60000]\n",
      "loss: 0.724242  [47005/60000]\n",
      "loss: 0.660169  [47505/60000]\n",
      "loss: 0.572828  [48005/60000]\n",
      "loss: 0.791187  [48505/60000]\n",
      "loss: 0.671153  [49005/60000]\n",
      "loss: 0.608644  [49505/60000]\n",
      "loss: 0.410778  [50005/60000]\n",
      "loss: 0.467728  [50505/60000]\n",
      "loss: 0.872720  [51005/60000]\n",
      "loss: 0.327184  [51505/60000]\n",
      "loss: 0.371180  [52005/60000]\n",
      "loss: 1.034116  [52505/60000]\n",
      "loss: 1.249833  [53005/60000]\n",
      "loss: 0.663366  [53505/60000]\n",
      "loss: 0.754667  [54005/60000]\n",
      "loss: 0.531201  [54505/60000]\n",
      "loss: 1.098550  [55005/60000]\n",
      "loss: 0.526139  [55505/60000]\n",
      "loss: 0.792815  [56005/60000]\n",
      "loss: 0.939848  [56505/60000]\n",
      "loss: 1.546594  [57005/60000]\n",
      "loss: 0.775837  [57505/60000]\n",
      "loss: 0.244768  [58005/60000]\n",
      "loss: 0.412685  [58505/60000]\n",
      "loss: 0.679417  [59005/60000]\n",
      "loss: 0.802445  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.724279 \n",
      "\n",
      "time: 26.323035717010498\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 1.520212  [    5/60000]\n",
      "loss: 1.183082  [  505/60000]\n",
      "loss: 0.639052  [ 1005/60000]\n",
      "loss: 0.339662  [ 1505/60000]\n",
      "loss: 0.381134  [ 2005/60000]\n",
      "loss: 0.682103  [ 2505/60000]\n",
      "loss: 1.263323  [ 3005/60000]\n",
      "loss: 1.077390  [ 3505/60000]\n",
      "loss: 0.260820  [ 4005/60000]\n",
      "loss: 1.439808  [ 4505/60000]\n",
      "loss: 0.417480  [ 5005/60000]\n",
      "loss: 1.354672  [ 5505/60000]\n",
      "loss: 1.192693  [ 6005/60000]\n",
      "loss: 0.750880  [ 6505/60000]\n",
      "loss: 0.582427  [ 7005/60000]\n",
      "loss: 0.248970  [ 7505/60000]\n",
      "loss: 0.615904  [ 8005/60000]\n",
      "loss: 0.100922  [ 8505/60000]\n",
      "loss: 0.592803  [ 9005/60000]\n",
      "loss: 1.237916  [ 9505/60000]\n",
      "loss: 0.482411  [10005/60000]\n",
      "loss: 0.078696  [10505/60000]\n",
      "loss: 0.648312  [11005/60000]\n",
      "loss: 0.598113  [11505/60000]\n",
      "loss: 0.773263  [12005/60000]\n",
      "loss: 0.719137  [12505/60000]\n",
      "loss: 0.690042  [13005/60000]\n",
      "loss: 0.781665  [13505/60000]\n",
      "loss: 0.221622  [14005/60000]\n",
      "loss: 0.674809  [14505/60000]\n",
      "loss: 0.522601  [15005/60000]\n",
      "loss: 0.479541  [15505/60000]\n",
      "loss: 1.473249  [16005/60000]\n",
      "loss: 0.144398  [16505/60000]\n",
      "loss: 0.324741  [17005/60000]\n",
      "loss: 0.631885  [17505/60000]\n",
      "loss: 1.286709  [18005/60000]\n",
      "loss: 0.338311  [18505/60000]\n",
      "loss: 0.439937  [19005/60000]\n",
      "loss: 0.885802  [19505/60000]\n",
      "loss: 0.554389  [20005/60000]\n",
      "loss: 0.443730  [20505/60000]\n",
      "loss: 0.252267  [21005/60000]\n",
      "loss: 0.317196  [21505/60000]\n",
      "loss: 0.735801  [22005/60000]\n",
      "loss: 0.849278  [22505/60000]\n",
      "loss: 0.647603  [23005/60000]\n",
      "loss: 0.527445  [23505/60000]\n",
      "loss: 0.756029  [24005/60000]\n",
      "loss: 0.226264  [24505/60000]\n",
      "loss: 0.672218  [25005/60000]\n",
      "loss: 1.264662  [25505/60000]\n",
      "loss: 0.780758  [26005/60000]\n",
      "loss: 0.513896  [26505/60000]\n",
      "loss: 0.175298  [27005/60000]\n",
      "loss: 0.784634  [27505/60000]\n",
      "loss: 0.895994  [28005/60000]\n",
      "loss: 0.719108  [28505/60000]\n",
      "loss: 0.488290  [29005/60000]\n",
      "loss: 1.864770  [29505/60000]\n",
      "loss: 0.974742  [30005/60000]\n",
      "loss: 0.166740  [30505/60000]\n",
      "loss: 0.311516  [31005/60000]\n",
      "loss: 0.472045  [31505/60000]\n",
      "loss: 1.092448  [32005/60000]\n",
      "loss: 0.369121  [32505/60000]\n",
      "loss: 0.107987  [33005/60000]\n",
      "loss: 0.809700  [33505/60000]\n",
      "loss: 0.300000  [34005/60000]\n",
      "loss: 0.143421  [34505/60000]\n",
      "loss: 0.354119  [35005/60000]\n",
      "loss: 0.437549  [35505/60000]\n",
      "loss: 0.349059  [36005/60000]\n",
      "loss: 1.009696  [36505/60000]\n",
      "loss: 0.252326  [37005/60000]\n",
      "loss: 0.933551  [37505/60000]\n",
      "loss: 0.757096  [38005/60000]\n",
      "loss: 0.092105  [38505/60000]\n",
      "loss: 0.963511  [39005/60000]\n",
      "loss: 0.709692  [39505/60000]\n",
      "loss: 0.616889  [40005/60000]\n",
      "loss: 0.488258  [40505/60000]\n",
      "loss: 0.228432  [41005/60000]\n",
      "loss: 0.384694  [41505/60000]\n",
      "loss: 0.162785  [42005/60000]\n",
      "loss: 1.610992  [42505/60000]\n",
      "loss: 0.624172  [43005/60000]\n",
      "loss: 0.360575  [43505/60000]\n",
      "loss: 0.311771  [44005/60000]\n",
      "loss: 0.420268  [44505/60000]\n",
      "loss: 0.694340  [45005/60000]\n",
      "loss: 0.396438  [45505/60000]\n",
      "loss: 0.768538  [46005/60000]\n",
      "loss: 0.185313  [46505/60000]\n",
      "loss: 0.525580  [47005/60000]\n",
      "loss: 0.363138  [47505/60000]\n",
      "loss: 0.237555  [48005/60000]\n",
      "loss: 0.451851  [48505/60000]\n",
      "loss: 0.606870  [49005/60000]\n",
      "loss: 0.489618  [49505/60000]\n",
      "loss: 0.277087  [50005/60000]\n",
      "loss: 0.413004  [50505/60000]\n",
      "loss: 0.722286  [51005/60000]\n",
      "loss: 0.151881  [51505/60000]\n",
      "loss: 0.224000  [52005/60000]\n",
      "loss: 1.205014  [52505/60000]\n",
      "loss: 1.007766  [53005/60000]\n",
      "loss: 0.731926  [53505/60000]\n",
      "loss: 0.674815  [54005/60000]\n",
      "loss: 0.184855  [54505/60000]\n",
      "loss: 0.914044  [55005/60000]\n",
      "loss: 0.181024  [55505/60000]\n",
      "loss: 0.648821  [56005/60000]\n",
      "loss: 0.975548  [56505/60000]\n",
      "loss: 1.370823  [57005/60000]\n",
      "loss: 0.530452  [57505/60000]\n",
      "loss: 0.233200  [58005/60000]\n",
      "loss: 0.198452  [58505/60000]\n",
      "loss: 0.463306  [59005/60000]\n",
      "loss: 0.665758  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.584438 \n",
      "\n",
      "time: 22.433573246002197\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 1.475554  [    5/60000]\n",
      "loss: 1.306827  [  505/60000]\n",
      "loss: 0.392558  [ 1005/60000]\n",
      "loss: 0.234529  [ 1505/60000]\n",
      "loss: 0.242374  [ 2005/60000]\n",
      "loss: 0.651417  [ 2505/60000]\n",
      "loss: 0.937476  [ 3005/60000]\n",
      "loss: 0.953311  [ 3505/60000]\n",
      "loss: 0.130578  [ 4005/60000]\n",
      "loss: 1.365203  [ 4505/60000]\n",
      "loss: 0.287247  [ 5005/60000]\n",
      "loss: 1.504959  [ 5505/60000]\n",
      "loss: 1.072186  [ 6005/60000]\n",
      "loss: 0.697328  [ 6505/60000]\n",
      "loss: 0.606167  [ 7005/60000]\n",
      "loss: 0.151592  [ 7505/60000]\n",
      "loss: 0.492251  [ 8005/60000]\n",
      "loss: 0.054617  [ 8505/60000]\n",
      "loss: 0.345925  [ 9005/60000]\n",
      "loss: 1.081061  [ 9505/60000]\n",
      "loss: 0.405823  [10005/60000]\n",
      "loss: 0.050652  [10505/60000]\n",
      "loss: 0.550341  [11005/60000]\n",
      "loss: 0.386734  [11505/60000]\n",
      "loss: 0.714914  [12005/60000]\n",
      "loss: 0.668319  [12505/60000]\n",
      "loss: 0.676872  [13005/60000]\n",
      "loss: 0.609976  [13505/60000]\n",
      "loss: 0.095036  [14005/60000]\n",
      "loss: 0.502015  [14505/60000]\n",
      "loss: 0.346768  [15005/60000]\n",
      "loss: 0.326246  [15505/60000]\n",
      "loss: 1.805164  [16005/60000]\n",
      "loss: 0.099895  [16505/60000]\n",
      "loss: 0.225205  [17005/60000]\n",
      "loss: 0.456361  [17505/60000]\n",
      "loss: 1.204417  [18005/60000]\n",
      "loss: 0.184852  [18505/60000]\n",
      "loss: 0.441328  [19005/60000]\n",
      "loss: 0.792045  [19505/60000]\n",
      "loss: 0.237848  [20005/60000]\n",
      "loss: 0.324114  [20505/60000]\n",
      "loss: 0.167896  [21005/60000]\n",
      "loss: 0.150823  [21505/60000]\n",
      "loss: 0.627060  [22005/60000]\n",
      "loss: 1.024479  [22505/60000]\n",
      "loss: 0.474962  [23005/60000]\n",
      "loss: 0.376752  [23505/60000]\n",
      "loss: 0.623611  [24005/60000]\n",
      "loss: 0.174431  [24505/60000]\n",
      "loss: 0.661982  [25005/60000]\n",
      "loss: 1.282417  [25505/60000]\n",
      "loss: 0.676280  [26005/60000]\n",
      "loss: 0.332843  [26505/60000]\n",
      "loss: 0.127199  [27005/60000]\n",
      "loss: 0.705731  [27505/60000]\n",
      "loss: 0.911482  [28005/60000]\n",
      "loss: 0.546481  [28505/60000]\n",
      "loss: 0.495230  [29005/60000]\n",
      "loss: 2.031514  [29505/60000]\n",
      "loss: 0.926977  [30005/60000]\n",
      "loss: 0.098931  [30505/60000]\n",
      "loss: 0.226413  [31005/60000]\n",
      "loss: 0.444098  [31505/60000]\n",
      "loss: 0.910036  [32005/60000]\n",
      "loss: 0.414622  [32505/60000]\n",
      "loss: 0.057678  [33005/60000]\n",
      "loss: 0.901728  [33505/60000]\n",
      "loss: 0.166868  [34005/60000]\n",
      "loss: 0.129343  [34505/60000]\n",
      "loss: 0.297646  [35005/60000]\n",
      "loss: 0.379196  [35505/60000]\n",
      "loss: 0.307173  [36005/60000]\n",
      "loss: 0.900590  [36505/60000]\n",
      "loss: 0.178913  [37005/60000]\n",
      "loss: 0.910949  [37505/60000]\n",
      "loss: 0.641686  [38005/60000]\n",
      "loss: 0.035477  [38505/60000]\n",
      "loss: 0.749959  [39005/60000]\n",
      "loss: 0.451903  [39505/60000]\n",
      "loss: 0.774509  [40005/60000]\n",
      "loss: 0.338934  [40505/60000]\n",
      "loss: 0.141736  [41005/60000]\n",
      "loss: 0.322853  [41505/60000]\n",
      "loss: 0.102494  [42005/60000]\n",
      "loss: 1.564363  [42505/60000]\n",
      "loss: 0.545341  [43005/60000]\n",
      "loss: 0.216473  [43505/60000]\n",
      "loss: 0.332223  [44005/60000]\n",
      "loss: 0.399412  [44505/60000]\n",
      "loss: 0.707170  [45005/60000]\n",
      "loss: 0.385503  [45505/60000]\n",
      "loss: 0.682195  [46005/60000]\n",
      "loss: 0.118845  [46505/60000]\n",
      "loss: 0.412198  [47005/60000]\n",
      "loss: 0.247904  [47505/60000]\n",
      "loss: 0.143810  [48005/60000]\n",
      "loss: 0.324831  [48505/60000]\n",
      "loss: 0.576993  [49005/60000]\n",
      "loss: 0.492501  [49505/60000]\n",
      "loss: 0.240269  [50005/60000]\n",
      "loss: 0.356638  [50505/60000]\n",
      "loss: 0.697612  [51005/60000]\n",
      "loss: 0.105289  [51505/60000]\n",
      "loss: 0.177841  [52005/60000]\n",
      "loss: 1.276832  [52505/60000]\n",
      "loss: 0.909865  [53005/60000]\n",
      "loss: 0.715317  [53505/60000]\n",
      "loss: 0.612684  [54005/60000]\n",
      "loss: 0.093052  [54505/60000]\n",
      "loss: 0.888069  [55005/60000]\n",
      "loss: 0.078312  [55505/60000]\n",
      "loss: 0.534728  [56005/60000]\n",
      "loss: 1.003819  [56505/60000]\n",
      "loss: 1.249545  [57005/60000]\n",
      "loss: 0.442669  [57505/60000]\n",
      "loss: 0.220928  [58005/60000]\n",
      "loss: 0.140556  [58505/60000]\n",
      "loss: 0.357315  [59005/60000]\n",
      "loss: 0.593425  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.527194 \n",
      "\n",
      "time: 22.582427978515625\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 1.337315  [    5/60000]\n",
      "loss: 1.400383  [  505/60000]\n",
      "loss: 0.254765  [ 1005/60000]\n",
      "loss: 0.208352  [ 1505/60000]\n",
      "loss: 0.186976  [ 2005/60000]\n",
      "loss: 0.726503  [ 2505/60000]\n",
      "loss: 0.792592  [ 3005/60000]\n",
      "loss: 0.782342  [ 3505/60000]\n",
      "loss: 0.079772  [ 4005/60000]\n",
      "loss: 1.266448  [ 4505/60000]\n",
      "loss: 0.238864  [ 5005/60000]\n",
      "loss: 1.506183  [ 5505/60000]\n",
      "loss: 0.957809  [ 6005/60000]\n",
      "loss: 0.580737  [ 6505/60000]\n",
      "loss: 0.701822  [ 7005/60000]\n",
      "loss: 0.115325  [ 7505/60000]\n",
      "loss: 0.406371  [ 8005/60000]\n",
      "loss: 0.033874  [ 8505/60000]\n",
      "loss: 0.250861  [ 9005/60000]\n",
      "loss: 0.960525  [ 9505/60000]\n",
      "loss: 0.373387  [10005/60000]\n",
      "loss: 0.044561  [10505/60000]\n",
      "loss: 0.515663  [11005/60000]\n",
      "loss: 0.272813  [11505/60000]\n",
      "loss: 0.711463  [12005/60000]\n",
      "loss: 0.637223  [12505/60000]\n",
      "loss: 0.624001  [13005/60000]\n",
      "loss: 0.539196  [13505/60000]\n",
      "loss: 0.065244  [14005/60000]\n",
      "loss: 0.468782  [14505/60000]\n",
      "loss: 0.237200  [15005/60000]\n",
      "loss: 0.270967  [15505/60000]\n",
      "loss: 1.970389  [16005/60000]\n",
      "loss: 0.087561  [16505/60000]\n",
      "loss: 0.179249  [17005/60000]\n",
      "loss: 0.393403  [17505/60000]\n",
      "loss: 1.092578  [18005/60000]\n",
      "loss: 0.146306  [18505/60000]\n",
      "loss: 0.464930  [19005/60000]\n",
      "loss: 0.710662  [19505/60000]\n",
      "loss: 0.136487  [20005/60000]\n",
      "loss: 0.281593  [20505/60000]\n",
      "loss: 0.107633  [21005/60000]\n",
      "loss: 0.108661  [21505/60000]\n",
      "loss: 0.615287  [22005/60000]\n",
      "loss: 1.196926  [22505/60000]\n",
      "loss: 0.427171  [23005/60000]\n",
      "loss: 0.327409  [23505/60000]\n",
      "loss: 0.487018  [24005/60000]\n",
      "loss: 0.142255  [24505/60000]\n",
      "loss: 0.623779  [25005/60000]\n",
      "loss: 1.267980  [25505/60000]\n",
      "loss: 0.586419  [26005/60000]\n",
      "loss: 0.262613  [26505/60000]\n",
      "loss: 0.093349  [27005/60000]\n",
      "loss: 0.652145  [27505/60000]\n",
      "loss: 0.934235  [28005/60000]\n",
      "loss: 0.477122  [28505/60000]\n",
      "loss: 0.456700  [29005/60000]\n",
      "loss: 2.142073  [29505/60000]\n",
      "loss: 0.830064  [30005/60000]\n",
      "loss: 0.081421  [30505/60000]\n",
      "loss: 0.168209  [31005/60000]\n",
      "loss: 0.397678  [31505/60000]\n",
      "loss: 0.709915  [32005/60000]\n",
      "loss: 0.382339  [32505/60000]\n",
      "loss: 0.038230  [33005/60000]\n",
      "loss: 0.980514  [33505/60000]\n",
      "loss: 0.117330  [34005/60000]\n",
      "loss: 0.120224  [34505/60000]\n",
      "loss: 0.301601  [35005/60000]\n",
      "loss: 0.358216  [35505/60000]\n",
      "loss: 0.292375  [36005/60000]\n",
      "loss: 0.827543  [36505/60000]\n",
      "loss: 0.135886  [37005/60000]\n",
      "loss: 0.894624  [37505/60000]\n",
      "loss: 0.584165  [38005/60000]\n",
      "loss: 0.026406  [38505/60000]\n",
      "loss: 0.684345  [39005/60000]\n",
      "loss: 0.344696  [39505/60000]\n",
      "loss: 0.854772  [40005/60000]\n",
      "loss: 0.257041  [40505/60000]\n",
      "loss: 0.115841  [41005/60000]\n",
      "loss: 0.291053  [41505/60000]\n",
      "loss: 0.080749  [42005/60000]\n",
      "loss: 1.483410  [42505/60000]\n",
      "loss: 0.468487  [43005/60000]\n",
      "loss: 0.150215  [43505/60000]\n",
      "loss: 0.367450  [44005/60000]\n",
      "loss: 0.386577  [44505/60000]\n",
      "loss: 0.726262  [45005/60000]\n",
      "loss: 0.382200  [45505/60000]\n",
      "loss: 0.626207  [46005/60000]\n",
      "loss: 0.090297  [46505/60000]\n",
      "loss: 0.334444  [47005/60000]\n",
      "loss: 0.253292  [47505/60000]\n",
      "loss: 0.111161  [48005/60000]\n",
      "loss: 0.255233  [48505/60000]\n",
      "loss: 0.542831  [49005/60000]\n",
      "loss: 0.480147  [49505/60000]\n",
      "loss: 0.239818  [50005/60000]\n",
      "loss: 0.309718  [50505/60000]\n",
      "loss: 0.678739  [51005/60000]\n",
      "loss: 0.086948  [51505/60000]\n",
      "loss: 0.152977  [52005/60000]\n",
      "loss: 1.319859  [52505/60000]\n",
      "loss: 0.901834  [53005/60000]\n",
      "loss: 0.691328  [53505/60000]\n",
      "loss: 0.545561  [54005/60000]\n",
      "loss: 0.062296  [54505/60000]\n",
      "loss: 0.870004  [55005/60000]\n",
      "loss: 0.050522  [55505/60000]\n",
      "loss: 0.466688  [56005/60000]\n",
      "loss: 0.975358  [56505/60000]\n",
      "loss: 1.130710  [57005/60000]\n",
      "loss: 0.397635  [57505/60000]\n",
      "loss: 0.180929  [58005/60000]\n",
      "loss: 0.117386  [58505/60000]\n",
      "loss: 0.304475  [59005/60000]\n",
      "loss: 0.550437  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.497295 \n",
      "\n",
      "time: 22.84367299079895\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 1.220535  [    5/60000]\n",
      "loss: 1.458005  [  505/60000]\n",
      "loss: 0.194373  [ 1005/60000]\n",
      "loss: 0.192481  [ 1505/60000]\n",
      "loss: 0.161861  [ 2005/60000]\n",
      "loss: 0.760020  [ 2505/60000]\n",
      "loss: 0.754321  [ 3005/60000]\n",
      "loss: 0.689923  [ 3505/60000]\n",
      "loss: 0.058519  [ 4005/60000]\n",
      "loss: 1.173357  [ 4505/60000]\n",
      "loss: 0.211053  [ 5005/60000]\n",
      "loss: 1.482431  [ 5505/60000]\n",
      "loss: 0.887008  [ 6005/60000]\n",
      "loss: 0.503746  [ 6505/60000]\n",
      "loss: 0.777210  [ 7005/60000]\n",
      "loss: 0.093450  [ 7505/60000]\n",
      "loss: 0.329387  [ 8005/60000]\n",
      "loss: 0.024777  [ 8505/60000]\n",
      "loss: 0.205958  [ 9005/60000]\n",
      "loss: 0.878098  [ 9505/60000]\n",
      "loss: 0.357274  [10005/60000]\n",
      "loss: 0.045431  [10505/60000]\n",
      "loss: 0.476071  [11005/60000]\n",
      "loss: 0.210896  [11505/60000]\n",
      "loss: 0.717630  [12005/60000]\n",
      "loss: 0.651287  [12505/60000]\n",
      "loss: 0.573134  [13005/60000]\n",
      "loss: 0.503177  [13505/60000]\n",
      "loss: 0.055457  [14005/60000]\n",
      "loss: 0.447915  [14505/60000]\n",
      "loss: 0.183435  [15005/60000]\n",
      "loss: 0.251290  [15505/60000]\n",
      "loss: 2.009677  [16005/60000]\n",
      "loss: 0.078597  [16505/60000]\n",
      "loss: 0.139905  [17005/60000]\n",
      "loss: 0.355211  [17505/60000]\n",
      "loss: 1.009143  [18005/60000]\n",
      "loss: 0.135341  [18505/60000]\n",
      "loss: 0.483265  [19005/60000]\n",
      "loss: 0.646488  [19505/60000]\n",
      "loss: 0.096631  [20005/60000]\n",
      "loss: 0.269514  [20505/60000]\n",
      "loss: 0.075438  [21005/60000]\n",
      "loss: 0.092837  [21505/60000]\n",
      "loss: 0.626365  [22005/60000]\n",
      "loss: 1.298340  [22505/60000]\n",
      "loss: 0.388315  [23005/60000]\n",
      "loss: 0.307481  [23505/60000]\n",
      "loss: 0.387912  [24005/60000]\n",
      "loss: 0.121428  [24505/60000]\n",
      "loss: 0.578222  [25005/60000]\n",
      "loss: 1.271231  [25505/60000]\n",
      "loss: 0.526379  [26005/60000]\n",
      "loss: 0.236983  [26505/60000]\n",
      "loss: 0.075465  [27005/60000]\n",
      "loss: 0.632690  [27505/60000]\n",
      "loss: 0.933809  [28005/60000]\n",
      "loss: 0.460031  [28505/60000]\n",
      "loss: 0.402882  [29005/60000]\n",
      "loss: 2.179225  [29505/60000]\n",
      "loss: 0.743190  [30005/60000]\n",
      "loss: 0.076724  [30505/60000]\n",
      "loss: 0.132587  [31005/60000]\n",
      "loss: 0.372149  [31505/60000]\n",
      "loss: 0.612646  [32005/60000]\n",
      "loss: 0.325286  [32505/60000]\n",
      "loss: 0.031152  [33005/60000]\n",
      "loss: 1.022410  [33505/60000]\n",
      "loss: 0.087931  [34005/60000]\n",
      "loss: 0.112933  [34505/60000]\n",
      "loss: 0.305334  [35005/60000]\n",
      "loss: 0.348869  [35505/60000]\n",
      "loss: 0.272936  [36005/60000]\n",
      "loss: 0.766822  [36505/60000]\n",
      "loss: 0.106177  [37005/60000]\n",
      "loss: 0.892810  [37505/60000]\n",
      "loss: 0.556668  [38005/60000]\n",
      "loss: 0.024096  [38505/60000]\n",
      "loss: 0.676597  [39005/60000]\n",
      "loss: 0.278676  [39505/60000]\n",
      "loss: 0.890005  [40005/60000]\n",
      "loss: 0.210279  [40505/60000]\n",
      "loss: 0.102352  [41005/60000]\n",
      "loss: 0.253841  [41505/60000]\n",
      "loss: 0.069425  [42005/60000]\n",
      "loss: 1.371312  [42505/60000]\n",
      "loss: 0.400778  [43005/60000]\n",
      "loss: 0.119837  [43505/60000]\n",
      "loss: 0.391399  [44005/60000]\n",
      "loss: 0.378619  [44505/60000]\n",
      "loss: 0.732163  [45005/60000]\n",
      "loss: 0.370291  [45505/60000]\n",
      "loss: 0.597187  [46005/60000]\n",
      "loss: 0.079014  [46505/60000]\n",
      "loss: 0.280255  [47005/60000]\n",
      "loss: 0.277928  [47505/60000]\n",
      "loss: 0.094457  [48005/60000]\n",
      "loss: 0.212817  [48505/60000]\n",
      "loss: 0.528229  [49005/60000]\n",
      "loss: 0.425380  [49505/60000]\n",
      "loss: 0.240509  [50005/60000]\n",
      "loss: 0.283675  [50505/60000]\n",
      "loss: 0.677420  [51005/60000]\n",
      "loss: 0.084312  [51505/60000]\n",
      "loss: 0.133422  [52005/60000]\n",
      "loss: 1.343224  [52505/60000]\n",
      "loss: 0.895928  [53005/60000]\n",
      "loss: 0.699873  [53505/60000]\n",
      "loss: 0.500326  [54005/60000]\n",
      "loss: 0.051625  [54505/60000]\n",
      "loss: 0.839173  [55005/60000]\n",
      "loss: 0.041954  [55505/60000]\n",
      "loss: 0.423950  [56005/60000]\n",
      "loss: 0.911680  [56505/60000]\n",
      "loss: 1.037228  [57005/60000]\n",
      "loss: 0.362418  [57505/60000]\n",
      "loss: 0.146820  [58005/60000]\n",
      "loss: 0.104409  [58505/60000]\n",
      "loss: 0.279890  [59005/60000]\n",
      "loss: 0.504812  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.478216 \n",
      "\n",
      "time: 22.533527374267578\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 1.139174  [    5/60000]\n",
      "loss: 1.482126  [  505/60000]\n",
      "loss: 0.165198  [ 1005/60000]\n",
      "loss: 0.178222  [ 1505/60000]\n",
      "loss: 0.148712  [ 2005/60000]\n",
      "loss: 0.754185  [ 2505/60000]\n",
      "loss: 0.747902  [ 3005/60000]\n",
      "loss: 0.643332  [ 3505/60000]\n",
      "loss: 0.047836  [ 4005/60000]\n",
      "loss: 1.090390  [ 4505/60000]\n",
      "loss: 0.189954  [ 5005/60000]\n",
      "loss: 1.449980  [ 5505/60000]\n",
      "loss: 0.837881  [ 6005/60000]\n",
      "loss: 0.479701  [ 6505/60000]\n",
      "loss: 0.809267  [ 7005/60000]\n",
      "loss: 0.080331  [ 7505/60000]\n",
      "loss: 0.272803  [ 8005/60000]\n",
      "loss: 0.020442  [ 8505/60000]\n",
      "loss: 0.175976  [ 9005/60000]\n",
      "loss: 0.810211  [ 9505/60000]\n",
      "loss: 0.346915  [10005/60000]\n",
      "loss: 0.049415  [10505/60000]\n",
      "loss: 0.430830  [11005/60000]\n",
      "loss: 0.173320  [11505/60000]\n",
      "loss: 0.715346  [12005/60000]\n",
      "loss: 0.662717  [12505/60000]\n",
      "loss: 0.535235  [13005/60000]\n",
      "loss: 0.475848  [13505/60000]\n",
      "loss: 0.051042  [14005/60000]\n",
      "loss: 0.422556  [14505/60000]\n",
      "loss: 0.155344  [15005/60000]\n",
      "loss: 0.239101  [15505/60000]\n",
      "loss: 2.024331  [16005/60000]\n",
      "loss: 0.071193  [16505/60000]\n",
      "loss: 0.114123  [17005/60000]\n",
      "loss: 0.328261  [17505/60000]\n",
      "loss: 0.961717  [18005/60000]\n",
      "loss: 0.130550  [18505/60000]\n",
      "loss: 0.501945  [19005/60000]\n",
      "loss: 0.586737  [19505/60000]\n",
      "loss: 0.078664  [20005/60000]\n",
      "loss: 0.265076  [20505/60000]\n",
      "loss: 0.057496  [21005/60000]\n",
      "loss: 0.088480  [21505/60000]\n",
      "loss: 0.638172  [22005/60000]\n",
      "loss: 1.350317  [22505/60000]\n",
      "loss: 0.353594  [23005/60000]\n",
      "loss: 0.293237  [23505/60000]\n",
      "loss: 0.332565  [24005/60000]\n",
      "loss: 0.108161  [24505/60000]\n",
      "loss: 0.551951  [25005/60000]\n",
      "loss: 1.287395  [25505/60000]\n",
      "loss: 0.477941  [26005/60000]\n",
      "loss: 0.233942  [26505/60000]\n",
      "loss: 0.067169  [27005/60000]\n",
      "loss: 0.621493  [27505/60000]\n",
      "loss: 0.912512  [28005/60000]\n",
      "loss: 0.461522  [28505/60000]\n",
      "loss: 0.349426  [29005/60000]\n",
      "loss: 2.167256  [29505/60000]\n",
      "loss: 0.669652  [30005/60000]\n",
      "loss: 0.075310  [30505/60000]\n",
      "loss: 0.115832  [31005/60000]\n",
      "loss: 0.356563  [31505/60000]\n",
      "loss: 0.572761  [32005/60000]\n",
      "loss: 0.278382  [32505/60000]\n",
      "loss: 0.028281  [33005/60000]\n",
      "loss: 1.042450  [33505/60000]\n",
      "loss: 0.070752  [34005/60000]\n",
      "loss: 0.107187  [34505/60000]\n",
      "loss: 0.308732  [35005/60000]\n",
      "loss: 0.342287  [35505/60000]\n",
      "loss: 0.252454  [36005/60000]\n",
      "loss: 0.709612  [36505/60000]\n",
      "loss: 0.086249  [37005/60000]\n",
      "loss: 0.892970  [37505/60000]\n",
      "loss: 0.536209  [38005/60000]\n",
      "loss: 0.024182  [38505/60000]\n",
      "loss: 0.677486  [39005/60000]\n",
      "loss: 0.231761  [39505/60000]\n",
      "loss: 0.898019  [40005/60000]\n",
      "loss: 0.181431  [40505/60000]\n",
      "loss: 0.094604  [41005/60000]\n",
      "loss: 0.219447  [41505/60000]\n",
      "loss: 0.064687  [42005/60000]\n",
      "loss: 1.245165  [42505/60000]\n",
      "loss: 0.350654  [43005/60000]\n",
      "loss: 0.103136  [43505/60000]\n",
      "loss: 0.401216  [44005/60000]\n",
      "loss: 0.377973  [44505/60000]\n",
      "loss: 0.721198  [45005/60000]\n",
      "loss: 0.352510  [45505/60000]\n",
      "loss: 0.577162  [46005/60000]\n",
      "loss: 0.075068  [46505/60000]\n",
      "loss: 0.244104  [47005/60000]\n",
      "loss: 0.295125  [47505/60000]\n",
      "loss: 0.089722  [48005/60000]\n",
      "loss: 0.182275  [48505/60000]\n",
      "loss: 0.520800  [49005/60000]\n",
      "loss: 0.358876  [49505/60000]\n",
      "loss: 0.239974  [50005/60000]\n",
      "loss: 0.270235  [50505/60000]\n",
      "loss: 0.681527  [51005/60000]\n",
      "loss: 0.087345  [51505/60000]\n",
      "loss: 0.119373  [52005/60000]\n",
      "loss: 1.349526  [52505/60000]\n",
      "loss: 0.891048  [53005/60000]\n",
      "loss: 0.710804  [53505/60000]\n",
      "loss: 0.470409  [54005/60000]\n",
      "loss: 0.047859  [54505/60000]\n",
      "loss: 0.804204  [55005/60000]\n",
      "loss: 0.039365  [55505/60000]\n",
      "loss: 0.396102  [56005/60000]\n",
      "loss: 0.835599  [56505/60000]\n",
      "loss: 0.961307  [57005/60000]\n",
      "loss: 0.332025  [57505/60000]\n",
      "loss: 0.121661  [58005/60000]\n",
      "loss: 0.094585  [58505/60000]\n",
      "loss: 0.265000  [59005/60000]\n",
      "loss: 0.465959  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.463860 \n",
      "\n",
      "time: 22.350534200668335\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 1.079143  [    5/60000]\n",
      "loss: 1.485734  [  505/60000]\n",
      "loss: 0.143156  [ 1005/60000]\n",
      "loss: 0.163864  [ 1505/60000]\n",
      "loss: 0.140013  [ 2005/60000]\n",
      "loss: 0.741809  [ 2505/60000]\n",
      "loss: 0.745299  [ 3005/60000]\n",
      "loss: 0.608445  [ 3505/60000]\n",
      "loss: 0.043569  [ 4005/60000]\n",
      "loss: 1.022943  [ 4505/60000]\n",
      "loss: 0.173975  [ 5005/60000]\n",
      "loss: 1.434206  [ 5505/60000]\n",
      "loss: 0.801054  [ 6005/60000]\n",
      "loss: 0.485958  [ 6505/60000]\n",
      "loss: 0.813030  [ 7005/60000]\n",
      "loss: 0.071137  [ 7505/60000]\n",
      "loss: 0.234375  [ 8005/60000]\n",
      "loss: 0.018634  [ 8505/60000]\n",
      "loss: 0.152991  [ 9005/60000]\n",
      "loss: 0.741844  [ 9505/60000]\n",
      "loss: 0.338982  [10005/60000]\n",
      "loss: 0.053456  [10505/60000]\n",
      "loss: 0.387654  [11005/60000]\n",
      "loss: 0.149571  [11505/60000]\n",
      "loss: 0.701693  [12005/60000]\n",
      "loss: 0.672321  [12505/60000]\n",
      "loss: 0.508839  [13005/60000]\n",
      "loss: 0.455074  [13505/60000]\n",
      "loss: 0.048475  [14005/60000]\n",
      "loss: 0.395859  [14505/60000]\n",
      "loss: 0.135534  [15005/60000]\n",
      "loss: 0.225754  [15505/60000]\n",
      "loss: 2.038265  [16005/60000]\n",
      "loss: 0.064674  [16505/60000]\n",
      "loss: 0.097219  [17005/60000]\n",
      "loss: 0.307269  [17505/60000]\n",
      "loss: 0.930659  [18005/60000]\n",
      "loss: 0.126433  [18505/60000]\n",
      "loss: 0.516070  [19005/60000]\n",
      "loss: 0.533226  [19505/60000]\n",
      "loss: 0.070237  [20005/60000]\n",
      "loss: 0.263657  [20505/60000]\n",
      "loss: 0.047112  [21005/60000]\n",
      "loss: 0.087962  [21505/60000]\n",
      "loss: 0.636163  [22005/60000]\n",
      "loss: 1.369429  [22505/60000]\n",
      "loss: 0.328970  [23005/60000]\n",
      "loss: 0.277518  [23505/60000]\n",
      "loss: 0.307525  [24005/60000]\n",
      "loss: 0.099131  [24505/60000]\n",
      "loss: 0.540115  [25005/60000]\n",
      "loss: 1.304172  [25505/60000]\n",
      "loss: 0.437763  [26005/60000]\n",
      "loss: 0.240222  [26505/60000]\n",
      "loss: 0.062188  [27005/60000]\n",
      "loss: 0.610727  [27505/60000]\n",
      "loss: 0.885535  [28005/60000]\n",
      "loss: 0.462210  [28505/60000]\n",
      "loss: 0.302921  [29005/60000]\n",
      "loss: 2.128495  [29505/60000]\n",
      "loss: 0.604692  [30005/60000]\n",
      "loss: 0.073009  [30505/60000]\n",
      "loss: 0.109982  [31005/60000]\n",
      "loss: 0.344952  [31505/60000]\n",
      "loss: 0.547713  [32005/60000]\n",
      "loss: 0.244422  [32505/60000]\n",
      "loss: 0.027014  [33005/60000]\n",
      "loss: 1.044521  [33505/60000]\n",
      "loss: 0.058517  [34005/60000]\n",
      "loss: 0.102982  [34505/60000]\n",
      "loss: 0.313117  [35005/60000]\n",
      "loss: 0.331408  [35505/60000]\n",
      "loss: 0.230804  [36005/60000]\n",
      "loss: 0.658634  [36505/60000]\n",
      "loss: 0.072947  [37005/60000]\n",
      "loss: 0.896784  [37505/60000]\n",
      "loss: 0.519590  [38005/60000]\n",
      "loss: 0.024982  [38505/60000]\n",
      "loss: 0.702426  [39005/60000]\n",
      "loss: 0.191878  [39505/60000]\n",
      "loss: 0.888957  [40005/60000]\n",
      "loss: 0.160738  [40505/60000]\n",
      "loss: 0.087253  [41005/60000]\n",
      "loss: 0.188923  [41505/60000]\n",
      "loss: 0.062268  [42005/60000]\n",
      "loss: 1.129892  [42505/60000]\n",
      "loss: 0.309590  [43005/60000]\n",
      "loss: 0.095346  [43505/60000]\n",
      "loss: 0.400088  [44005/60000]\n",
      "loss: 0.373962  [44505/60000]\n",
      "loss: 0.702610  [45005/60000]\n",
      "loss: 0.330291  [45505/60000]\n",
      "loss: 0.560702  [46005/60000]\n",
      "loss: 0.073936  [46505/60000]\n",
      "loss: 0.220116  [47005/60000]\n",
      "loss: 0.308087  [47505/60000]\n",
      "loss: 0.086716  [48005/60000]\n",
      "loss: 0.157272  [48505/60000]\n",
      "loss: 0.513833  [49005/60000]\n",
      "loss: 0.305167  [49505/60000]\n",
      "loss: 0.238542  [50005/60000]\n",
      "loss: 0.255866  [50505/60000]\n",
      "loss: 0.677066  [51005/60000]\n",
      "loss: 0.090363  [51505/60000]\n",
      "loss: 0.109658  [52005/60000]\n",
      "loss: 1.341316  [52505/60000]\n",
      "loss: 0.875888  [53005/60000]\n",
      "loss: 0.708295  [53505/60000]\n",
      "loss: 0.450332  [54005/60000]\n",
      "loss: 0.044340  [54505/60000]\n",
      "loss: 0.775510  [55005/60000]\n",
      "loss: 0.038561  [55505/60000]\n",
      "loss: 0.374329  [56005/60000]\n",
      "loss: 0.755208  [56505/60000]\n",
      "loss: 0.899421  [57005/60000]\n",
      "loss: 0.304060  [57505/60000]\n",
      "loss: 0.103817  [58005/60000]\n",
      "loss: 0.086137  [58505/60000]\n",
      "loss: 0.255998  [59005/60000]\n",
      "loss: 0.432054  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.451814 \n",
      "\n",
      "time: 22.50177788734436\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 1.039086  [    5/60000]\n",
      "loss: 1.458553  [  505/60000]\n",
      "loss: 0.127973  [ 1005/60000]\n",
      "loss: 0.150677  [ 1505/60000]\n",
      "loss: 0.133771  [ 2005/60000]\n",
      "loss: 0.732872  [ 2505/60000]\n",
      "loss: 0.751234  [ 3005/60000]\n",
      "loss: 0.585318  [ 3505/60000]\n",
      "loss: 0.040513  [ 4005/60000]\n",
      "loss: 0.959996  [ 4505/60000]\n",
      "loss: 0.164296  [ 5005/60000]\n",
      "loss: 1.418077  [ 5505/60000]\n",
      "loss: 0.772396  [ 6005/60000]\n",
      "loss: 0.511980  [ 6505/60000]\n",
      "loss: 0.809439  [ 7005/60000]\n",
      "loss: 0.065486  [ 7505/60000]\n",
      "loss: 0.205074  [ 8005/60000]\n",
      "loss: 0.017459  [ 8505/60000]\n",
      "loss: 0.135301  [ 9005/60000]\n",
      "loss: 0.684968  [ 9505/60000]\n",
      "loss: 0.333697  [10005/60000]\n",
      "loss: 0.058332  [10505/60000]\n",
      "loss: 0.352682  [11005/60000]\n",
      "loss: 0.131410  [11505/60000]\n",
      "loss: 0.699161  [12005/60000]\n",
      "loss: 0.675163  [12505/60000]\n",
      "loss: 0.482208  [13005/60000]\n",
      "loss: 0.436299  [13505/60000]\n",
      "loss: 0.046674  [14005/60000]\n",
      "loss: 0.366553  [14505/60000]\n",
      "loss: 0.123045  [15005/60000]\n",
      "loss: 0.213499  [15505/60000]\n",
      "loss: 2.049173  [16005/60000]\n",
      "loss: 0.059565  [16505/60000]\n",
      "loss: 0.084590  [17005/60000]\n",
      "loss: 0.288752  [17505/60000]\n",
      "loss: 0.918943  [18005/60000]\n",
      "loss: 0.123602  [18505/60000]\n",
      "loss: 0.528722  [19005/60000]\n",
      "loss: 0.490481  [19505/60000]\n",
      "loss: 0.066734  [20005/60000]\n",
      "loss: 0.262669  [20505/60000]\n",
      "loss: 0.041081  [21005/60000]\n",
      "loss: 0.093282  [21505/60000]\n",
      "loss: 0.624947  [22005/60000]\n",
      "loss: 1.371557  [22505/60000]\n",
      "loss: 0.315367  [23005/60000]\n",
      "loss: 0.261923  [23505/60000]\n",
      "loss: 0.299654  [24005/60000]\n",
      "loss: 0.091272  [24505/60000]\n",
      "loss: 0.537917  [25005/60000]\n",
      "loss: 1.316199  [25505/60000]\n",
      "loss: 0.401833  [26005/60000]\n",
      "loss: 0.250111  [26505/60000]\n",
      "loss: 0.059026  [27005/60000]\n",
      "loss: 0.599592  [27505/60000]\n",
      "loss: 0.859692  [28005/60000]\n",
      "loss: 0.465411  [28505/60000]\n",
      "loss: 0.270101  [29005/60000]\n",
      "loss: 2.086597  [29505/60000]\n",
      "loss: 0.549809  [30005/60000]\n",
      "loss: 0.071052  [30505/60000]\n",
      "loss: 0.107450  [31005/60000]\n",
      "loss: 0.340390  [31505/60000]\n",
      "loss: 0.528573  [32005/60000]\n",
      "loss: 0.220295  [32505/60000]\n",
      "loss: 0.026133  [33005/60000]\n",
      "loss: 1.038866  [33505/60000]\n",
      "loss: 0.049647  [34005/60000]\n",
      "loss: 0.100499  [34505/60000]\n",
      "loss: 0.316454  [35005/60000]\n",
      "loss: 0.327659  [35505/60000]\n",
      "loss: 0.209830  [36005/60000]\n",
      "loss: 0.621360  [36505/60000]\n",
      "loss: 0.064538  [37005/60000]\n",
      "loss: 0.893751  [37505/60000]\n",
      "loss: 0.506722  [38005/60000]\n",
      "loss: 0.026085  [38505/60000]\n",
      "loss: 0.733562  [39005/60000]\n",
      "loss: 0.160715  [39505/60000]\n",
      "loss: 0.878594  [40005/60000]\n",
      "loss: 0.144956  [40505/60000]\n",
      "loss: 0.081854  [41005/60000]\n",
      "loss: 0.166420  [41505/60000]\n",
      "loss: 0.061928  [42005/60000]\n",
      "loss: 1.026901  [42505/60000]\n",
      "loss: 0.283663  [43005/60000]\n",
      "loss: 0.091003  [43505/60000]\n",
      "loss: 0.396231  [44005/60000]\n",
      "loss: 0.369568  [44505/60000]\n",
      "loss: 0.687136  [45005/60000]\n",
      "loss: 0.311093  [45505/60000]\n",
      "loss: 0.538513  [46005/60000]\n",
      "loss: 0.073031  [46505/60000]\n",
      "loss: 0.204318  [47005/60000]\n",
      "loss: 0.319504  [47505/60000]\n",
      "loss: 0.086435  [48005/60000]\n",
      "loss: 0.139150  [48505/60000]\n",
      "loss: 0.505672  [49005/60000]\n",
      "loss: 0.266287  [49505/60000]\n",
      "loss: 0.233232  [50005/60000]\n",
      "loss: 0.242604  [50505/60000]\n",
      "loss: 0.675608  [51005/60000]\n",
      "loss: 0.092927  [51505/60000]\n",
      "loss: 0.102760  [52005/60000]\n",
      "loss: 1.325003  [52505/60000]\n",
      "loss: 0.861105  [53005/60000]\n",
      "loss: 0.715528  [53505/60000]\n",
      "loss: 0.434621  [54005/60000]\n",
      "loss: 0.041434  [54505/60000]\n",
      "loss: 0.744382  [55005/60000]\n",
      "loss: 0.038540  [55505/60000]\n",
      "loss: 0.356711  [56005/60000]\n",
      "loss: 0.685115  [56505/60000]\n",
      "loss: 0.854643  [57005/60000]\n",
      "loss: 0.280816  [57505/60000]\n",
      "loss: 0.089480  [58005/60000]\n",
      "loss: 0.078864  [58505/60000]\n",
      "loss: 0.246078  [59005/60000]\n",
      "loss: 0.407143  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.441684 \n",
      "\n",
      "time: 22.36214518547058\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 1.005528  [    5/60000]\n",
      "loss: 1.428536  [  505/60000]\n",
      "loss: 0.116269  [ 1005/60000]\n",
      "loss: 0.140903  [ 1505/60000]\n",
      "loss: 0.129856  [ 2005/60000]\n",
      "loss: 0.723406  [ 2505/60000]\n",
      "loss: 0.754998  [ 3005/60000]\n",
      "loss: 0.571742  [ 3505/60000]\n",
      "loss: 0.038951  [ 4005/60000]\n",
      "loss: 0.899290  [ 4505/60000]\n",
      "loss: 0.154911  [ 5005/60000]\n",
      "loss: 1.399462  [ 5505/60000]\n",
      "loss: 0.753828  [ 6005/60000]\n",
      "loss: 0.534501  [ 6505/60000]\n",
      "loss: 0.798756  [ 7005/60000]\n",
      "loss: 0.060797  [ 7505/60000]\n",
      "loss: 0.186490  [ 8005/60000]\n",
      "loss: 0.017368  [ 8505/60000]\n",
      "loss: 0.122601  [ 9005/60000]\n",
      "loss: 0.636390  [ 9505/60000]\n",
      "loss: 0.325923  [10005/60000]\n",
      "loss: 0.062009  [10505/60000]\n",
      "loss: 0.322588  [11005/60000]\n",
      "loss: 0.116635  [11505/60000]\n",
      "loss: 0.690278  [12005/60000]\n",
      "loss: 0.670691  [12505/60000]\n",
      "loss: 0.451174  [13005/60000]\n",
      "loss: 0.420725  [13505/60000]\n",
      "loss: 0.045195  [14005/60000]\n",
      "loss: 0.339284  [14505/60000]\n",
      "loss: 0.115941  [15005/60000]\n",
      "loss: 0.205003  [15505/60000]\n",
      "loss: 2.042623  [16005/60000]\n",
      "loss: 0.055425  [16505/60000]\n",
      "loss: 0.075624  [17005/60000]\n",
      "loss: 0.272610  [17505/60000]\n",
      "loss: 0.910334  [18005/60000]\n",
      "loss: 0.122491  [18505/60000]\n",
      "loss: 0.533805  [19005/60000]\n",
      "loss: 0.455018  [19505/60000]\n",
      "loss: 0.062252  [20005/60000]\n",
      "loss: 0.259769  [20505/60000]\n",
      "loss: 0.036928  [21005/60000]\n",
      "loss: 0.101781  [21505/60000]\n",
      "loss: 0.608205  [22005/60000]\n",
      "loss: 1.355030  [22505/60000]\n",
      "loss: 0.303589  [23005/60000]\n",
      "loss: 0.244891  [23505/60000]\n",
      "loss: 0.298456  [24005/60000]\n",
      "loss: 0.084451  [24505/60000]\n",
      "loss: 0.530019  [25005/60000]\n",
      "loss: 1.325570  [25505/60000]\n",
      "loss: 0.375481  [26005/60000]\n",
      "loss: 0.261964  [26505/60000]\n",
      "loss: 0.057092  [27005/60000]\n",
      "loss: 0.586911  [27505/60000]\n",
      "loss: 0.833128  [28005/60000]\n",
      "loss: 0.465547  [28505/60000]\n",
      "loss: 0.246158  [29005/60000]\n",
      "loss: 2.033726  [29505/60000]\n",
      "loss: 0.496722  [30005/60000]\n",
      "loss: 0.068969  [30505/60000]\n",
      "loss: 0.109748  [31005/60000]\n",
      "loss: 0.335818  [31505/60000]\n",
      "loss: 0.504770  [32005/60000]\n",
      "loss: 0.198690  [32505/60000]\n",
      "loss: 0.025157  [33005/60000]\n",
      "loss: 1.014462  [33505/60000]\n",
      "loss: 0.042856  [34005/60000]\n",
      "loss: 0.096171  [34505/60000]\n",
      "loss: 0.317959  [35005/60000]\n",
      "loss: 0.317126  [35505/60000]\n",
      "loss: 0.194642  [36005/60000]\n",
      "loss: 0.597111  [36505/60000]\n",
      "loss: 0.058232  [37005/60000]\n",
      "loss: 0.885036  [37505/60000]\n",
      "loss: 0.495219  [38005/60000]\n",
      "loss: 0.026997  [38505/60000]\n",
      "loss: 0.760202  [39005/60000]\n",
      "loss: 0.137425  [39505/60000]\n",
      "loss: 0.864009  [40005/60000]\n",
      "loss: 0.130856  [40505/60000]\n",
      "loss: 0.077029  [41005/60000]\n",
      "loss: 0.149799  [41505/60000]\n",
      "loss: 0.063218  [42005/60000]\n",
      "loss: 0.929245  [42505/60000]\n",
      "loss: 0.268814  [43005/60000]\n",
      "loss: 0.089427  [43505/60000]\n",
      "loss: 0.387071  [44005/60000]\n",
      "loss: 0.363737  [44505/60000]\n",
      "loss: 0.673411  [45005/60000]\n",
      "loss: 0.293460  [45505/60000]\n",
      "loss: 0.522439  [46005/60000]\n",
      "loss: 0.072404  [46505/60000]\n",
      "loss: 0.193312  [47005/60000]\n",
      "loss: 0.338928  [47505/60000]\n",
      "loss: 0.087082  [48005/60000]\n",
      "loss: 0.125870  [48505/60000]\n",
      "loss: 0.496781  [49005/60000]\n",
      "loss: 0.234904  [49505/60000]\n",
      "loss: 0.229890  [50005/60000]\n",
      "loss: 0.227029  [50505/60000]\n",
      "loss: 0.666525  [51005/60000]\n",
      "loss: 0.096201  [51505/60000]\n",
      "loss: 0.098097  [52005/60000]\n",
      "loss: 1.305960  [52505/60000]\n",
      "loss: 0.846038  [53005/60000]\n",
      "loss: 0.707594  [53505/60000]\n",
      "loss: 0.422986  [54005/60000]\n",
      "loss: 0.038215  [54505/60000]\n",
      "loss: 0.716920  [55005/60000]\n",
      "loss: 0.039086  [55505/60000]\n",
      "loss: 0.341166  [56005/60000]\n",
      "loss: 0.618947  [56505/60000]\n",
      "loss: 0.816992  [57005/60000]\n",
      "loss: 0.259371  [57505/60000]\n",
      "loss: 0.077171  [58005/60000]\n",
      "loss: 0.072288  [58505/60000]\n",
      "loss: 0.237298  [59005/60000]\n",
      "loss: 0.387875  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.432633 \n",
      "\n",
      "time: 22.36716628074646\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 0.975572  [    5/60000]\n",
      "loss: 1.399698  [  505/60000]\n",
      "loss: 0.106869  [ 1005/60000]\n",
      "loss: 0.132477  [ 1505/60000]\n",
      "loss: 0.124758  [ 2005/60000]\n",
      "loss: 0.723365  [ 2505/60000]\n",
      "loss: 0.759186  [ 3005/60000]\n",
      "loss: 0.558022  [ 3505/60000]\n",
      "loss: 0.039452  [ 4005/60000]\n",
      "loss: 0.830227  [ 4505/60000]\n",
      "loss: 0.150765  [ 5005/60000]\n",
      "loss: 1.381335  [ 5505/60000]\n",
      "loss: 0.735312  [ 6005/60000]\n",
      "loss: 0.557061  [ 6505/60000]\n",
      "loss: 0.781824  [ 7005/60000]\n",
      "loss: 0.057309  [ 7505/60000]\n",
      "loss: 0.172619  [ 8005/60000]\n",
      "loss: 0.017840  [ 8505/60000]\n",
      "loss: 0.112221  [ 9005/60000]\n",
      "loss: 0.590207  [ 9505/60000]\n",
      "loss: 0.320351  [10005/60000]\n",
      "loss: 0.066308  [10505/60000]\n",
      "loss: 0.298210  [11005/60000]\n",
      "loss: 0.105973  [11505/60000]\n",
      "loss: 0.668323  [12005/60000]\n",
      "loss: 0.655732  [12505/60000]\n",
      "loss: 0.427162  [13005/60000]\n",
      "loss: 0.406734  [13505/60000]\n",
      "loss: 0.044169  [14005/60000]\n",
      "loss: 0.316157  [14505/60000]\n",
      "loss: 0.111143  [15005/60000]\n",
      "loss: 0.194249  [15505/60000]\n",
      "loss: 2.049249  [16005/60000]\n",
      "loss: 0.051619  [16505/60000]\n",
      "loss: 0.067847  [17005/60000]\n",
      "loss: 0.259360  [17505/60000]\n",
      "loss: 0.907899  [18005/60000]\n",
      "loss: 0.118300  [18505/60000]\n",
      "loss: 0.538102  [19005/60000]\n",
      "loss: 0.428425  [19505/60000]\n",
      "loss: 0.060524  [20005/60000]\n",
      "loss: 0.257227  [20505/60000]\n",
      "loss: 0.034092  [21005/60000]\n",
      "loss: 0.110029  [21505/60000]\n",
      "loss: 0.588441  [22005/60000]\n",
      "loss: 1.342824  [22505/60000]\n",
      "loss: 0.289815  [23005/60000]\n",
      "loss: 0.228695  [23505/60000]\n",
      "loss: 0.298013  [24005/60000]\n",
      "loss: 0.078510  [24505/60000]\n",
      "loss: 0.524054  [25005/60000]\n",
      "loss: 1.324603  [25505/60000]\n",
      "loss: 0.343829  [26005/60000]\n",
      "loss: 0.272893  [26505/60000]\n",
      "loss: 0.055499  [27005/60000]\n",
      "loss: 0.568241  [27505/60000]\n",
      "loss: 0.804315  [28005/60000]\n",
      "loss: 0.468521  [28505/60000]\n",
      "loss: 0.228713  [29005/60000]\n",
      "loss: 1.978264  [29505/60000]\n",
      "loss: 0.460216  [30005/60000]\n",
      "loss: 0.066346  [30505/60000]\n",
      "loss: 0.110187  [31005/60000]\n",
      "loss: 0.335143  [31505/60000]\n",
      "loss: 0.484372  [32005/60000]\n",
      "loss: 0.182393  [32505/60000]\n",
      "loss: 0.024526  [33005/60000]\n",
      "loss: 0.998861  [33505/60000]\n",
      "loss: 0.037276  [34005/60000]\n",
      "loss: 0.091412  [34505/60000]\n",
      "loss: 0.320845  [35005/60000]\n",
      "loss: 0.308679  [35505/60000]\n",
      "loss: 0.175089  [36005/60000]\n",
      "loss: 0.578158  [36505/60000]\n",
      "loss: 0.054276  [37005/60000]\n",
      "loss: 0.873350  [37505/60000]\n",
      "loss: 0.483036  [38005/60000]\n",
      "loss: 0.028370  [38505/60000]\n",
      "loss: 0.797675  [39005/60000]\n",
      "loss: 0.120363  [39505/60000]\n",
      "loss: 0.854045  [40005/60000]\n",
      "loss: 0.117876  [40505/60000]\n",
      "loss: 0.072315  [41005/60000]\n",
      "loss: 0.134921  [41505/60000]\n",
      "loss: 0.065098  [42005/60000]\n",
      "loss: 0.841409  [42505/60000]\n",
      "loss: 0.253521  [43005/60000]\n",
      "loss: 0.089041  [43505/60000]\n",
      "loss: 0.372940  [44005/60000]\n",
      "loss: 0.355425  [44505/60000]\n",
      "loss: 0.657506  [45005/60000]\n",
      "loss: 0.276823  [45505/60000]\n",
      "loss: 0.505925  [46005/60000]\n",
      "loss: 0.071287  [46505/60000]\n",
      "loss: 0.187033  [47005/60000]\n",
      "loss: 0.350518  [47505/60000]\n",
      "loss: 0.087027  [48005/60000]\n",
      "loss: 0.117380  [48505/60000]\n",
      "loss: 0.488004  [49005/60000]\n",
      "loss: 0.208131  [49505/60000]\n",
      "loss: 0.227646  [50005/60000]\n",
      "loss: 0.209210  [50505/60000]\n",
      "loss: 0.651624  [51005/60000]\n",
      "loss: 0.093185  [51505/60000]\n",
      "loss: 0.095937  [52005/60000]\n",
      "loss: 1.273874  [52505/60000]\n",
      "loss: 0.834902  [53005/60000]\n",
      "loss: 0.689255  [53505/60000]\n",
      "loss: 0.408756  [54005/60000]\n",
      "loss: 0.035072  [54505/60000]\n",
      "loss: 0.694436  [55005/60000]\n",
      "loss: 0.039901  [55505/60000]\n",
      "loss: 0.322655  [56005/60000]\n",
      "loss: 0.567396  [56505/60000]\n",
      "loss: 0.779445  [57005/60000]\n",
      "loss: 0.240573  [57505/60000]\n",
      "loss: 0.066942  [58005/60000]\n",
      "loss: 0.066572  [58505/60000]\n",
      "loss: 0.227624  [59005/60000]\n",
      "loss: 0.370396  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.424578 \n",
      "\n",
      "time: 22.93192410469055\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "loss: 0.949432  [    5/60000]\n",
      "loss: 1.380190  [  505/60000]\n",
      "loss: 0.098233  [ 1005/60000]\n",
      "loss: 0.124874  [ 1505/60000]\n",
      "loss: 0.121091  [ 2005/60000]\n",
      "loss: 0.724744  [ 2505/60000]\n",
      "loss: 0.765262  [ 3005/60000]\n",
      "loss: 0.543270  [ 3505/60000]\n",
      "loss: 0.038801  [ 4005/60000]\n",
      "loss: 0.767739  [ 4505/60000]\n",
      "loss: 0.149365  [ 5005/60000]\n",
      "loss: 1.356346  [ 5505/60000]\n",
      "loss: 0.732577  [ 6005/60000]\n",
      "loss: 0.586406  [ 6505/60000]\n",
      "loss: 0.761193  [ 7005/60000]\n",
      "loss: 0.054023  [ 7505/60000]\n",
      "loss: 0.159564  [ 8005/60000]\n",
      "loss: 0.018680  [ 8505/60000]\n",
      "loss: 0.102705  [ 9005/60000]\n",
      "loss: 0.551180  [ 9505/60000]\n",
      "loss: 0.312589  [10005/60000]\n",
      "loss: 0.068710  [10505/60000]\n",
      "loss: 0.276616  [11005/60000]\n",
      "loss: 0.095603  [11505/60000]\n",
      "loss: 0.644189  [12005/60000]\n",
      "loss: 0.642658  [12505/60000]\n",
      "loss: 0.401405  [13005/60000]\n",
      "loss: 0.393741  [13505/60000]\n",
      "loss: 0.042703  [14005/60000]\n",
      "loss: 0.288488  [14505/60000]\n",
      "loss: 0.108215  [15005/60000]\n",
      "loss: 0.185563  [15505/60000]\n",
      "loss: 2.052681  [16005/60000]\n",
      "loss: 0.048286  [16505/60000]\n",
      "loss: 0.061503  [17005/60000]\n",
      "loss: 0.246939  [17505/60000]\n",
      "loss: 0.918883  [18005/60000]\n",
      "loss: 0.113044  [18505/60000]\n",
      "loss: 0.541053  [19005/60000]\n",
      "loss: 0.403705  [19505/60000]\n",
      "loss: 0.062363  [20005/60000]\n",
      "loss: 0.251775  [20505/60000]\n",
      "loss: 0.032017  [21005/60000]\n",
      "loss: 0.119493  [21505/60000]\n",
      "loss: 0.570955  [22005/60000]\n",
      "loss: 1.312981  [22505/60000]\n",
      "loss: 0.272273  [23005/60000]\n",
      "loss: 0.213136  [23505/60000]\n",
      "loss: 0.300543  [24005/60000]\n",
      "loss: 0.072717  [24505/60000]\n",
      "loss: 0.524297  [25005/60000]\n",
      "loss: 1.318408  [25505/60000]\n",
      "loss: 0.310884  [26005/60000]\n",
      "loss: 0.284528  [26505/60000]\n",
      "loss: 0.054571  [27005/60000]\n",
      "loss: 0.542237  [27505/60000]\n",
      "loss: 0.775138  [28005/60000]\n",
      "loss: 0.468357  [28505/60000]\n",
      "loss: 0.215284  [29005/60000]\n",
      "loss: 1.915544  [29505/60000]\n",
      "loss: 0.432974  [30005/60000]\n",
      "loss: 0.064568  [30505/60000]\n",
      "loss: 0.110305  [31005/60000]\n",
      "loss: 0.332488  [31505/60000]\n",
      "loss: 0.454640  [32005/60000]\n",
      "loss: 0.172944  [32505/60000]\n",
      "loss: 0.023991  [33005/60000]\n",
      "loss: 1.000246  [33505/60000]\n",
      "loss: 0.033310  [34005/60000]\n",
      "loss: 0.086756  [34505/60000]\n",
      "loss: 0.322720  [35005/60000]\n",
      "loss: 0.301603  [35505/60000]\n",
      "loss: 0.155743  [36005/60000]\n",
      "loss: 0.566728  [36505/60000]\n",
      "loss: 0.052026  [37005/60000]\n",
      "loss: 0.857790  [37505/60000]\n",
      "loss: 0.472342  [38005/60000]\n",
      "loss: 0.028878  [38505/60000]\n",
      "loss: 0.821331  [39005/60000]\n",
      "loss: 0.107522  [39505/60000]\n",
      "loss: 0.837261  [40005/60000]\n",
      "loss: 0.107188  [40505/60000]\n",
      "loss: 0.068381  [41005/60000]\n",
      "loss: 0.121261  [41505/60000]\n",
      "loss: 0.067101  [42005/60000]\n",
      "loss: 0.768664  [42505/60000]\n",
      "loss: 0.239985  [43005/60000]\n",
      "loss: 0.089038  [43505/60000]\n",
      "loss: 0.360887  [44005/60000]\n",
      "loss: 0.346305  [44505/60000]\n",
      "loss: 0.647504  [45005/60000]\n",
      "loss: 0.263307  [45505/60000]\n",
      "loss: 0.492246  [46005/60000]\n",
      "loss: 0.070241  [46505/60000]\n",
      "loss: 0.183808  [47005/60000]\n",
      "loss: 0.359797  [47505/60000]\n",
      "loss: 0.084650  [48005/60000]\n",
      "loss: 0.110871  [48505/60000]\n",
      "loss: 0.476388  [49005/60000]\n",
      "loss: 0.191599  [49505/60000]\n",
      "loss: 0.222751  [50005/60000]\n",
      "loss: 0.192032  [50505/60000]\n",
      "loss: 0.637698  [51005/60000]\n",
      "loss: 0.089530  [51505/60000]\n",
      "loss: 0.093817  [52005/60000]\n",
      "loss: 1.244258  [52505/60000]\n",
      "loss: 0.824295  [53005/60000]\n",
      "loss: 0.675178  [53505/60000]\n",
      "loss: 0.397774  [54005/60000]\n",
      "loss: 0.032396  [54505/60000]\n",
      "loss: 0.674935  [55005/60000]\n",
      "loss: 0.040696  [55505/60000]\n",
      "loss: 0.306605  [56005/60000]\n",
      "loss: 0.520309  [56505/60000]\n",
      "loss: 0.758607  [57005/60000]\n",
      "loss: 0.220491  [57505/60000]\n",
      "loss: 0.059461  [58005/60000]\n",
      "loss: 0.061966  [58505/60000]\n",
      "loss: 0.221729  [59005/60000]\n",
      "loss: 0.364744  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.417371 \n",
      "\n",
      "time: 22.946798086166382\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "loss: 0.925630  [    5/60000]\n",
      "loss: 1.360870  [  505/60000]\n",
      "loss: 0.093540  [ 1005/60000]\n",
      "loss: 0.117809  [ 1505/60000]\n",
      "loss: 0.117018  [ 2005/60000]\n",
      "loss: 0.730402  [ 2505/60000]\n",
      "loss: 0.775108  [ 3005/60000]\n",
      "loss: 0.537240  [ 3505/60000]\n",
      "loss: 0.038639  [ 4005/60000]\n",
      "loss: 0.702079  [ 4505/60000]\n",
      "loss: 0.146797  [ 5005/60000]\n",
      "loss: 1.329974  [ 5505/60000]\n",
      "loss: 0.733287  [ 6005/60000]\n",
      "loss: 0.609644  [ 6505/60000]\n",
      "loss: 0.736582  [ 7005/60000]\n",
      "loss: 0.051379  [ 7505/60000]\n",
      "loss: 0.150414  [ 8005/60000]\n",
      "loss: 0.019191  [ 8505/60000]\n",
      "loss: 0.094619  [ 9005/60000]\n",
      "loss: 0.517474  [ 9505/60000]\n",
      "loss: 0.303475  [10005/60000]\n",
      "loss: 0.068818  [10505/60000]\n",
      "loss: 0.258078  [11005/60000]\n",
      "loss: 0.088247  [11505/60000]\n",
      "loss: 0.615474  [12005/60000]\n",
      "loss: 0.632561  [12505/60000]\n",
      "loss: 0.379151  [13005/60000]\n",
      "loss: 0.383192  [13505/60000]\n",
      "loss: 0.040914  [14005/60000]\n",
      "loss: 0.267400  [14505/60000]\n",
      "loss: 0.102413  [15005/60000]\n",
      "loss: 0.179310  [15505/60000]\n",
      "loss: 2.050837  [16005/60000]\n",
      "loss: 0.047049  [16505/60000]\n",
      "loss: 0.057674  [17005/60000]\n",
      "loss: 0.236474  [17505/60000]\n",
      "loss: 0.931127  [18005/60000]\n",
      "loss: 0.106987  [18505/60000]\n",
      "loss: 0.541808  [19005/60000]\n",
      "loss: 0.384386  [19505/60000]\n",
      "loss: 0.062195  [20005/60000]\n",
      "loss: 0.245024  [20505/60000]\n",
      "loss: 0.030230  [21005/60000]\n",
      "loss: 0.127376  [21505/60000]\n",
      "loss: 0.548164  [22005/60000]\n",
      "loss: 1.279332  [22505/60000]\n",
      "loss: 0.262089  [23005/60000]\n",
      "loss: 0.200804  [23505/60000]\n",
      "loss: 0.306356  [24005/60000]\n",
      "loss: 0.067943  [24505/60000]\n",
      "loss: 0.522652  [25005/60000]\n",
      "loss: 1.311747  [25505/60000]\n",
      "loss: 0.286897  [26005/60000]\n",
      "loss: 0.293696  [26505/60000]\n",
      "loss: 0.053244  [27005/60000]\n",
      "loss: 0.515758  [27505/60000]\n",
      "loss: 0.748994  [28005/60000]\n",
      "loss: 0.468214  [28505/60000]\n",
      "loss: 0.203375  [29005/60000]\n",
      "loss: 1.861573  [29505/60000]\n",
      "loss: 0.415953  [30005/60000]\n",
      "loss: 0.062978  [30505/60000]\n",
      "loss: 0.114519  [31005/60000]\n",
      "loss: 0.327026  [31505/60000]\n",
      "loss: 0.426119  [32005/60000]\n",
      "loss: 0.163202  [32505/60000]\n",
      "loss: 0.022873  [33005/60000]\n",
      "loss: 0.997839  [33505/60000]\n",
      "loss: 0.029666  [34005/60000]\n",
      "loss: 0.082581  [34505/60000]\n",
      "loss: 0.319627  [35005/60000]\n",
      "loss: 0.293003  [35505/60000]\n",
      "loss: 0.139478  [36005/60000]\n",
      "loss: 0.559186  [36505/60000]\n",
      "loss: 0.049571  [37005/60000]\n",
      "loss: 0.839137  [37505/60000]\n",
      "loss: 0.462392  [38005/60000]\n",
      "loss: 0.028875  [38505/60000]\n",
      "loss: 0.844301  [39005/60000]\n",
      "loss: 0.098224  [39505/60000]\n",
      "loss: 0.812273  [40005/60000]\n",
      "loss: 0.099184  [40505/60000]\n",
      "loss: 0.064946  [41005/60000]\n",
      "loss: 0.110462  [41505/60000]\n",
      "loss: 0.069465  [42005/60000]\n",
      "loss: 0.698753  [42505/60000]\n",
      "loss: 0.228410  [43005/60000]\n",
      "loss: 0.093409  [43505/60000]\n",
      "loss: 0.346229  [44005/60000]\n",
      "loss: 0.337848  [44505/60000]\n",
      "loss: 0.641580  [45005/60000]\n",
      "loss: 0.248801  [45505/60000]\n",
      "loss: 0.483007  [46005/60000]\n",
      "loss: 0.069004  [46505/60000]\n",
      "loss: 0.180179  [47005/60000]\n",
      "loss: 0.375871  [47505/60000]\n",
      "loss: 0.081688  [48005/60000]\n",
      "loss: 0.105669  [48505/60000]\n",
      "loss: 0.459269  [49005/60000]\n",
      "loss: 0.177072  [49505/60000]\n",
      "loss: 0.227575  [50005/60000]\n",
      "loss: 0.176872  [50505/60000]\n",
      "loss: 0.628182  [51005/60000]\n",
      "loss: 0.086352  [51505/60000]\n",
      "loss: 0.093460  [52005/60000]\n",
      "loss: 1.215113  [52505/60000]\n",
      "loss: 0.805679  [53005/60000]\n",
      "loss: 0.657628  [53505/60000]\n",
      "loss: 0.382227  [54005/60000]\n",
      "loss: 0.029872  [54505/60000]\n",
      "loss: 0.658555  [55005/60000]\n",
      "loss: 0.040078  [55505/60000]\n",
      "loss: 0.289941  [56005/60000]\n",
      "loss: 0.479451  [56505/60000]\n",
      "loss: 0.743083  [57005/60000]\n",
      "loss: 0.204265  [57505/60000]\n",
      "loss: 0.053759  [58005/60000]\n",
      "loss: 0.057394  [58505/60000]\n",
      "loss: 0.216792  [59005/60000]\n",
      "loss: 0.360829  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.410835 \n",
      "\n",
      "time: 22.30476713180542\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "loss: 0.900838  [    5/60000]\n",
      "loss: 1.341897  [  505/60000]\n",
      "loss: 0.094899  [ 1005/60000]\n",
      "loss: 0.112742  [ 1505/60000]\n",
      "loss: 0.113880  [ 2005/60000]\n",
      "loss: 0.725934  [ 2505/60000]\n",
      "loss: 0.774465  [ 3005/60000]\n",
      "loss: 0.533864  [ 3505/60000]\n",
      "loss: 0.038765  [ 4005/60000]\n",
      "loss: 0.647055  [ 4505/60000]\n",
      "loss: 0.146390  [ 5005/60000]\n",
      "loss: 1.299304  [ 5505/60000]\n",
      "loss: 0.730114  [ 6005/60000]\n",
      "loss: 0.626345  [ 6505/60000]\n",
      "loss: 0.715926  [ 7005/60000]\n",
      "loss: 0.049179  [ 7505/60000]\n",
      "loss: 0.141063  [ 8005/60000]\n",
      "loss: 0.019623  [ 8505/60000]\n",
      "loss: 0.088438  [ 9005/60000]\n",
      "loss: 0.490513  [ 9505/60000]\n",
      "loss: 0.293306  [10005/60000]\n",
      "loss: 0.068526  [10505/60000]\n",
      "loss: 0.242951  [11005/60000]\n",
      "loss: 0.080851  [11505/60000]\n",
      "loss: 0.584692  [12005/60000]\n",
      "loss: 0.613091  [12505/60000]\n",
      "loss: 0.353975  [13005/60000]\n",
      "loss: 0.370711  [13505/60000]\n",
      "loss: 0.039050  [14005/60000]\n",
      "loss: 0.249888  [14505/60000]\n",
      "loss: 0.094025  [15005/60000]\n",
      "loss: 0.175128  [15505/60000]\n",
      "loss: 2.054988  [16005/60000]\n",
      "loss: 0.044868  [16505/60000]\n",
      "loss: 0.053893  [17005/60000]\n",
      "loss: 0.224034  [17505/60000]\n",
      "loss: 0.939494  [18005/60000]\n",
      "loss: 0.100385  [18505/60000]\n",
      "loss: 0.549217  [19005/60000]\n",
      "loss: 0.372525  [19505/60000]\n",
      "loss: 0.062467  [20005/60000]\n",
      "loss: 0.238122  [20505/60000]\n",
      "loss: 0.028516  [21005/60000]\n",
      "loss: 0.136550  [21505/60000]\n",
      "loss: 0.526210  [22005/60000]\n",
      "loss: 1.238765  [22505/60000]\n",
      "loss: 0.251113  [23005/60000]\n",
      "loss: 0.191215  [23505/60000]\n",
      "loss: 0.312551  [24005/60000]\n",
      "loss: 0.063659  [24505/60000]\n",
      "loss: 0.517385  [25005/60000]\n",
      "loss: 1.307534  [25505/60000]\n",
      "loss: 0.265975  [26005/60000]\n",
      "loss: 0.298892  [26505/60000]\n",
      "loss: 0.052360  [27005/60000]\n",
      "loss: 0.492575  [27505/60000]\n",
      "loss: 0.729294  [28005/60000]\n",
      "loss: 0.464793  [28505/60000]\n",
      "loss: 0.194378  [29005/60000]\n",
      "loss: 1.799781  [29505/60000]\n",
      "loss: 0.399098  [30005/60000]\n",
      "loss: 0.060801  [30505/60000]\n",
      "loss: 0.117482  [31005/60000]\n",
      "loss: 0.329573  [31505/60000]\n",
      "loss: 0.396297  [32005/60000]\n",
      "loss: 0.155390  [32505/60000]\n",
      "loss: 0.021388  [33005/60000]\n",
      "loss: 0.993713  [33505/60000]\n",
      "loss: 0.026663  [34005/60000]\n",
      "loss: 0.078232  [34505/60000]\n",
      "loss: 0.313328  [35005/60000]\n",
      "loss: 0.287751  [35505/60000]\n",
      "loss: 0.124242  [36005/60000]\n",
      "loss: 0.554484  [36505/60000]\n",
      "loss: 0.047890  [37005/60000]\n",
      "loss: 0.811556  [37505/60000]\n",
      "loss: 0.452386  [38005/60000]\n",
      "loss: 0.028501  [38505/60000]\n",
      "loss: 0.861903  [39005/60000]\n",
      "loss: 0.088906  [39505/60000]\n",
      "loss: 0.797517  [40005/60000]\n",
      "loss: 0.091117  [40505/60000]\n",
      "loss: 0.062513  [41005/60000]\n",
      "loss: 0.100520  [41505/60000]\n",
      "loss: 0.072020  [42005/60000]\n",
      "loss: 0.639001  [42505/60000]\n",
      "loss: 0.221092  [43005/60000]\n",
      "loss: 0.097447  [43505/60000]\n",
      "loss: 0.334221  [44005/60000]\n",
      "loss: 0.325840  [44505/60000]\n",
      "loss: 0.637580  [45005/60000]\n",
      "loss: 0.236496  [45505/60000]\n",
      "loss: 0.474901  [46005/60000]\n",
      "loss: 0.067300  [46505/60000]\n",
      "loss: 0.178819  [47005/60000]\n",
      "loss: 0.390650  [47505/60000]\n",
      "loss: 0.078325  [48005/60000]\n",
      "loss: 0.102165  [48505/60000]\n",
      "loss: 0.446137  [49005/60000]\n",
      "loss: 0.163824  [49505/60000]\n",
      "loss: 0.224690  [50005/60000]\n",
      "loss: 0.167332  [50505/60000]\n",
      "loss: 0.623779  [51005/60000]\n",
      "loss: 0.079091  [51505/60000]\n",
      "loss: 0.093095  [52005/60000]\n",
      "loss: 1.180222  [52505/60000]\n",
      "loss: 0.787043  [53005/60000]\n",
      "loss: 0.644340  [53505/60000]\n",
      "loss: 0.366898  [54005/60000]\n",
      "loss: 0.027673  [54505/60000]\n",
      "loss: 0.647533  [55005/60000]\n",
      "loss: 0.039490  [55505/60000]\n",
      "loss: 0.275023  [56005/60000]\n",
      "loss: 0.444921  [56505/60000]\n",
      "loss: 0.733649  [57005/60000]\n",
      "loss: 0.188941  [57505/60000]\n",
      "loss: 0.048138  [58005/60000]\n",
      "loss: 0.053834  [58505/60000]\n",
      "loss: 0.213828  [59005/60000]\n",
      "loss: 0.361230  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.404945 \n",
      "\n",
      "time: 22.166809797286987\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "loss: 0.869671  [    5/60000]\n",
      "loss: 1.329804  [  505/60000]\n",
      "loss: 0.095159  [ 1005/60000]\n",
      "loss: 0.108458  [ 1505/60000]\n",
      "loss: 0.109652  [ 2005/60000]\n",
      "loss: 0.730660  [ 2505/60000]\n",
      "loss: 0.774276  [ 3005/60000]\n",
      "loss: 0.528860  [ 3505/60000]\n",
      "loss: 0.038515  [ 4005/60000]\n",
      "loss: 0.594004  [ 4505/60000]\n",
      "loss: 0.144919  [ 5005/60000]\n",
      "loss: 1.264268  [ 5505/60000]\n",
      "loss: 0.738627  [ 6005/60000]\n",
      "loss: 0.631763  [ 6505/60000]\n",
      "loss: 0.695501  [ 7005/60000]\n",
      "loss: 0.047783  [ 7505/60000]\n",
      "loss: 0.135663  [ 8005/60000]\n",
      "loss: 0.020190  [ 8505/60000]\n",
      "loss: 0.083368  [ 9005/60000]\n",
      "loss: 0.470434  [ 9505/60000]\n",
      "loss: 0.284300  [10005/60000]\n",
      "loss: 0.067894  [10505/60000]\n",
      "loss: 0.229713  [11005/60000]\n",
      "loss: 0.075112  [11505/60000]\n",
      "loss: 0.552228  [12005/60000]\n",
      "loss: 0.592383  [12505/60000]\n",
      "loss: 0.324018  [13005/60000]\n",
      "loss: 0.360652  [13505/60000]\n",
      "loss: 0.037469  [14005/60000]\n",
      "loss: 0.236799  [14505/60000]\n",
      "loss: 0.086425  [15005/60000]\n",
      "loss: 0.170626  [15505/60000]\n",
      "loss: 2.047814  [16005/60000]\n",
      "loss: 0.043032  [16505/60000]\n",
      "loss: 0.051414  [17005/60000]\n",
      "loss: 0.215062  [17505/60000]\n",
      "loss: 0.950884  [18005/60000]\n",
      "loss: 0.090893  [18505/60000]\n",
      "loss: 0.553580  [19005/60000]\n",
      "loss: 0.357990  [19505/60000]\n",
      "loss: 0.062962  [20005/60000]\n",
      "loss: 0.230722  [20505/60000]\n",
      "loss: 0.027409  [21005/60000]\n",
      "loss: 0.147313  [21505/60000]\n",
      "loss: 0.502861  [22005/60000]\n",
      "loss: 1.198182  [22505/60000]\n",
      "loss: 0.240881  [23005/60000]\n",
      "loss: 0.183854  [23505/60000]\n",
      "loss: 0.316893  [24005/60000]\n",
      "loss: 0.059156  [24505/60000]\n",
      "loss: 0.517981  [25005/60000]\n",
      "loss: 1.298292  [25505/60000]\n",
      "loss: 0.247515  [26005/60000]\n",
      "loss: 0.307228  [26505/60000]\n",
      "loss: 0.051515  [27005/60000]\n",
      "loss: 0.469657  [27505/60000]\n",
      "loss: 0.708915  [28005/60000]\n",
      "loss: 0.460240  [28505/60000]\n",
      "loss: 0.187387  [29005/60000]\n",
      "loss: 1.735864  [29505/60000]\n",
      "loss: 0.383778  [30005/60000]\n",
      "loss: 0.058695  [30505/60000]\n",
      "loss: 0.118022  [31005/60000]\n",
      "loss: 0.325335  [31505/60000]\n",
      "loss: 0.365250  [32005/60000]\n",
      "loss: 0.146307  [32505/60000]\n",
      "loss: 0.020465  [33005/60000]\n",
      "loss: 0.983429  [33505/60000]\n",
      "loss: 0.024425  [34005/60000]\n",
      "loss: 0.075064  [34505/60000]\n",
      "loss: 0.310026  [35005/60000]\n",
      "loss: 0.277591  [35505/60000]\n",
      "loss: 0.110421  [36005/60000]\n",
      "loss: 0.549452  [36505/60000]\n",
      "loss: 0.046165  [37005/60000]\n",
      "loss: 0.788642  [37505/60000]\n",
      "loss: 0.445369  [38005/60000]\n",
      "loss: 0.027919  [38505/60000]\n",
      "loss: 0.882258  [39005/60000]\n",
      "loss: 0.081804  [39505/60000]\n",
      "loss: 0.780389  [40005/60000]\n",
      "loss: 0.084406  [40505/60000]\n",
      "loss: 0.059768  [41005/60000]\n",
      "loss: 0.091917  [41505/60000]\n",
      "loss: 0.075061  [42005/60000]\n",
      "loss: 0.594654  [42505/60000]\n",
      "loss: 0.217068  [43005/60000]\n",
      "loss: 0.101984  [43505/60000]\n",
      "loss: 0.324926  [44005/60000]\n",
      "loss: 0.313375  [44505/60000]\n",
      "loss: 0.631377  [45005/60000]\n",
      "loss: 0.225543  [45505/60000]\n",
      "loss: 0.467131  [46005/60000]\n",
      "loss: 0.065518  [46505/60000]\n",
      "loss: 0.176805  [47005/60000]\n",
      "loss: 0.403229  [47505/60000]\n",
      "loss: 0.074603  [48005/60000]\n",
      "loss: 0.097906  [48505/60000]\n",
      "loss: 0.437285  [49005/60000]\n",
      "loss: 0.152027  [49505/60000]\n",
      "loss: 0.220209  [50005/60000]\n",
      "loss: 0.157296  [50505/60000]\n",
      "loss: 0.619313  [51005/60000]\n",
      "loss: 0.075468  [51505/60000]\n",
      "loss: 0.091580  [52005/60000]\n",
      "loss: 1.143904  [52505/60000]\n",
      "loss: 0.770306  [53005/60000]\n",
      "loss: 0.625692  [53505/60000]\n",
      "loss: 0.356736  [54005/60000]\n",
      "loss: 0.025518  [54505/60000]\n",
      "loss: 0.636547  [55005/60000]\n",
      "loss: 0.038088  [55505/60000]\n",
      "loss: 0.256836  [56005/60000]\n",
      "loss: 0.418232  [56505/60000]\n",
      "loss: 0.737658  [57005/60000]\n",
      "loss: 0.174128  [57505/60000]\n",
      "loss: 0.043915  [58005/60000]\n",
      "loss: 0.050374  [58505/60000]\n",
      "loss: 0.210711  [59005/60000]\n",
      "loss: 0.364862  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.399565 \n",
      "\n",
      "time: 22.42209029197693\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "loss: 0.837637  [    5/60000]\n",
      "loss: 1.326725  [  505/60000]\n",
      "loss: 0.095206  [ 1005/60000]\n",
      "loss: 0.105336  [ 1505/60000]\n",
      "loss: 0.105859  [ 2005/60000]\n",
      "loss: 0.728315  [ 2505/60000]\n",
      "loss: 0.777529  [ 3005/60000]\n",
      "loss: 0.516387  [ 3505/60000]\n",
      "loss: 0.038367  [ 4005/60000]\n",
      "loss: 0.557874  [ 4505/60000]\n",
      "loss: 0.144247  [ 5005/60000]\n",
      "loss: 1.229408  [ 5505/60000]\n",
      "loss: 0.740076  [ 6005/60000]\n",
      "loss: 0.638739  [ 6505/60000]\n",
      "loss: 0.673028  [ 7005/60000]\n",
      "loss: 0.046342  [ 7505/60000]\n",
      "loss: 0.129095  [ 8005/60000]\n",
      "loss: 0.021259  [ 8505/60000]\n",
      "loss: 0.078710  [ 9005/60000]\n",
      "loss: 0.445975  [ 9505/60000]\n",
      "loss: 0.273291  [10005/60000]\n",
      "loss: 0.066500  [10505/60000]\n",
      "loss: 0.217554  [11005/60000]\n",
      "loss: 0.069641  [11505/60000]\n",
      "loss: 0.523837  [12005/60000]\n",
      "loss: 0.570417  [12505/60000]\n",
      "loss: 0.300408  [13005/60000]\n",
      "loss: 0.348321  [13505/60000]\n",
      "loss: 0.036196  [14005/60000]\n",
      "loss: 0.227082  [14505/60000]\n",
      "loss: 0.078666  [15005/60000]\n",
      "loss: 0.164961  [15505/60000]\n",
      "loss: 2.049086  [16005/60000]\n",
      "loss: 0.041816  [16505/60000]\n",
      "loss: 0.048383  [17005/60000]\n",
      "loss: 0.203371  [17505/60000]\n",
      "loss: 0.961396  [18005/60000]\n",
      "loss: 0.082704  [18505/60000]\n",
      "loss: 0.558404  [19005/60000]\n",
      "loss: 0.349174  [19505/60000]\n",
      "loss: 0.062339  [20005/60000]\n",
      "loss: 0.221824  [20505/60000]\n",
      "loss: 0.026717  [21005/60000]\n",
      "loss: 0.155274  [21505/60000]\n",
      "loss: 0.478481  [22005/60000]\n",
      "loss: 1.158895  [22505/60000]\n",
      "loss: 0.233314  [23005/60000]\n",
      "loss: 0.178492  [23505/60000]\n",
      "loss: 0.326026  [24005/60000]\n",
      "loss: 0.055556  [24505/60000]\n",
      "loss: 0.515706  [25005/60000]\n",
      "loss: 1.286955  [25505/60000]\n",
      "loss: 0.233316  [26005/60000]\n",
      "loss: 0.312789  [26505/60000]\n",
      "loss: 0.050911  [27005/60000]\n",
      "loss: 0.448117  [27505/60000]\n",
      "loss: 0.688251  [28005/60000]\n",
      "loss: 0.456168  [28505/60000]\n",
      "loss: 0.176850  [29005/60000]\n",
      "loss: 1.665839  [29505/60000]\n",
      "loss: 0.378460  [30005/60000]\n",
      "loss: 0.056886  [30505/60000]\n",
      "loss: 0.116381  [31005/60000]\n",
      "loss: 0.327921  [31505/60000]\n",
      "loss: 0.336400  [32005/60000]\n",
      "loss: 0.136204  [32505/60000]\n",
      "loss: 0.019489  [33005/60000]\n",
      "loss: 0.968902  [33505/60000]\n",
      "loss: 0.022084  [34005/60000]\n",
      "loss: 0.069560  [34505/60000]\n",
      "loss: 0.305660  [35005/60000]\n",
      "loss: 0.269412  [35505/60000]\n",
      "loss: 0.101104  [36005/60000]\n",
      "loss: 0.547367  [36505/60000]\n",
      "loss: 0.045105  [37005/60000]\n",
      "loss: 0.767703  [37505/60000]\n",
      "loss: 0.438901  [38005/60000]\n",
      "loss: 0.027657  [38505/60000]\n",
      "loss: 0.902459  [39005/60000]\n",
      "loss: 0.078053  [39505/60000]\n",
      "loss: 0.767122  [40005/60000]\n",
      "loss: 0.078311  [40505/60000]\n",
      "loss: 0.058759  [41005/60000]\n",
      "loss: 0.084776  [41505/60000]\n",
      "loss: 0.078776  [42005/60000]\n",
      "loss: 0.543819  [42505/60000]\n",
      "loss: 0.216164  [43005/60000]\n",
      "loss: 0.103951  [43505/60000]\n",
      "loss: 0.313504  [44005/60000]\n",
      "loss: 0.302468  [44505/60000]\n",
      "loss: 0.630468  [45005/60000]\n",
      "loss: 0.212658  [45505/60000]\n",
      "loss: 0.462590  [46005/60000]\n",
      "loss: 0.063885  [46505/60000]\n",
      "loss: 0.174738  [47005/60000]\n",
      "loss: 0.406051  [47505/60000]\n",
      "loss: 0.070185  [48005/60000]\n",
      "loss: 0.098065  [48505/60000]\n",
      "loss: 0.430673  [49005/60000]\n",
      "loss: 0.141562  [49505/60000]\n",
      "loss: 0.220409  [50005/60000]\n",
      "loss: 0.148959  [50505/60000]\n",
      "loss: 0.614870  [51005/60000]\n",
      "loss: 0.072078  [51505/60000]\n",
      "loss: 0.091479  [52005/60000]\n",
      "loss: 1.099238  [52505/60000]\n",
      "loss: 0.750386  [53005/60000]\n",
      "loss: 0.604449  [53505/60000]\n",
      "loss: 0.344038  [54005/60000]\n",
      "loss: 0.024228  [54505/60000]\n",
      "loss: 0.630477  [55005/60000]\n",
      "loss: 0.036419  [55505/60000]\n",
      "loss: 0.239871  [56005/60000]\n",
      "loss: 0.389640  [56505/60000]\n",
      "loss: 0.739844  [57005/60000]\n",
      "loss: 0.159162  [57505/60000]\n",
      "loss: 0.040766  [58005/60000]\n",
      "loss: 0.047268  [58505/60000]\n",
      "loss: 0.206801  [59005/60000]\n",
      "loss: 0.368711  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.394545 \n",
      "\n",
      "time: 22.489800930023193\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "loss: 0.804858  [    5/60000]\n",
      "loss: 1.328008  [  505/60000]\n",
      "loss: 0.097140  [ 1005/60000]\n",
      "loss: 0.102604  [ 1505/60000]\n",
      "loss: 0.102269  [ 2005/60000]\n",
      "loss: 0.728656  [ 2505/60000]\n",
      "loss: 0.783565  [ 3005/60000]\n",
      "loss: 0.504881  [ 3505/60000]\n",
      "loss: 0.038403  [ 4005/60000]\n",
      "loss: 0.523368  [ 4505/60000]\n",
      "loss: 0.143724  [ 5005/60000]\n",
      "loss: 1.180365  [ 5505/60000]\n",
      "loss: 0.744987  [ 6005/60000]\n",
      "loss: 0.643854  [ 6505/60000]\n",
      "loss: 0.652800  [ 7005/60000]\n",
      "loss: 0.044817  [ 7505/60000]\n",
      "loss: 0.124120  [ 8005/60000]\n",
      "loss: 0.022033  [ 8505/60000]\n",
      "loss: 0.074310  [ 9005/60000]\n",
      "loss: 0.432457  [ 9505/60000]\n",
      "loss: 0.261973  [10005/60000]\n",
      "loss: 0.067430  [10505/60000]\n",
      "loss: 0.207875  [11005/60000]\n",
      "loss: 0.066102  [11505/60000]\n",
      "loss: 0.495314  [12005/60000]\n",
      "loss: 0.547418  [12505/60000]\n",
      "loss: 0.275242  [13005/60000]\n",
      "loss: 0.338376  [13505/60000]\n",
      "loss: 0.034727  [14005/60000]\n",
      "loss: 0.218082  [14505/60000]\n",
      "loss: 0.073577  [15005/60000]\n",
      "loss: 0.160074  [15505/60000]\n",
      "loss: 2.039606  [16005/60000]\n",
      "loss: 0.040611  [16505/60000]\n",
      "loss: 0.045397  [17005/60000]\n",
      "loss: 0.194675  [17505/60000]\n",
      "loss: 0.962711  [18005/60000]\n",
      "loss: 0.074252  [18505/60000]\n",
      "loss: 0.559904  [19005/60000]\n",
      "loss: 0.342826  [19505/60000]\n",
      "loss: 0.062183  [20005/60000]\n",
      "loss: 0.213480  [20505/60000]\n",
      "loss: 0.026217  [21005/60000]\n",
      "loss: 0.166364  [21505/60000]\n",
      "loss: 0.452025  [22005/60000]\n",
      "loss: 1.108249  [22505/60000]\n",
      "loss: 0.226763  [23005/60000]\n",
      "loss: 0.174551  [23505/60000]\n",
      "loss: 0.330990  [24005/60000]\n",
      "loss: 0.052174  [24505/60000]\n",
      "loss: 0.515020  [25005/60000]\n",
      "loss: 1.275583  [25505/60000]\n",
      "loss: 0.219817  [26005/60000]\n",
      "loss: 0.315838  [26505/60000]\n",
      "loss: 0.050075  [27005/60000]\n",
      "loss: 0.427166  [27505/60000]\n",
      "loss: 0.670895  [28005/60000]\n",
      "loss: 0.453441  [28505/60000]\n",
      "loss: 0.168752  [29005/60000]\n",
      "loss: 1.598880  [29505/60000]\n",
      "loss: 0.366371  [30005/60000]\n",
      "loss: 0.054671  [30505/60000]\n",
      "loss: 0.118197  [31005/60000]\n",
      "loss: 0.335704  [31505/60000]\n",
      "loss: 0.311782  [32005/60000]\n",
      "loss: 0.135217  [32505/60000]\n",
      "loss: 0.018721  [33005/60000]\n",
      "loss: 0.954524  [33505/60000]\n",
      "loss: 0.020881  [34005/60000]\n",
      "loss: 0.064786  [34505/60000]\n",
      "loss: 0.299329  [35005/60000]\n",
      "loss: 0.264394  [35505/60000]\n",
      "loss: 0.090570  [36005/60000]\n",
      "loss: 0.548860  [36505/60000]\n",
      "loss: 0.043972  [37005/60000]\n",
      "loss: 0.736462  [37505/60000]\n",
      "loss: 0.432878  [38005/60000]\n",
      "loss: 0.026825  [38505/60000]\n",
      "loss: 0.910825  [39005/60000]\n",
      "loss: 0.073854  [39505/60000]\n",
      "loss: 0.755516  [40005/60000]\n",
      "loss: 0.073072  [40505/60000]\n",
      "loss: 0.057174  [41005/60000]\n",
      "loss: 0.078785  [41505/60000]\n",
      "loss: 0.081841  [42005/60000]\n",
      "loss: 0.497289  [42505/60000]\n",
      "loss: 0.216260  [43005/60000]\n",
      "loss: 0.107883  [43505/60000]\n",
      "loss: 0.298463  [44005/60000]\n",
      "loss: 0.292568  [44505/60000]\n",
      "loss: 0.627461  [45005/60000]\n",
      "loss: 0.200428  [45505/60000]\n",
      "loss: 0.457124  [46005/60000]\n",
      "loss: 0.062454  [46505/60000]\n",
      "loss: 0.171098  [47005/60000]\n",
      "loss: 0.418524  [47505/60000]\n",
      "loss: 0.064784  [48005/60000]\n",
      "loss: 0.097451  [48505/60000]\n",
      "loss: 0.417563  [49005/60000]\n",
      "loss: 0.130753  [49505/60000]\n",
      "loss: 0.215062  [50005/60000]\n",
      "loss: 0.139945  [50505/60000]\n",
      "loss: 0.607798  [51005/60000]\n",
      "loss: 0.065550  [51505/60000]\n",
      "loss: 0.091978  [52005/60000]\n",
      "loss: 1.059972  [52505/60000]\n",
      "loss: 0.735875  [53005/60000]\n",
      "loss: 0.588540  [53505/60000]\n",
      "loss: 0.327694  [54005/60000]\n",
      "loss: 0.022863  [54505/60000]\n",
      "loss: 0.623938  [55005/60000]\n",
      "loss: 0.034692  [55505/60000]\n",
      "loss: 0.220036  [56005/60000]\n",
      "loss: 0.365507  [56505/60000]\n",
      "loss: 0.749634  [57005/60000]\n",
      "loss: 0.146105  [57505/60000]\n",
      "loss: 0.038213  [58005/60000]\n",
      "loss: 0.044906  [58505/60000]\n",
      "loss: 0.198787  [59005/60000]\n",
      "loss: 0.372741  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.390008 \n",
      "\n",
      "time: 21.958406925201416\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "loss: 0.775007  [    5/60000]\n",
      "loss: 1.340594  [  505/60000]\n",
      "loss: 0.096941  [ 1005/60000]\n",
      "loss: 0.099995  [ 1505/60000]\n",
      "loss: 0.099756  [ 2005/60000]\n",
      "loss: 0.723446  [ 2505/60000]\n",
      "loss: 0.793008  [ 3005/60000]\n",
      "loss: 0.478484  [ 3505/60000]\n",
      "loss: 0.038032  [ 4005/60000]\n",
      "loss: 0.501110  [ 4505/60000]\n",
      "loss: 0.145139  [ 5005/60000]\n",
      "loss: 1.136506  [ 5505/60000]\n",
      "loss: 0.746929  [ 6005/60000]\n",
      "loss: 0.641932  [ 6505/60000]\n",
      "loss: 0.636458  [ 7005/60000]\n",
      "loss: 0.044187  [ 7505/60000]\n",
      "loss: 0.120739  [ 8005/60000]\n",
      "loss: 0.022767  [ 8505/60000]\n",
      "loss: 0.070410  [ 9005/60000]\n",
      "loss: 0.415527  [ 9505/60000]\n",
      "loss: 0.250528  [10005/60000]\n",
      "loss: 0.066513  [10505/60000]\n",
      "loss: 0.197788  [11005/60000]\n",
      "loss: 0.062905  [11505/60000]\n",
      "loss: 0.474483  [12005/60000]\n",
      "loss: 0.523367  [12505/60000]\n",
      "loss: 0.255396  [13005/60000]\n",
      "loss: 0.331959  [13505/60000]\n",
      "loss: 0.032432  [14005/60000]\n",
      "loss: 0.204278  [14505/60000]\n",
      "loss: 0.065662  [15005/60000]\n",
      "loss: 0.153949  [15505/60000]\n",
      "loss: 2.036356  [16005/60000]\n",
      "loss: 0.039526  [16505/60000]\n",
      "loss: 0.043226  [17005/60000]\n",
      "loss: 0.187428  [17505/60000]\n",
      "loss: 0.968305  [18005/60000]\n",
      "loss: 0.066675  [18505/60000]\n",
      "loss: 0.553875  [19005/60000]\n",
      "loss: 0.335772  [19505/60000]\n",
      "loss: 0.062389  [20005/60000]\n",
      "loss: 0.208809  [20505/60000]\n",
      "loss: 0.025788  [21005/60000]\n",
      "loss: 0.182603  [21505/60000]\n",
      "loss: 0.437862  [22005/60000]\n",
      "loss: 1.058147  [22505/60000]\n",
      "loss: 0.220002  [23005/60000]\n",
      "loss: 0.171923  [23505/60000]\n",
      "loss: 0.340862  [24005/60000]\n",
      "loss: 0.049078  [24505/60000]\n",
      "loss: 0.511146  [25005/60000]\n",
      "loss: 1.262834  [25505/60000]\n",
      "loss: 0.206806  [26005/60000]\n",
      "loss: 0.314498  [26505/60000]\n",
      "loss: 0.049330  [27005/60000]\n",
      "loss: 0.407646  [27505/60000]\n",
      "loss: 0.650482  [28005/60000]\n",
      "loss: 0.452471  [28505/60000]\n",
      "loss: 0.160298  [29005/60000]\n",
      "loss: 1.535669  [29505/60000]\n",
      "loss: 0.364823  [30005/60000]\n",
      "loss: 0.052764  [30505/60000]\n",
      "loss: 0.118267  [31005/60000]\n",
      "loss: 0.330561  [31505/60000]\n",
      "loss: 0.289762  [32005/60000]\n",
      "loss: 0.127320  [32505/60000]\n",
      "loss: 0.018196  [33005/60000]\n",
      "loss: 0.940143  [33505/60000]\n",
      "loss: 0.019147  [34005/60000]\n",
      "loss: 0.060205  [34505/60000]\n",
      "loss: 0.289776  [35005/60000]\n",
      "loss: 0.260460  [35505/60000]\n",
      "loss: 0.082925  [36005/60000]\n",
      "loss: 0.546828  [36505/60000]\n",
      "loss: 0.042726  [37005/60000]\n",
      "loss: 0.704010  [37505/60000]\n",
      "loss: 0.428459  [38005/60000]\n",
      "loss: 0.025879  [38505/60000]\n",
      "loss: 0.923907  [39005/60000]\n",
      "loss: 0.068179  [39505/60000]\n",
      "loss: 0.740771  [40005/60000]\n",
      "loss: 0.068549  [40505/60000]\n",
      "loss: 0.056231  [41005/60000]\n",
      "loss: 0.073343  [41505/60000]\n",
      "loss: 0.084956  [42005/60000]\n",
      "loss: 0.462678  [42505/60000]\n",
      "loss: 0.215085  [43005/60000]\n",
      "loss: 0.109512  [43505/60000]\n",
      "loss: 0.287421  [44005/60000]\n",
      "loss: 0.284734  [44505/60000]\n",
      "loss: 0.626472  [45005/60000]\n",
      "loss: 0.193148  [45505/60000]\n",
      "loss: 0.451533  [46005/60000]\n",
      "loss: 0.060966  [46505/60000]\n",
      "loss: 0.169545  [47005/60000]\n",
      "loss: 0.421050  [47505/60000]\n",
      "loss: 0.060167  [48005/60000]\n",
      "loss: 0.097543  [48505/60000]\n",
      "loss: 0.407489  [49005/60000]\n",
      "loss: 0.121091  [49505/60000]\n",
      "loss: 0.215609  [50005/60000]\n",
      "loss: 0.133681  [50505/60000]\n",
      "loss: 0.606500  [51005/60000]\n",
      "loss: 0.063864  [51505/60000]\n",
      "loss: 0.091935  [52005/60000]\n",
      "loss: 1.017438  [52505/60000]\n",
      "loss: 0.725234  [53005/60000]\n",
      "loss: 0.558475  [53505/60000]\n",
      "loss: 0.311826  [54005/60000]\n",
      "loss: 0.021609  [54505/60000]\n",
      "loss: 0.620245  [55005/60000]\n",
      "loss: 0.034075  [55505/60000]\n",
      "loss: 0.204681  [56005/60000]\n",
      "loss: 0.344700  [56505/60000]\n",
      "loss: 0.752771  [57005/60000]\n",
      "loss: 0.135790  [57505/60000]\n",
      "loss: 0.036768  [58005/60000]\n",
      "loss: 0.042785  [58505/60000]\n",
      "loss: 0.197953  [59005/60000]\n",
      "loss: 0.382399  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.385925 \n",
      "\n",
      "time: 22.371551513671875\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "loss: 0.750257  [    5/60000]\n",
      "loss: 1.333588  [  505/60000]\n",
      "loss: 0.095878  [ 1005/60000]\n",
      "loss: 0.096177  [ 1505/60000]\n",
      "loss: 0.096808  [ 2005/60000]\n",
      "loss: 0.727123  [ 2505/60000]\n",
      "loss: 0.795601  [ 3005/60000]\n",
      "loss: 0.453237  [ 3505/60000]\n",
      "loss: 0.036193  [ 4005/60000]\n",
      "loss: 0.471184  [ 4505/60000]\n",
      "loss: 0.145052  [ 5005/60000]\n",
      "loss: 1.095331  [ 5505/60000]\n",
      "loss: 0.746782  [ 6005/60000]\n",
      "loss: 0.635579  [ 6505/60000]\n",
      "loss: 0.617093  [ 7005/60000]\n",
      "loss: 0.043117  [ 7505/60000]\n",
      "loss: 0.116928  [ 8005/60000]\n",
      "loss: 0.023459  [ 8505/60000]\n",
      "loss: 0.066679  [ 9005/60000]\n",
      "loss: 0.401298  [ 9505/60000]\n",
      "loss: 0.241117  [10005/60000]\n",
      "loss: 0.063664  [10505/60000]\n",
      "loss: 0.189590  [11005/60000]\n",
      "loss: 0.059947  [11505/60000]\n",
      "loss: 0.459602  [12005/60000]\n",
      "loss: 0.500378  [12505/60000]\n",
      "loss: 0.237100  [13005/60000]\n",
      "loss: 0.322988  [13505/60000]\n",
      "loss: 0.031080  [14005/60000]\n",
      "loss: 0.192796  [14505/60000]\n",
      "loss: 0.060511  [15005/60000]\n",
      "loss: 0.155229  [15505/60000]\n",
      "loss: 2.031370  [16005/60000]\n",
      "loss: 0.039141  [16505/60000]\n",
      "loss: 0.041693  [17005/60000]\n",
      "loss: 0.177632  [17505/60000]\n",
      "loss: 0.968492  [18005/60000]\n",
      "loss: 0.059182  [18505/60000]\n",
      "loss: 0.543492  [19005/60000]\n",
      "loss: 0.336416  [19505/60000]\n",
      "loss: 0.061924  [20005/60000]\n",
      "loss: 0.204535  [20505/60000]\n",
      "loss: 0.025563  [21005/60000]\n",
      "loss: 0.190800  [21505/60000]\n",
      "loss: 0.419021  [22005/60000]\n",
      "loss: 1.017122  [22505/60000]\n",
      "loss: 0.208576  [23005/60000]\n",
      "loss: 0.171962  [23505/60000]\n",
      "loss: 0.347071  [24005/60000]\n",
      "loss: 0.046431  [24505/60000]\n",
      "loss: 0.504343  [25005/60000]\n",
      "loss: 1.245352  [25505/60000]\n",
      "loss: 0.196669  [26005/60000]\n",
      "loss: 0.315339  [26505/60000]\n",
      "loss: 0.048858  [27005/60000]\n",
      "loss: 0.390452  [27505/60000]\n",
      "loss: 0.627047  [28005/60000]\n",
      "loss: 0.455406  [28505/60000]\n",
      "loss: 0.152615  [29005/60000]\n",
      "loss: 1.471727  [29505/60000]\n",
      "loss: 0.354108  [30005/60000]\n",
      "loss: 0.050718  [30505/60000]\n",
      "loss: 0.117303  [31005/60000]\n",
      "loss: 0.337203  [31505/60000]\n",
      "loss: 0.276510  [32005/60000]\n",
      "loss: 0.127123  [32505/60000]\n",
      "loss: 0.017432  [33005/60000]\n",
      "loss: 0.921576  [33505/60000]\n",
      "loss: 0.018058  [34005/60000]\n",
      "loss: 0.055711  [34505/60000]\n",
      "loss: 0.283466  [35005/60000]\n",
      "loss: 0.254330  [35505/60000]\n",
      "loss: 0.076438  [36005/60000]\n",
      "loss: 0.544441  [36505/60000]\n",
      "loss: 0.041573  [37005/60000]\n",
      "loss: 0.672748  [37505/60000]\n",
      "loss: 0.425336  [38005/60000]\n",
      "loss: 0.024687  [38505/60000]\n",
      "loss: 0.932600  [39005/60000]\n",
      "loss: 0.065374  [39505/60000]\n",
      "loss: 0.724355  [40005/60000]\n",
      "loss: 0.065250  [40505/60000]\n",
      "loss: 0.054629  [41005/60000]\n",
      "loss: 0.068048  [41505/60000]\n",
      "loss: 0.087656  [42005/60000]\n",
      "loss: 0.431436  [42505/60000]\n",
      "loss: 0.215925  [43005/60000]\n",
      "loss: 0.113290  [43505/60000]\n",
      "loss: 0.272241  [44005/60000]\n",
      "loss: 0.273260  [44505/60000]\n",
      "loss: 0.618738  [45005/60000]\n",
      "loss: 0.182730  [45505/60000]\n",
      "loss: 0.444393  [46005/60000]\n",
      "loss: 0.058952  [46505/60000]\n",
      "loss: 0.166998  [47005/60000]\n",
      "loss: 0.424721  [47505/60000]\n",
      "loss: 0.055306  [48005/60000]\n",
      "loss: 0.097036  [48505/60000]\n",
      "loss: 0.404425  [49005/60000]\n",
      "loss: 0.112368  [49505/60000]\n",
      "loss: 0.217239  [50005/60000]\n",
      "loss: 0.127553  [50505/60000]\n",
      "loss: 0.602968  [51005/60000]\n",
      "loss: 0.060070  [51505/60000]\n",
      "loss: 0.091730  [52005/60000]\n",
      "loss: 0.973634  [52505/60000]\n",
      "loss: 0.712312  [53005/60000]\n",
      "loss: 0.537562  [53505/60000]\n",
      "loss: 0.296031  [54005/60000]\n",
      "loss: 0.020889  [54505/60000]\n",
      "loss: 0.612867  [55005/60000]\n",
      "loss: 0.033713  [55505/60000]\n",
      "loss: 0.188586  [56005/60000]\n",
      "loss: 0.327454  [56505/60000]\n",
      "loss: 0.762115  [57005/60000]\n",
      "loss: 0.126308  [57505/60000]\n",
      "loss: 0.035443  [58005/60000]\n",
      "loss: 0.040620  [58505/60000]\n",
      "loss: 0.196452  [59005/60000]\n",
      "loss: 0.385974  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.381988 \n",
      "\n",
      "time: 22.915062189102173\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "loss: 0.726172  [    5/60000]\n",
      "loss: 1.348823  [  505/60000]\n",
      "loss: 0.096545  [ 1005/60000]\n",
      "loss: 0.093474  [ 1505/60000]\n",
      "loss: 0.093954  [ 2005/60000]\n",
      "loss: 0.719210  [ 2505/60000]\n",
      "loss: 0.792636  [ 3005/60000]\n",
      "loss: 0.434712  [ 3505/60000]\n",
      "loss: 0.035906  [ 4005/60000]\n",
      "loss: 0.454252  [ 4505/60000]\n",
      "loss: 0.145262  [ 5005/60000]\n",
      "loss: 1.051137  [ 5505/60000]\n",
      "loss: 0.750580  [ 6005/60000]\n",
      "loss: 0.631960  [ 6505/60000]\n",
      "loss: 0.596012  [ 7005/60000]\n",
      "loss: 0.042461  [ 7505/60000]\n",
      "loss: 0.114349  [ 8005/60000]\n",
      "loss: 0.024133  [ 8505/60000]\n",
      "loss: 0.064641  [ 9005/60000]\n",
      "loss: 0.392491  [ 9505/60000]\n",
      "loss: 0.229473  [10005/60000]\n",
      "loss: 0.061454  [10505/60000]\n",
      "loss: 0.181509  [11005/60000]\n",
      "loss: 0.056460  [11505/60000]\n",
      "loss: 0.437959  [12005/60000]\n",
      "loss: 0.479493  [12505/60000]\n",
      "loss: 0.221394  [13005/60000]\n",
      "loss: 0.315871  [13505/60000]\n",
      "loss: 0.029139  [14005/60000]\n",
      "loss: 0.183052  [14505/60000]\n",
      "loss: 0.055070  [15005/60000]\n",
      "loss: 0.151848  [15505/60000]\n",
      "loss: 2.036938  [16005/60000]\n",
      "loss: 0.038475  [16505/60000]\n",
      "loss: 0.040526  [17005/60000]\n",
      "loss: 0.169988  [17505/60000]\n",
      "loss: 0.962672  [18005/60000]\n",
      "loss: 0.053530  [18505/60000]\n",
      "loss: 0.543374  [19005/60000]\n",
      "loss: 0.331821  [19505/60000]\n",
      "loss: 0.061697  [20005/60000]\n",
      "loss: 0.198464  [20505/60000]\n",
      "loss: 0.024643  [21005/60000]\n",
      "loss: 0.197170  [21505/60000]\n",
      "loss: 0.412416  [22005/60000]\n",
      "loss: 0.979098  [22505/60000]\n",
      "loss: 0.206707  [23005/60000]\n",
      "loss: 0.169901  [23505/60000]\n",
      "loss: 0.351281  [24005/60000]\n",
      "loss: 0.043076  [24505/60000]\n",
      "loss: 0.498636  [25005/60000]\n",
      "loss: 1.234629  [25505/60000]\n",
      "loss: 0.186073  [26005/60000]\n",
      "loss: 0.313093  [26505/60000]\n",
      "loss: 0.047659  [27005/60000]\n",
      "loss: 0.370720  [27505/60000]\n",
      "loss: 0.609195  [28005/60000]\n",
      "loss: 0.455512  [28505/60000]\n",
      "loss: 0.144592  [29005/60000]\n",
      "loss: 1.403733  [29505/60000]\n",
      "loss: 0.357263  [30005/60000]\n",
      "loss: 0.048373  [30505/60000]\n",
      "loss: 0.116678  [31005/60000]\n",
      "loss: 0.351807  [31505/60000]\n",
      "loss: 0.258733  [32005/60000]\n",
      "loss: 0.121806  [32505/60000]\n",
      "loss: 0.016529  [33005/60000]\n",
      "loss: 0.912165  [33505/60000]\n",
      "loss: 0.017554  [34005/60000]\n",
      "loss: 0.052804  [34505/60000]\n",
      "loss: 0.274385  [35005/60000]\n",
      "loss: 0.247680  [35505/60000]\n",
      "loss: 0.073471  [36005/60000]\n",
      "loss: 0.547827  [36505/60000]\n",
      "loss: 0.040493  [37005/60000]\n",
      "loss: 0.654054  [37505/60000]\n",
      "loss: 0.423776  [38005/60000]\n",
      "loss: 0.023599  [38505/60000]\n",
      "loss: 0.938951  [39005/60000]\n",
      "loss: 0.063674  [39505/60000]\n",
      "loss: 0.710942  [40005/60000]\n",
      "loss: 0.062518  [40505/60000]\n",
      "loss: 0.054366  [41005/60000]\n",
      "loss: 0.064462  [41505/60000]\n",
      "loss: 0.089461  [42005/60000]\n",
      "loss: 0.401866  [42505/60000]\n",
      "loss: 0.214965  [43005/60000]\n",
      "loss: 0.116379  [43505/60000]\n",
      "loss: 0.263496  [44005/60000]\n",
      "loss: 0.259489  [44505/60000]\n",
      "loss: 0.615003  [45005/60000]\n",
      "loss: 0.172600  [45505/60000]\n",
      "loss: 0.430849  [46005/60000]\n",
      "loss: 0.056071  [46505/60000]\n",
      "loss: 0.166211  [47005/60000]\n",
      "loss: 0.428170  [47505/60000]\n",
      "loss: 0.049446  [48005/60000]\n",
      "loss: 0.097085  [48505/60000]\n",
      "loss: 0.396864  [49005/60000]\n",
      "loss: 0.104106  [49505/60000]\n",
      "loss: 0.222712  [50005/60000]\n",
      "loss: 0.121733  [50505/60000]\n",
      "loss: 0.599312  [51005/60000]\n",
      "loss: 0.056891  [51505/60000]\n",
      "loss: 0.090959  [52005/60000]\n",
      "loss: 0.920355  [52505/60000]\n",
      "loss: 0.696858  [53005/60000]\n",
      "loss: 0.528544  [53505/60000]\n",
      "loss: 0.283338  [54005/60000]\n",
      "loss: 0.020350  [54505/60000]\n",
      "loss: 0.600399  [55005/60000]\n",
      "loss: 0.033430  [55505/60000]\n",
      "loss: 0.171890  [56005/60000]\n",
      "loss: 0.311360  [56505/60000]\n",
      "loss: 0.772981  [57005/60000]\n",
      "loss: 0.117535  [57505/60000]\n",
      "loss: 0.034455  [58005/60000]\n",
      "loss: 0.038545  [58505/60000]\n",
      "loss: 0.199252  [59005/60000]\n",
      "loss: 0.394828  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.378104 \n",
      "\n",
      "time: 24.11550998687744\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "loss: 0.693481  [    5/60000]\n",
      "loss: 1.340039  [  505/60000]\n",
      "loss: 0.098920  [ 1005/60000]\n",
      "loss: 0.089429  [ 1505/60000]\n",
      "loss: 0.092989  [ 2005/60000]\n",
      "loss: 0.717717  [ 2505/60000]\n",
      "loss: 0.793903  [ 3005/60000]\n",
      "loss: 0.406878  [ 3505/60000]\n",
      "loss: 0.035591  [ 4005/60000]\n",
      "loss: 0.435206  [ 4505/60000]\n",
      "loss: 0.144724  [ 5005/60000]\n",
      "loss: 1.009194  [ 5505/60000]\n",
      "loss: 0.740774  [ 6005/60000]\n",
      "loss: 0.623494  [ 6505/60000]\n",
      "loss: 0.575019  [ 7005/60000]\n",
      "loss: 0.041080  [ 7505/60000]\n",
      "loss: 0.109056  [ 8005/60000]\n",
      "loss: 0.024208  [ 8505/60000]\n",
      "loss: 0.062409  [ 9005/60000]\n",
      "loss: 0.383043  [ 9505/60000]\n",
      "loss: 0.218922  [10005/60000]\n",
      "loss: 0.059268  [10505/60000]\n",
      "loss: 0.176370  [11005/60000]\n",
      "loss: 0.053056  [11505/60000]\n",
      "loss: 0.429723  [12005/60000]\n",
      "loss: 0.462684  [12505/60000]\n",
      "loss: 0.205435  [13005/60000]\n",
      "loss: 0.311367  [13505/60000]\n",
      "loss: 0.027629  [14005/60000]\n",
      "loss: 0.172650  [14505/60000]\n",
      "loss: 0.050742  [15005/60000]\n",
      "loss: 0.161000  [15505/60000]\n",
      "loss: 2.044097  [16005/60000]\n",
      "loss: 0.037639  [16505/60000]\n",
      "loss: 0.040060  [17005/60000]\n",
      "loss: 0.162273  [17505/60000]\n",
      "loss: 0.954003  [18005/60000]\n",
      "loss: 0.047230  [18505/60000]\n",
      "loss: 0.532967  [19005/60000]\n",
      "loss: 0.335985  [19505/60000]\n",
      "loss: 0.062982  [20005/60000]\n",
      "loss: 0.190005  [20505/60000]\n",
      "loss: 0.024633  [21005/60000]\n",
      "loss: 0.210303  [21505/60000]\n",
      "loss: 0.387150  [22005/60000]\n",
      "loss: 0.934766  [22505/60000]\n",
      "loss: 0.198756  [23005/60000]\n",
      "loss: 0.165018  [23505/60000]\n",
      "loss: 0.358944  [24005/60000]\n",
      "loss: 0.040984  [24505/60000]\n",
      "loss: 0.484154  [25005/60000]\n",
      "loss: 1.216937  [25505/60000]\n",
      "loss: 0.178825  [26005/60000]\n",
      "loss: 0.311169  [26505/60000]\n",
      "loss: 0.046858  [27005/60000]\n",
      "loss: 0.358728  [27505/60000]\n",
      "loss: 0.594595  [28005/60000]\n",
      "loss: 0.454182  [28505/60000]\n",
      "loss: 0.138051  [29005/60000]\n",
      "loss: 1.333374  [29505/60000]\n",
      "loss: 0.355411  [30005/60000]\n",
      "loss: 0.046845  [30505/60000]\n",
      "loss: 0.116495  [31005/60000]\n",
      "loss: 0.347841  [31505/60000]\n",
      "loss: 0.246183  [32005/60000]\n",
      "loss: 0.117834  [32505/60000]\n",
      "loss: 0.015666  [33005/60000]\n",
      "loss: 0.894485  [33505/60000]\n",
      "loss: 0.016387  [34005/60000]\n",
      "loss: 0.047890  [34505/60000]\n",
      "loss: 0.267431  [35005/60000]\n",
      "loss: 0.241582  [35505/60000]\n",
      "loss: 0.068263  [36005/60000]\n",
      "loss: 0.545652  [36505/60000]\n",
      "loss: 0.039679  [37005/60000]\n",
      "loss: 0.622480  [37505/60000]\n",
      "loss: 0.422488  [38005/60000]\n",
      "loss: 0.022187  [38505/60000]\n",
      "loss: 0.943810  [39005/60000]\n",
      "loss: 0.063275  [39505/60000]\n",
      "loss: 0.696422  [40005/60000]\n",
      "loss: 0.059076  [40505/60000]\n",
      "loss: 0.053494  [41005/60000]\n",
      "loss: 0.060312  [41505/60000]\n",
      "loss: 0.091612  [42005/60000]\n",
      "loss: 0.369372  [42505/60000]\n",
      "loss: 0.211655  [43005/60000]\n",
      "loss: 0.117928  [43505/60000]\n",
      "loss: 0.255211  [44005/60000]\n",
      "loss: 0.249153  [44505/60000]\n",
      "loss: 0.608466  [45005/60000]\n",
      "loss: 0.162971  [45505/60000]\n",
      "loss: 0.428728  [46005/60000]\n",
      "loss: 0.054134  [46505/60000]\n",
      "loss: 0.164707  [47005/60000]\n",
      "loss: 0.434892  [47505/60000]\n",
      "loss: 0.044896  [48005/60000]\n",
      "loss: 0.097167  [48505/60000]\n",
      "loss: 0.394531  [49005/60000]\n",
      "loss: 0.098310  [49505/60000]\n",
      "loss: 0.223124  [50005/60000]\n",
      "loss: 0.114609  [50505/60000]\n",
      "loss: 0.597272  [51005/60000]\n",
      "loss: 0.052385  [51505/60000]\n",
      "loss: 0.090451  [52005/60000]\n",
      "loss: 0.876172  [52505/60000]\n",
      "loss: 0.683945  [53005/60000]\n",
      "loss: 0.506231  [53505/60000]\n",
      "loss: 0.268445  [54005/60000]\n",
      "loss: 0.020042  [54505/60000]\n",
      "loss: 0.593190  [55005/60000]\n",
      "loss: 0.032096  [55505/60000]\n",
      "loss: 0.158396  [56005/60000]\n",
      "loss: 0.293457  [56505/60000]\n",
      "loss: 0.778763  [57005/60000]\n",
      "loss: 0.109768  [57505/60000]\n",
      "loss: 0.033982  [58005/60000]\n",
      "loss: 0.036963  [58505/60000]\n",
      "loss: 0.202085  [59005/60000]\n",
      "loss: 0.397674  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.374562 \n",
      "\n",
      "time: 22.436233520507812\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "loss: 0.663729  [    5/60000]\n",
      "loss: 1.354811  [  505/60000]\n",
      "loss: 0.099857  [ 1005/60000]\n",
      "loss: 0.084868  [ 1505/60000]\n",
      "loss: 0.091305  [ 2005/60000]\n",
      "loss: 0.711234  [ 2505/60000]\n",
      "loss: 0.793441  [ 3005/60000]\n",
      "loss: 0.385452  [ 3505/60000]\n",
      "loss: 0.034473  [ 4005/60000]\n",
      "loss: 0.421440  [ 4505/60000]\n",
      "loss: 0.146387  [ 5005/60000]\n",
      "loss: 0.968789  [ 5505/60000]\n",
      "loss: 0.736109  [ 6005/60000]\n",
      "loss: 0.618171  [ 6505/60000]\n",
      "loss: 0.558820  [ 7005/60000]\n",
      "loss: 0.040576  [ 7505/60000]\n",
      "loss: 0.106381  [ 8005/60000]\n",
      "loss: 0.025258  [ 8505/60000]\n",
      "loss: 0.059896  [ 9005/60000]\n",
      "loss: 0.372012  [ 9505/60000]\n",
      "loss: 0.205410  [10005/60000]\n",
      "loss: 0.056162  [10505/60000]\n",
      "loss: 0.169681  [11005/60000]\n",
      "loss: 0.050732  [11505/60000]\n",
      "loss: 0.407921  [12005/60000]\n",
      "loss: 0.438406  [12505/60000]\n",
      "loss: 0.196948  [13005/60000]\n",
      "loss: 0.305682  [13505/60000]\n",
      "loss: 0.025942  [14005/60000]\n",
      "loss: 0.162251  [14505/60000]\n",
      "loss: 0.046541  [15005/60000]\n",
      "loss: 0.159139  [15505/60000]\n",
      "loss: 2.044567  [16005/60000]\n",
      "loss: 0.036076  [16505/60000]\n",
      "loss: 0.038823  [17005/60000]\n",
      "loss: 0.154303  [17505/60000]\n",
      "loss: 0.960901  [18005/60000]\n",
      "loss: 0.041647  [18505/60000]\n",
      "loss: 0.534313  [19005/60000]\n",
      "loss: 0.333512  [19505/60000]\n",
      "loss: 0.062359  [20005/60000]\n",
      "loss: 0.186884  [20505/60000]\n",
      "loss: 0.024599  [21005/60000]\n",
      "loss: 0.215704  [21505/60000]\n",
      "loss: 0.375731  [22005/60000]\n",
      "loss: 0.905657  [22505/60000]\n",
      "loss: 0.191749  [23005/60000]\n",
      "loss: 0.162389  [23505/60000]\n",
      "loss: 0.364075  [24005/60000]\n",
      "loss: 0.039019  [24505/60000]\n",
      "loss: 0.465234  [25005/60000]\n",
      "loss: 1.198678  [25505/60000]\n",
      "loss: 0.172442  [26005/60000]\n",
      "loss: 0.313034  [26505/60000]\n",
      "loss: 0.045888  [27005/60000]\n",
      "loss: 0.343041  [27505/60000]\n",
      "loss: 0.578805  [28005/60000]\n",
      "loss: 0.465305  [28505/60000]\n",
      "loss: 0.134229  [29005/60000]\n",
      "loss: 1.263139  [29505/60000]\n",
      "loss: 0.352617  [30005/60000]\n",
      "loss: 0.045223  [30505/60000]\n",
      "loss: 0.115514  [31005/60000]\n",
      "loss: 0.355139  [31505/60000]\n",
      "loss: 0.235905  [32005/60000]\n",
      "loss: 0.113301  [32505/60000]\n",
      "loss: 0.014523  [33005/60000]\n",
      "loss: 0.885155  [33505/60000]\n",
      "loss: 0.015878  [34005/60000]\n",
      "loss: 0.047479  [34505/60000]\n",
      "loss: 0.258926  [35005/60000]\n",
      "loss: 0.232791  [35505/60000]\n",
      "loss: 0.063715  [36005/60000]\n",
      "loss: 0.546912  [36505/60000]\n",
      "loss: 0.038760  [37005/60000]\n",
      "loss: 0.595268  [37505/60000]\n",
      "loss: 0.421694  [38005/60000]\n",
      "loss: 0.020818  [38505/60000]\n",
      "loss: 0.943748  [39005/60000]\n",
      "loss: 0.061863  [39505/60000]\n",
      "loss: 0.676442  [40005/60000]\n",
      "loss: 0.056018  [40505/60000]\n",
      "loss: 0.053297  [41005/60000]\n",
      "loss: 0.056926  [41505/60000]\n",
      "loss: 0.093081  [42005/60000]\n",
      "loss: 0.344283  [42505/60000]\n",
      "loss: 0.211952  [43005/60000]\n",
      "loss: 0.119133  [43505/60000]\n",
      "loss: 0.245198  [44005/60000]\n",
      "loss: 0.242023  [44505/60000]\n",
      "loss: 0.605628  [45005/60000]\n",
      "loss: 0.153875  [45505/60000]\n",
      "loss: 0.410677  [46005/60000]\n",
      "loss: 0.052101  [46505/60000]\n",
      "loss: 0.162127  [47005/60000]\n",
      "loss: 0.445008  [47505/60000]\n",
      "loss: 0.040019  [48005/60000]\n",
      "loss: 0.097940  [48505/60000]\n",
      "loss: 0.387852  [49005/60000]\n",
      "loss: 0.093704  [49505/60000]\n",
      "loss: 0.225834  [50005/60000]\n",
      "loss: 0.107093  [50505/60000]\n",
      "loss: 0.595021  [51005/60000]\n",
      "loss: 0.052699  [51505/60000]\n",
      "loss: 0.087243  [52005/60000]\n",
      "loss: 0.830882  [52505/60000]\n",
      "loss: 0.678210  [53005/60000]\n",
      "loss: 0.502252  [53505/60000]\n",
      "loss: 0.251291  [54005/60000]\n",
      "loss: 0.019406  [54505/60000]\n",
      "loss: 0.584892  [55005/60000]\n",
      "loss: 0.031146  [55505/60000]\n",
      "loss: 0.145442  [56005/60000]\n",
      "loss: 0.278343  [56505/60000]\n",
      "loss: 0.791963  [57005/60000]\n",
      "loss: 0.102326  [57505/60000]\n",
      "loss: 0.033326  [58005/60000]\n",
      "loss: 0.035328  [58505/60000]\n",
      "loss: 0.204066  [59005/60000]\n",
      "loss: 0.401882  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.371388 \n",
      "\n",
      "time: 22.528565645217896\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "loss: 0.635775  [    5/60000]\n",
      "loss: 1.374208  [  505/60000]\n",
      "loss: 0.101465  [ 1005/60000]\n",
      "loss: 0.083448  [ 1505/60000]\n",
      "loss: 0.088573  [ 2005/60000]\n",
      "loss: 0.700908  [ 2505/60000]\n",
      "loss: 0.778191  [ 3005/60000]\n",
      "loss: 0.364975  [ 3505/60000]\n",
      "loss: 0.034692  [ 4005/60000]\n",
      "loss: 0.411923  [ 4505/60000]\n",
      "loss: 0.146973  [ 5005/60000]\n",
      "loss: 0.925912  [ 5505/60000]\n",
      "loss: 0.729344  [ 6005/60000]\n",
      "loss: 0.616498  [ 6505/60000]\n",
      "loss: 0.542217  [ 7005/60000]\n",
      "loss: 0.039385  [ 7505/60000]\n",
      "loss: 0.107291  [ 8005/60000]\n",
      "loss: 0.025332  [ 8505/60000]\n",
      "loss: 0.057812  [ 9005/60000]\n",
      "loss: 0.367285  [ 9505/60000]\n",
      "loss: 0.194400  [10005/60000]\n",
      "loss: 0.053822  [10505/60000]\n",
      "loss: 0.166280  [11005/60000]\n",
      "loss: 0.048810  [11505/60000]\n",
      "loss: 0.392783  [12005/60000]\n",
      "loss: 0.419163  [12505/60000]\n",
      "loss: 0.183452  [13005/60000]\n",
      "loss: 0.299208  [13505/60000]\n",
      "loss: 0.024448  [14005/60000]\n",
      "loss: 0.154859  [14505/60000]\n",
      "loss: 0.042762  [15005/60000]\n",
      "loss: 0.163336  [15505/60000]\n",
      "loss: 2.037334  [16005/60000]\n",
      "loss: 0.036179  [16505/60000]\n",
      "loss: 0.037944  [17005/60000]\n",
      "loss: 0.149224  [17505/60000]\n",
      "loss: 0.945057  [18005/60000]\n",
      "loss: 0.037131  [18505/60000]\n",
      "loss: 0.518681  [19005/60000]\n",
      "loss: 0.333055  [19505/60000]\n",
      "loss: 0.061584  [20005/60000]\n",
      "loss: 0.182096  [20505/60000]\n",
      "loss: 0.023412  [21005/60000]\n",
      "loss: 0.225624  [21505/60000]\n",
      "loss: 0.361658  [22005/60000]\n",
      "loss: 0.876060  [22505/60000]\n",
      "loss: 0.188682  [23005/60000]\n",
      "loss: 0.161198  [23505/60000]\n",
      "loss: 0.366051  [24005/60000]\n",
      "loss: 0.035854  [24505/60000]\n",
      "loss: 0.462936  [25005/60000]\n",
      "loss: 1.175620  [25505/60000]\n",
      "loss: 0.165392  [26005/60000]\n",
      "loss: 0.311045  [26505/60000]\n",
      "loss: 0.044571  [27005/60000]\n",
      "loss: 0.330801  [27505/60000]\n",
      "loss: 0.562915  [28005/60000]\n",
      "loss: 0.458767  [28505/60000]\n",
      "loss: 0.127722  [29005/60000]\n",
      "loss: 1.206289  [29505/60000]\n",
      "loss: 0.346326  [30005/60000]\n",
      "loss: 0.043767  [30505/60000]\n",
      "loss: 0.113154  [31005/60000]\n",
      "loss: 0.357904  [31505/60000]\n",
      "loss: 0.225158  [32005/60000]\n",
      "loss: 0.107586  [32505/60000]\n",
      "loss: 0.013746  [33005/60000]\n",
      "loss: 0.870147  [33505/60000]\n",
      "loss: 0.015555  [34005/60000]\n",
      "loss: 0.047256  [34505/60000]\n",
      "loss: 0.253523  [35005/60000]\n",
      "loss: 0.228342  [35505/60000]\n",
      "loss: 0.062525  [36005/60000]\n",
      "loss: 0.546977  [36505/60000]\n",
      "loss: 0.037835  [37005/60000]\n",
      "loss: 0.566726  [37505/60000]\n",
      "loss: 0.420623  [38005/60000]\n",
      "loss: 0.019520  [38505/60000]\n",
      "loss: 0.939574  [39005/60000]\n",
      "loss: 0.061653  [39505/60000]\n",
      "loss: 0.665000  [40005/60000]\n",
      "loss: 0.053325  [40505/60000]\n",
      "loss: 0.053594  [41005/60000]\n",
      "loss: 0.053693  [41505/60000]\n",
      "loss: 0.096124  [42005/60000]\n",
      "loss: 0.317954  [42505/60000]\n",
      "loss: 0.211748  [43005/60000]\n",
      "loss: 0.123073  [43505/60000]\n",
      "loss: 0.234182  [44005/60000]\n",
      "loss: 0.231242  [44505/60000]\n",
      "loss: 0.601287  [45005/60000]\n",
      "loss: 0.145037  [45505/60000]\n",
      "loss: 0.401244  [46005/60000]\n",
      "loss: 0.049694  [46505/60000]\n",
      "loss: 0.158845  [47005/60000]\n",
      "loss: 0.457377  [47505/60000]\n",
      "loss: 0.038056  [48005/60000]\n",
      "loss: 0.099384  [48505/60000]\n",
      "loss: 0.391103  [49005/60000]\n",
      "loss: 0.091467  [49505/60000]\n",
      "loss: 0.212739  [50005/60000]\n",
      "loss: 0.102821  [50505/60000]\n",
      "loss: 0.595044  [51005/60000]\n",
      "loss: 0.049813  [51505/60000]\n",
      "loss: 0.084823  [52005/60000]\n",
      "loss: 0.785332  [52505/60000]\n",
      "loss: 0.667425  [53005/60000]\n",
      "loss: 0.491093  [53505/60000]\n",
      "loss: 0.234353  [54005/60000]\n",
      "loss: 0.018864  [54505/60000]\n",
      "loss: 0.581924  [55005/60000]\n",
      "loss: 0.029968  [55505/60000]\n",
      "loss: 0.134706  [56005/60000]\n",
      "loss: 0.267495  [56505/60000]\n",
      "loss: 0.801283  [57005/60000]\n",
      "loss: 0.095980  [57505/60000]\n",
      "loss: 0.032821  [58005/60000]\n",
      "loss: 0.033190  [58505/60000]\n",
      "loss: 0.205763  [59005/60000]\n",
      "loss: 0.404806  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.368234 \n",
      "\n",
      "time: 22.311667680740356\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "loss: 0.611376  [    5/60000]\n",
      "loss: 1.368970  [  505/60000]\n",
      "loss: 0.102413  [ 1005/60000]\n",
      "loss: 0.081161  [ 1505/60000]\n",
      "loss: 0.087707  [ 2005/60000]\n",
      "loss: 0.688435  [ 2505/60000]\n",
      "loss: 0.777240  [ 3005/60000]\n",
      "loss: 0.349686  [ 3505/60000]\n",
      "loss: 0.033225  [ 4005/60000]\n",
      "loss: 0.402323  [ 4505/60000]\n",
      "loss: 0.149505  [ 5005/60000]\n",
      "loss: 0.877636  [ 5505/60000]\n",
      "loss: 0.734255  [ 6005/60000]\n",
      "loss: 0.604956  [ 6505/60000]\n",
      "loss: 0.529725  [ 7005/60000]\n",
      "loss: 0.039068  [ 7505/60000]\n",
      "loss: 0.103738  [ 8005/60000]\n",
      "loss: 0.026364  [ 8505/60000]\n",
      "loss: 0.055466  [ 9005/60000]\n",
      "loss: 0.353161  [ 9505/60000]\n",
      "loss: 0.184639  [10005/60000]\n",
      "loss: 0.051091  [10505/60000]\n",
      "loss: 0.160746  [11005/60000]\n",
      "loss: 0.046206  [11505/60000]\n",
      "loss: 0.376017  [12005/60000]\n",
      "loss: 0.399320  [12505/60000]\n",
      "loss: 0.178041  [13005/60000]\n",
      "loss: 0.294207  [13505/60000]\n",
      "loss: 0.022650  [14005/60000]\n",
      "loss: 0.143511  [14505/60000]\n",
      "loss: 0.039464  [15005/60000]\n",
      "loss: 0.166265  [15505/60000]\n",
      "loss: 2.055603  [16005/60000]\n",
      "loss: 0.035770  [16505/60000]\n",
      "loss: 0.036656  [17005/60000]\n",
      "loss: 0.144757  [17505/60000]\n",
      "loss: 0.952025  [18005/60000]\n",
      "loss: 0.033758  [18505/60000]\n",
      "loss: 0.519179  [19005/60000]\n",
      "loss: 0.334441  [19505/60000]\n",
      "loss: 0.062311  [20005/60000]\n",
      "loss: 0.179438  [20505/60000]\n",
      "loss: 0.022763  [21005/60000]\n",
      "loss: 0.238399  [21505/60000]\n",
      "loss: 0.345254  [22005/60000]\n",
      "loss: 0.839384  [22505/60000]\n",
      "loss: 0.181037  [23005/60000]\n",
      "loss: 0.158668  [23505/60000]\n",
      "loss: 0.364367  [24005/60000]\n",
      "loss: 0.034636  [24505/60000]\n",
      "loss: 0.453248  [25005/60000]\n",
      "loss: 1.153584  [25505/60000]\n",
      "loss: 0.159012  [26005/60000]\n",
      "loss: 0.309346  [26505/60000]\n",
      "loss: 0.044090  [27005/60000]\n",
      "loss: 0.323308  [27505/60000]\n",
      "loss: 0.546313  [28005/60000]\n",
      "loss: 0.462937  [28505/60000]\n",
      "loss: 0.122310  [29005/60000]\n",
      "loss: 1.139643  [29505/60000]\n",
      "loss: 0.337902  [30005/60000]\n",
      "loss: 0.041230  [30505/60000]\n",
      "loss: 0.109699  [31005/60000]\n",
      "loss: 0.357183  [31505/60000]\n",
      "loss: 0.220301  [32005/60000]\n",
      "loss: 0.106059  [32505/60000]\n",
      "loss: 0.012788  [33005/60000]\n",
      "loss: 0.858378  [33505/60000]\n",
      "loss: 0.014296  [34005/60000]\n",
      "loss: 0.043597  [34505/60000]\n",
      "loss: 0.244018  [35005/60000]\n",
      "loss: 0.221072  [35505/60000]\n",
      "loss: 0.060009  [36005/60000]\n",
      "loss: 0.549010  [36505/60000]\n",
      "loss: 0.037498  [37005/60000]\n",
      "loss: 0.538445  [37505/60000]\n",
      "loss: 0.421345  [38005/60000]\n",
      "loss: 0.018487  [38505/60000]\n",
      "loss: 0.926720  [39005/60000]\n",
      "loss: 0.059154  [39505/60000]\n",
      "loss: 0.650348  [40005/60000]\n",
      "loss: 0.051207  [40505/60000]\n",
      "loss: 0.054210  [41005/60000]\n",
      "loss: 0.051036  [41505/60000]\n",
      "loss: 0.098977  [42005/60000]\n",
      "loss: 0.303562  [42505/60000]\n",
      "loss: 0.209401  [43005/60000]\n",
      "loss: 0.122025  [43505/60000]\n",
      "loss: 0.224621  [44005/60000]\n",
      "loss: 0.226201  [44505/60000]\n",
      "loss: 0.598297  [45005/60000]\n",
      "loss: 0.137307  [45505/60000]\n",
      "loss: 0.393895  [46005/60000]\n",
      "loss: 0.047472  [46505/60000]\n",
      "loss: 0.153828  [47005/60000]\n",
      "loss: 0.462962  [47505/60000]\n",
      "loss: 0.034092  [48005/60000]\n",
      "loss: 0.099531  [48505/60000]\n",
      "loss: 0.382735  [49005/60000]\n",
      "loss: 0.088831  [49505/60000]\n",
      "loss: 0.209756  [50005/60000]\n",
      "loss: 0.099003  [50505/60000]\n",
      "loss: 0.593718  [51005/60000]\n",
      "loss: 0.048739  [51505/60000]\n",
      "loss: 0.085015  [52005/60000]\n",
      "loss: 0.742273  [52505/60000]\n",
      "loss: 0.660865  [53005/60000]\n",
      "loss: 0.479658  [53505/60000]\n",
      "loss: 0.226691  [54005/60000]\n",
      "loss: 0.018353  [54505/60000]\n",
      "loss: 0.573142  [55005/60000]\n",
      "loss: 0.028279  [55505/60000]\n",
      "loss: 0.121941  [56005/60000]\n",
      "loss: 0.257818  [56505/60000]\n",
      "loss: 0.809030  [57005/60000]\n",
      "loss: 0.091708  [57505/60000]\n",
      "loss: 0.032654  [58005/60000]\n",
      "loss: 0.031871  [58505/60000]\n",
      "loss: 0.205475  [59005/60000]\n",
      "loss: 0.406905  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.365061 \n",
      "\n",
      "time: 22.39083480834961\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "loss: 0.584867  [    5/60000]\n",
      "loss: 1.384644  [  505/60000]\n",
      "loss: 0.103123  [ 1005/60000]\n",
      "loss: 0.078452  [ 1505/60000]\n",
      "loss: 0.087381  [ 2005/60000]\n",
      "loss: 0.678303  [ 2505/60000]\n",
      "loss: 0.782764  [ 3005/60000]\n",
      "loss: 0.331062  [ 3505/60000]\n",
      "loss: 0.034140  [ 4005/60000]\n",
      "loss: 0.390508  [ 4505/60000]\n",
      "loss: 0.152486  [ 5005/60000]\n",
      "loss: 0.835119  [ 5505/60000]\n",
      "loss: 0.735155  [ 6005/60000]\n",
      "loss: 0.588217  [ 6505/60000]\n",
      "loss: 0.513605  [ 7005/60000]\n",
      "loss: 0.037682  [ 7505/60000]\n",
      "loss: 0.099943  [ 8005/60000]\n",
      "loss: 0.027450  [ 8505/60000]\n",
      "loss: 0.053460  [ 9005/60000]\n",
      "loss: 0.341960  [ 9505/60000]\n",
      "loss: 0.174884  [10005/60000]\n",
      "loss: 0.049605  [10505/60000]\n",
      "loss: 0.157188  [11005/60000]\n",
      "loss: 0.044403  [11505/60000]\n",
      "loss: 0.357442  [12005/60000]\n",
      "loss: 0.379520  [12505/60000]\n",
      "loss: 0.168192  [13005/60000]\n",
      "loss: 0.290982  [13505/60000]\n",
      "loss: 0.021622  [14005/60000]\n",
      "loss: 0.133212  [14505/60000]\n",
      "loss: 0.034680  [15005/60000]\n",
      "loss: 0.170215  [15505/60000]\n",
      "loss: 2.062758  [16005/60000]\n",
      "loss: 0.035518  [16505/60000]\n",
      "loss: 0.035899  [17005/60000]\n",
      "loss: 0.138202  [17505/60000]\n",
      "loss: 0.937234  [18005/60000]\n",
      "loss: 0.030142  [18505/60000]\n",
      "loss: 0.513503  [19005/60000]\n",
      "loss: 0.338078  [19505/60000]\n",
      "loss: 0.061255  [20005/60000]\n",
      "loss: 0.173285  [20505/60000]\n",
      "loss: 0.021982  [21005/60000]\n",
      "loss: 0.248239  [21505/60000]\n",
      "loss: 0.333503  [22005/60000]\n",
      "loss: 0.802750  [22505/60000]\n",
      "loss: 0.171555  [23005/60000]\n",
      "loss: 0.156447  [23505/60000]\n",
      "loss: 0.361792  [24005/60000]\n",
      "loss: 0.032885  [24505/60000]\n",
      "loss: 0.440998  [25005/60000]\n",
      "loss: 1.137463  [25505/60000]\n",
      "loss: 0.152322  [26005/60000]\n",
      "loss: 0.310321  [26505/60000]\n",
      "loss: 0.042987  [27005/60000]\n",
      "loss: 0.315473  [27505/60000]\n",
      "loss: 0.528811  [28005/60000]\n",
      "loss: 0.461825  [28505/60000]\n",
      "loss: 0.116864  [29005/60000]\n",
      "loss: 1.083631  [29505/60000]\n",
      "loss: 0.338532  [30005/60000]\n",
      "loss: 0.039263  [30505/60000]\n",
      "loss: 0.107680  [31005/60000]\n",
      "loss: 0.369625  [31505/60000]\n",
      "loss: 0.207514  [32005/60000]\n",
      "loss: 0.101448  [32505/60000]\n",
      "loss: 0.011945  [33005/60000]\n",
      "loss: 0.845907  [33505/60000]\n",
      "loss: 0.014360  [34005/60000]\n",
      "loss: 0.042023  [34505/60000]\n",
      "loss: 0.238267  [35005/60000]\n",
      "loss: 0.209913  [35505/60000]\n",
      "loss: 0.058811  [36005/60000]\n",
      "loss: 0.548302  [36505/60000]\n",
      "loss: 0.037088  [37005/60000]\n",
      "loss: 0.512508  [37505/60000]\n",
      "loss: 0.421127  [38005/60000]\n",
      "loss: 0.017375  [38505/60000]\n",
      "loss: 0.919062  [39005/60000]\n",
      "loss: 0.059232  [39505/60000]\n",
      "loss: 0.634859  [40005/60000]\n",
      "loss: 0.048721  [40505/60000]\n",
      "loss: 0.053912  [41005/60000]\n",
      "loss: 0.048707  [41505/60000]\n",
      "loss: 0.101777  [42005/60000]\n",
      "loss: 0.285113  [42505/60000]\n",
      "loss: 0.206284  [43005/60000]\n",
      "loss: 0.120197  [43505/60000]\n",
      "loss: 0.214055  [44005/60000]\n",
      "loss: 0.217845  [44505/60000]\n",
      "loss: 0.595147  [45005/60000]\n",
      "loss: 0.128478  [45505/60000]\n",
      "loss: 0.391661  [46005/60000]\n",
      "loss: 0.045294  [46505/60000]\n",
      "loss: 0.150709  [47005/60000]\n",
      "loss: 0.463931  [47505/60000]\n",
      "loss: 0.032626  [48005/60000]\n",
      "loss: 0.099662  [48505/60000]\n",
      "loss: 0.385216  [49005/60000]\n",
      "loss: 0.085516  [49505/60000]\n",
      "loss: 0.201222  [50005/60000]\n",
      "loss: 0.094125  [50505/60000]\n",
      "loss: 0.588521  [51005/60000]\n",
      "loss: 0.047019  [51505/60000]\n",
      "loss: 0.083132  [52005/60000]\n",
      "loss: 0.691379  [52505/60000]\n",
      "loss: 0.659158  [53005/60000]\n",
      "loss: 0.469737  [53505/60000]\n",
      "loss: 0.210640  [54005/60000]\n",
      "loss: 0.018607  [54505/60000]\n",
      "loss: 0.569725  [55005/60000]\n",
      "loss: 0.027040  [55505/60000]\n",
      "loss: 0.111462  [56005/60000]\n",
      "loss: 0.246128  [56505/60000]\n",
      "loss: 0.822776  [57005/60000]\n",
      "loss: 0.087492  [57505/60000]\n",
      "loss: 0.032382  [58005/60000]\n",
      "loss: 0.030534  [58505/60000]\n",
      "loss: 0.203081  [59005/60000]\n",
      "loss: 0.401848  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.362470 \n",
      "\n",
      "time: 23.02008843421936\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "loss: 0.562652  [    5/60000]\n",
      "loss: 1.377853  [  505/60000]\n",
      "loss: 0.098748  [ 1005/60000]\n",
      "loss: 0.074392  [ 1505/60000]\n",
      "loss: 0.086589  [ 2005/60000]\n",
      "loss: 0.669901  [ 2505/60000]\n",
      "loss: 0.762937  [ 3005/60000]\n",
      "loss: 0.306588  [ 3505/60000]\n",
      "loss: 0.033856  [ 4005/60000]\n",
      "loss: 0.388609  [ 4505/60000]\n",
      "loss: 0.153571  [ 5005/60000]\n",
      "loss: 0.789439  [ 5505/60000]\n",
      "loss: 0.724518  [ 6005/60000]\n",
      "loss: 0.577151  [ 6505/60000]\n",
      "loss: 0.501847  [ 7005/60000]\n",
      "loss: 0.036157  [ 7505/60000]\n",
      "loss: 0.097454  [ 8005/60000]\n",
      "loss: 0.028016  [ 8505/60000]\n",
      "loss: 0.051106  [ 9005/60000]\n",
      "loss: 0.333199  [ 9505/60000]\n",
      "loss: 0.165957  [10005/60000]\n",
      "loss: 0.045685  [10505/60000]\n",
      "loss: 0.153752  [11005/60000]\n",
      "loss: 0.044086  [11505/60000]\n",
      "loss: 0.346910  [12005/60000]\n",
      "loss: 0.367622  [12505/60000]\n",
      "loss: 0.157943  [13005/60000]\n",
      "loss: 0.288177  [13505/60000]\n",
      "loss: 0.020839  [14005/60000]\n",
      "loss: 0.125325  [14505/60000]\n",
      "loss: 0.032933  [15005/60000]\n",
      "loss: 0.178951  [15505/60000]\n",
      "loss: 2.075615  [16005/60000]\n",
      "loss: 0.035325  [16505/60000]\n",
      "loss: 0.033981  [17005/60000]\n",
      "loss: 0.132122  [17505/60000]\n",
      "loss: 0.937177  [18005/60000]\n",
      "loss: 0.026398  [18505/60000]\n",
      "loss: 0.517352  [19005/60000]\n",
      "loss: 0.335004  [19505/60000]\n",
      "loss: 0.063088  [20005/60000]\n",
      "loss: 0.172419  [20505/60000]\n",
      "loss: 0.021831  [21005/60000]\n",
      "loss: 0.255994  [21505/60000]\n",
      "loss: 0.326391  [22005/60000]\n",
      "loss: 0.765871  [22505/60000]\n",
      "loss: 0.162027  [23005/60000]\n",
      "loss: 0.159351  [23505/60000]\n",
      "loss: 0.365035  [24005/60000]\n",
      "loss: 0.032039  [24505/60000]\n",
      "loss: 0.435425  [25005/60000]\n",
      "loss: 1.116465  [25505/60000]\n",
      "loss: 0.143723  [26005/60000]\n",
      "loss: 0.311983  [26505/60000]\n",
      "loss: 0.041786  [27005/60000]\n",
      "loss: 0.302550  [27505/60000]\n",
      "loss: 0.513303  [28005/60000]\n",
      "loss: 0.466926  [28505/60000]\n",
      "loss: 0.111721  [29005/60000]\n",
      "loss: 1.014392  [29505/60000]\n",
      "loss: 0.327948  [30005/60000]\n",
      "loss: 0.037564  [30505/60000]\n",
      "loss: 0.100640  [31005/60000]\n",
      "loss: 0.369169  [31505/60000]\n",
      "loss: 0.200441  [32005/60000]\n",
      "loss: 0.097980  [32505/60000]\n",
      "loss: 0.010864  [33005/60000]\n",
      "loss: 0.835120  [33505/60000]\n",
      "loss: 0.013455  [34005/60000]\n",
      "loss: 0.039981  [34505/60000]\n",
      "loss: 0.230967  [35005/60000]\n",
      "loss: 0.205713  [35505/60000]\n",
      "loss: 0.058139  [36005/60000]\n",
      "loss: 0.536204  [36505/60000]\n",
      "loss: 0.037224  [37005/60000]\n",
      "loss: 0.490085  [37505/60000]\n",
      "loss: 0.420995  [38005/60000]\n",
      "loss: 0.016375  [38505/60000]\n",
      "loss: 0.920381  [39005/60000]\n",
      "loss: 0.056096  [39505/60000]\n",
      "loss: 0.622900  [40005/60000]\n",
      "loss: 0.046189  [40505/60000]\n",
      "loss: 0.053376  [41005/60000]\n",
      "loss: 0.046976  [41505/60000]\n",
      "loss: 0.102823  [42005/60000]\n",
      "loss: 0.267537  [42505/60000]\n",
      "loss: 0.203065  [43005/60000]\n",
      "loss: 0.121544  [43505/60000]\n",
      "loss: 0.206545  [44005/60000]\n",
      "loss: 0.212912  [44505/60000]\n",
      "loss: 0.592051  [45005/60000]\n",
      "loss: 0.122334  [45505/60000]\n",
      "loss: 0.383721  [46005/60000]\n",
      "loss: 0.043415  [46505/60000]\n",
      "loss: 0.147707  [47005/60000]\n",
      "loss: 0.469671  [47505/60000]\n",
      "loss: 0.030822  [48005/60000]\n",
      "loss: 0.099817  [48505/60000]\n",
      "loss: 0.379942  [49005/60000]\n",
      "loss: 0.081964  [49505/60000]\n",
      "loss: 0.201142  [50005/60000]\n",
      "loss: 0.089379  [50505/60000]\n",
      "loss: 0.580459  [51005/60000]\n",
      "loss: 0.044974  [51505/60000]\n",
      "loss: 0.082624  [52005/60000]\n",
      "loss: 0.648828  [52505/60000]\n",
      "loss: 0.650026  [53005/60000]\n",
      "loss: 0.461955  [53505/60000]\n",
      "loss: 0.199594  [54005/60000]\n",
      "loss: 0.018304  [54505/60000]\n",
      "loss: 0.560243  [55005/60000]\n",
      "loss: 0.025153  [55505/60000]\n",
      "loss: 0.103269  [56005/60000]\n",
      "loss: 0.238192  [56505/60000]\n",
      "loss: 0.831202  [57005/60000]\n",
      "loss: 0.083791  [57505/60000]\n",
      "loss: 0.032265  [58005/60000]\n",
      "loss: 0.029878  [58505/60000]\n",
      "loss: 0.205393  [59005/60000]\n",
      "loss: 0.409262  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.359858 \n",
      "\n",
      "time: 22.41850185394287\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "loss: 0.550609  [    5/60000]\n",
      "loss: 1.402588  [  505/60000]\n",
      "loss: 0.096457  [ 1005/60000]\n",
      "loss: 0.070931  [ 1505/60000]\n",
      "loss: 0.085182  [ 2005/60000]\n",
      "loss: 0.656279  [ 2505/60000]\n",
      "loss: 0.760778  [ 3005/60000]\n",
      "loss: 0.300178  [ 3505/60000]\n",
      "loss: 0.033439  [ 4005/60000]\n",
      "loss: 0.390124  [ 4505/60000]\n",
      "loss: 0.157521  [ 5005/60000]\n",
      "loss: 0.749427  [ 5505/60000]\n",
      "loss: 0.713917  [ 6005/60000]\n",
      "loss: 0.572172  [ 6505/60000]\n",
      "loss: 0.494321  [ 7005/60000]\n",
      "loss: 0.035109  [ 7505/60000]\n",
      "loss: 0.095117  [ 8005/60000]\n",
      "loss: 0.028895  [ 8505/60000]\n",
      "loss: 0.049077  [ 9005/60000]\n",
      "loss: 0.323041  [ 9505/60000]\n",
      "loss: 0.155929  [10005/60000]\n",
      "loss: 0.045259  [10505/60000]\n",
      "loss: 0.153223  [11005/60000]\n",
      "loss: 0.042956  [11505/60000]\n",
      "loss: 0.334212  [12005/60000]\n",
      "loss: 0.359467  [12505/60000]\n",
      "loss: 0.151524  [13005/60000]\n",
      "loss: 0.282273  [13505/60000]\n",
      "loss: 0.019850  [14005/60000]\n",
      "loss: 0.117973  [14505/60000]\n",
      "loss: 0.029352  [15005/60000]\n",
      "loss: 0.185954  [15505/60000]\n",
      "loss: 2.077081  [16005/60000]\n",
      "loss: 0.035817  [16505/60000]\n",
      "loss: 0.032792  [17005/60000]\n",
      "loss: 0.126593  [17505/60000]\n",
      "loss: 0.923182  [18005/60000]\n",
      "loss: 0.023591  [18505/60000]\n",
      "loss: 0.513394  [19005/60000]\n",
      "loss: 0.339335  [19505/60000]\n",
      "loss: 0.064766  [20005/60000]\n",
      "loss: 0.167922  [20505/60000]\n",
      "loss: 0.020736  [21005/60000]\n",
      "loss: 0.263952  [21505/60000]\n",
      "loss: 0.323341  [22005/60000]\n",
      "loss: 0.734906  [22505/60000]\n",
      "loss: 0.154951  [23005/60000]\n",
      "loss: 0.156384  [23505/60000]\n",
      "loss: 0.364504  [24005/60000]\n",
      "loss: 0.030016  [24505/60000]\n",
      "loss: 0.424795  [25005/60000]\n",
      "loss: 1.104449  [25505/60000]\n",
      "loss: 0.136265  [26005/60000]\n",
      "loss: 0.309193  [26505/60000]\n",
      "loss: 0.040471  [27005/60000]\n",
      "loss: 0.293988  [27505/60000]\n",
      "loss: 0.496500  [28005/60000]\n",
      "loss: 0.466325  [28505/60000]\n",
      "loss: 0.107345  [29005/60000]\n",
      "loss: 0.959083  [29505/60000]\n",
      "loss: 0.315949  [30005/60000]\n",
      "loss: 0.035959  [30505/60000]\n",
      "loss: 0.101089  [31005/60000]\n",
      "loss: 0.374306  [31505/60000]\n",
      "loss: 0.193938  [32005/60000]\n",
      "loss: 0.096449  [32505/60000]\n",
      "loss: 0.010001  [33005/60000]\n",
      "loss: 0.829032  [33505/60000]\n",
      "loss: 0.012882  [34005/60000]\n",
      "loss: 0.038565  [34505/60000]\n",
      "loss: 0.226878  [35005/60000]\n",
      "loss: 0.207542  [35505/60000]\n",
      "loss: 0.056126  [36005/60000]\n",
      "loss: 0.537256  [36505/60000]\n",
      "loss: 0.036429  [37005/60000]\n",
      "loss: 0.467522  [37505/60000]\n",
      "loss: 0.423481  [38005/60000]\n",
      "loss: 0.015294  [38505/60000]\n",
      "loss: 0.902832  [39005/60000]\n",
      "loss: 0.058658  [39505/60000]\n",
      "loss: 0.605445  [40005/60000]\n",
      "loss: 0.044646  [40505/60000]\n",
      "loss: 0.053826  [41005/60000]\n",
      "loss: 0.046297  [41505/60000]\n",
      "loss: 0.103387  [42005/60000]\n",
      "loss: 0.254414  [42505/60000]\n",
      "loss: 0.201669  [43005/60000]\n",
      "loss: 0.120420  [43505/60000]\n",
      "loss: 0.197497  [44005/60000]\n",
      "loss: 0.204775  [44505/60000]\n",
      "loss: 0.582840  [45005/60000]\n",
      "loss: 0.114911  [45505/60000]\n",
      "loss: 0.360202  [46005/60000]\n",
      "loss: 0.042171  [46505/60000]\n",
      "loss: 0.145819  [47005/60000]\n",
      "loss: 0.466778  [47505/60000]\n",
      "loss: 0.027196  [48005/60000]\n",
      "loss: 0.100069  [48505/60000]\n",
      "loss: 0.374304  [49005/60000]\n",
      "loss: 0.079251  [49505/60000]\n",
      "loss: 0.202490  [50005/60000]\n",
      "loss: 0.083407  [50505/60000]\n",
      "loss: 0.572872  [51005/60000]\n",
      "loss: 0.043895  [51505/60000]\n",
      "loss: 0.082909  [52005/60000]\n",
      "loss: 0.602708  [52505/60000]\n",
      "loss: 0.649238  [53005/60000]\n",
      "loss: 0.449569  [53505/60000]\n",
      "loss: 0.188096  [54005/60000]\n",
      "loss: 0.017922  [54505/60000]\n",
      "loss: 0.552719  [55005/60000]\n",
      "loss: 0.024020  [55505/60000]\n",
      "loss: 0.095423  [56005/60000]\n",
      "loss: 0.226061  [56505/60000]\n",
      "loss: 0.844875  [57005/60000]\n",
      "loss: 0.080568  [57505/60000]\n",
      "loss: 0.032720  [58005/60000]\n",
      "loss: 0.028982  [58505/60000]\n",
      "loss: 0.202189  [59005/60000]\n",
      "loss: 0.413879  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.357528 \n",
      "\n",
      "time: 22.304303407669067\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "loss: 0.533862  [    5/60000]\n",
      "loss: 1.406201  [  505/60000]\n",
      "loss: 0.093508  [ 1005/60000]\n",
      "loss: 0.066618  [ 1505/60000]\n",
      "loss: 0.084993  [ 2005/60000]\n",
      "loss: 0.640159  [ 2505/60000]\n",
      "loss: 0.764869  [ 3005/60000]\n",
      "loss: 0.281413  [ 3505/60000]\n",
      "loss: 0.031527  [ 4005/60000]\n",
      "loss: 0.379858  [ 4505/60000]\n",
      "loss: 0.161402  [ 5005/60000]\n",
      "loss: 0.708736  [ 5505/60000]\n",
      "loss: 0.713740  [ 6005/60000]\n",
      "loss: 0.552346  [ 6505/60000]\n",
      "loss: 0.486416  [ 7005/60000]\n",
      "loss: 0.033664  [ 7505/60000]\n",
      "loss: 0.094376  [ 8005/60000]\n",
      "loss: 0.029465  [ 8505/60000]\n",
      "loss: 0.047155  [ 9005/60000]\n",
      "loss: 0.313189  [ 9505/60000]\n",
      "loss: 0.145913  [10005/60000]\n",
      "loss: 0.043947  [10505/60000]\n",
      "loss: 0.149899  [11005/60000]\n",
      "loss: 0.040797  [11505/60000]\n",
      "loss: 0.317543  [12005/60000]\n",
      "loss: 0.347578  [12505/60000]\n",
      "loss: 0.143823  [13005/60000]\n",
      "loss: 0.277058  [13505/60000]\n",
      "loss: 0.018908  [14005/60000]\n",
      "loss: 0.111865  [14505/60000]\n",
      "loss: 0.026616  [15005/60000]\n",
      "loss: 0.189376  [15505/60000]\n",
      "loss: 2.099262  [16005/60000]\n",
      "loss: 0.035583  [16505/60000]\n",
      "loss: 0.031838  [17005/60000]\n",
      "loss: 0.121983  [17505/60000]\n",
      "loss: 0.905111  [18005/60000]\n",
      "loss: 0.021309  [18505/60000]\n",
      "loss: 0.509142  [19005/60000]\n",
      "loss: 0.336843  [19505/60000]\n",
      "loss: 0.063189  [20005/60000]\n",
      "loss: 0.163957  [20505/60000]\n",
      "loss: 0.020016  [21005/60000]\n",
      "loss: 0.272265  [21505/60000]\n",
      "loss: 0.312201  [22005/60000]\n",
      "loss: 0.684721  [22505/60000]\n",
      "loss: 0.146528  [23005/60000]\n",
      "loss: 0.155577  [23505/60000]\n",
      "loss: 0.358218  [24005/60000]\n",
      "loss: 0.028058  [24505/60000]\n",
      "loss: 0.413065  [25005/60000]\n",
      "loss: 1.088855  [25505/60000]\n",
      "loss: 0.130182  [26005/60000]\n",
      "loss: 0.309073  [26505/60000]\n",
      "loss: 0.039138  [27005/60000]\n",
      "loss: 0.285057  [27505/60000]\n",
      "loss: 0.487936  [28005/60000]\n",
      "loss: 0.455174  [28505/60000]\n",
      "loss: 0.102957  [29005/60000]\n",
      "loss: 0.908937  [29505/60000]\n",
      "loss: 0.301345  [30005/60000]\n",
      "loss: 0.034109  [30505/60000]\n",
      "loss: 0.096410  [31005/60000]\n",
      "loss: 0.379933  [31505/60000]\n",
      "loss: 0.189357  [32005/60000]\n",
      "loss: 0.093948  [32505/60000]\n",
      "loss: 0.009125  [33005/60000]\n",
      "loss: 0.818068  [33505/60000]\n",
      "loss: 0.012280  [34005/60000]\n",
      "loss: 0.036995  [34505/60000]\n",
      "loss: 0.219938  [35005/60000]\n",
      "loss: 0.206674  [35505/60000]\n",
      "loss: 0.056830  [36005/60000]\n",
      "loss: 0.523975  [36505/60000]\n",
      "loss: 0.036359  [37005/60000]\n",
      "loss: 0.442626  [37505/60000]\n",
      "loss: 0.423418  [38005/60000]\n",
      "loss: 0.014305  [38505/60000]\n",
      "loss: 0.893741  [39005/60000]\n",
      "loss: 0.056697  [39505/60000]\n",
      "loss: 0.588466  [40005/60000]\n",
      "loss: 0.043268  [40505/60000]\n",
      "loss: 0.054017  [41005/60000]\n",
      "loss: 0.044363  [41505/60000]\n",
      "loss: 0.104018  [42005/60000]\n",
      "loss: 0.240162  [42505/60000]\n",
      "loss: 0.201027  [43005/60000]\n",
      "loss: 0.119684  [43505/60000]\n",
      "loss: 0.189025  [44005/60000]\n",
      "loss: 0.197764  [44505/60000]\n",
      "loss: 0.579960  [45005/60000]\n",
      "loss: 0.108751  [45505/60000]\n",
      "loss: 0.343681  [46005/60000]\n",
      "loss: 0.040057  [46505/60000]\n",
      "loss: 0.140809  [47005/60000]\n",
      "loss: 0.476896  [47505/60000]\n",
      "loss: 0.026790  [48005/60000]\n",
      "loss: 0.100872  [48505/60000]\n",
      "loss: 0.369848  [49005/60000]\n",
      "loss: 0.075948  [49505/60000]\n",
      "loss: 0.198247  [50005/60000]\n",
      "loss: 0.078474  [50505/60000]\n",
      "loss: 0.558515  [51005/60000]\n",
      "loss: 0.042132  [51505/60000]\n",
      "loss: 0.078058  [52005/60000]\n",
      "loss: 0.568738  [52505/60000]\n",
      "loss: 0.642474  [53005/60000]\n",
      "loss: 0.433319  [53505/60000]\n",
      "loss: 0.178240  [54005/60000]\n",
      "loss: 0.017398  [54505/60000]\n",
      "loss: 0.555900  [55005/60000]\n",
      "loss: 0.021859  [55505/60000]\n",
      "loss: 0.087639  [56005/60000]\n",
      "loss: 0.219462  [56505/60000]\n",
      "loss: 0.854506  [57005/60000]\n",
      "loss: 0.078049  [57505/60000]\n",
      "loss: 0.032584  [58005/60000]\n",
      "loss: 0.028186  [58505/60000]\n",
      "loss: 0.202557  [59005/60000]\n",
      "loss: 0.403924  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.354682 \n",
      "\n",
      "time: 22.629294395446777\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "loss: 0.527500  [    5/60000]\n",
      "loss: 1.421463  [  505/60000]\n",
      "loss: 0.086841  [ 1005/60000]\n",
      "loss: 0.064808  [ 1505/60000]\n",
      "loss: 0.083468  [ 2005/60000]\n",
      "loss: 0.622593  [ 2505/60000]\n",
      "loss: 0.755287  [ 3005/60000]\n",
      "loss: 0.271012  [ 3505/60000]\n",
      "loss: 0.031364  [ 4005/60000]\n",
      "loss: 0.372726  [ 4505/60000]\n",
      "loss: 0.167745  [ 5005/60000]\n",
      "loss: 0.664966  [ 5505/60000]\n",
      "loss: 0.698543  [ 6005/60000]\n",
      "loss: 0.530707  [ 6505/60000]\n",
      "loss: 0.475044  [ 7005/60000]\n",
      "loss: 0.032983  [ 7505/60000]\n",
      "loss: 0.091174  [ 8005/60000]\n",
      "loss: 0.030072  [ 8505/60000]\n",
      "loss: 0.045402  [ 9005/60000]\n",
      "loss: 0.304487  [ 9505/60000]\n",
      "loss: 0.138004  [10005/60000]\n",
      "loss: 0.042294  [10505/60000]\n",
      "loss: 0.152194  [11005/60000]\n",
      "loss: 0.038968  [11505/60000]\n",
      "loss: 0.308151  [12005/60000]\n",
      "loss: 0.338731  [12505/60000]\n",
      "loss: 0.135664  [13005/60000]\n",
      "loss: 0.269400  [13505/60000]\n",
      "loss: 0.018156  [14005/60000]\n",
      "loss: 0.105882  [14505/60000]\n",
      "loss: 0.023824  [15005/60000]\n",
      "loss: 0.199349  [15505/60000]\n",
      "loss: 2.095320  [16005/60000]\n",
      "loss: 0.033660  [16505/60000]\n",
      "loss: 0.031139  [17005/60000]\n",
      "loss: 0.117917  [17505/60000]\n",
      "loss: 0.897098  [18005/60000]\n",
      "loss: 0.019224  [18505/60000]\n",
      "loss: 0.487539  [19005/60000]\n",
      "loss: 0.341336  [19505/60000]\n",
      "loss: 0.063630  [20005/60000]\n",
      "loss: 0.159909  [20505/60000]\n",
      "loss: 0.019670  [21005/60000]\n",
      "loss: 0.289154  [21505/60000]\n",
      "loss: 0.311410  [22005/60000]\n",
      "loss: 0.659123  [22505/60000]\n",
      "loss: 0.139091  [23005/60000]\n",
      "loss: 0.153692  [23505/60000]\n",
      "loss: 0.356122  [24005/60000]\n",
      "loss: 0.027478  [24505/60000]\n",
      "loss: 0.396048  [25005/60000]\n",
      "loss: 1.073867  [25505/60000]\n",
      "loss: 0.126945  [26005/60000]\n",
      "loss: 0.312327  [26505/60000]\n",
      "loss: 0.038291  [27005/60000]\n",
      "loss: 0.276990  [27505/60000]\n",
      "loss: 0.474796  [28005/60000]\n",
      "loss: 0.453460  [28505/60000]\n",
      "loss: 0.098730  [29005/60000]\n",
      "loss: 0.855117  [29505/60000]\n",
      "loss: 0.288682  [30005/60000]\n",
      "loss: 0.033261  [30505/60000]\n",
      "loss: 0.093920  [31005/60000]\n",
      "loss: 0.385929  [31505/60000]\n",
      "loss: 0.182083  [32005/60000]\n",
      "loss: 0.090251  [32505/60000]\n",
      "loss: 0.008639  [33005/60000]\n",
      "loss: 0.806507  [33505/60000]\n",
      "loss: 0.011828  [34005/60000]\n",
      "loss: 0.036380  [34505/60000]\n",
      "loss: 0.215992  [35005/60000]\n",
      "loss: 0.203026  [35505/60000]\n",
      "loss: 0.056288  [36005/60000]\n",
      "loss: 0.516986  [36505/60000]\n",
      "loss: 0.035941  [37005/60000]\n",
      "loss: 0.427959  [37505/60000]\n",
      "loss: 0.425765  [38005/60000]\n",
      "loss: 0.013571  [38505/60000]\n",
      "loss: 0.880983  [39005/60000]\n",
      "loss: 0.056251  [39505/60000]\n",
      "loss: 0.576400  [40005/60000]\n",
      "loss: 0.042657  [40505/60000]\n",
      "loss: 0.054300  [41005/60000]\n",
      "loss: 0.042568  [41505/60000]\n",
      "loss: 0.104853  [42005/60000]\n",
      "loss: 0.231776  [42505/60000]\n",
      "loss: 0.199404  [43005/60000]\n",
      "loss: 0.121322  [43505/60000]\n",
      "loss: 0.182531  [44005/60000]\n",
      "loss: 0.195639  [44505/60000]\n",
      "loss: 0.579023  [45005/60000]\n",
      "loss: 0.102918  [45505/60000]\n",
      "loss: 0.359972  [46005/60000]\n",
      "loss: 0.037585  [46505/60000]\n",
      "loss: 0.135326  [47005/60000]\n",
      "loss: 0.488438  [47505/60000]\n",
      "loss: 0.024224  [48005/60000]\n",
      "loss: 0.101366  [48505/60000]\n",
      "loss: 0.373363  [49005/60000]\n",
      "loss: 0.072106  [49505/60000]\n",
      "loss: 0.185635  [50005/60000]\n",
      "loss: 0.074672  [50505/60000]\n",
      "loss: 0.554143  [51005/60000]\n",
      "loss: 0.040402  [51505/60000]\n",
      "loss: 0.075726  [52005/60000]\n",
      "loss: 0.525775  [52505/60000]\n",
      "loss: 0.631616  [53005/60000]\n",
      "loss: 0.417830  [53505/60000]\n",
      "loss: 0.168257  [54005/60000]\n",
      "loss: 0.017123  [54505/60000]\n",
      "loss: 0.549185  [55005/60000]\n",
      "loss: 0.020000  [55505/60000]\n",
      "loss: 0.081656  [56005/60000]\n",
      "loss: 0.207906  [56505/60000]\n",
      "loss: 0.855792  [57005/60000]\n",
      "loss: 0.074942  [57505/60000]\n",
      "loss: 0.032898  [58005/60000]\n",
      "loss: 0.027956  [58505/60000]\n",
      "loss: 0.210676  [59005/60000]\n",
      "loss: 0.413456  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.352314 \n",
      "\n",
      "time: 22.576794147491455\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "loss: 0.507454  [    5/60000]\n",
      "loss: 1.425310  [  505/60000]\n",
      "loss: 0.089332  [ 1005/60000]\n",
      "loss: 0.062262  [ 1505/60000]\n",
      "loss: 0.082419  [ 2005/60000]\n",
      "loss: 0.614873  [ 2505/60000]\n",
      "loss: 0.761140  [ 3005/60000]\n",
      "loss: 0.253682  [ 3505/60000]\n",
      "loss: 0.031131  [ 4005/60000]\n",
      "loss: 0.372035  [ 4505/60000]\n",
      "loss: 0.166458  [ 5005/60000]\n",
      "loss: 0.623694  [ 5505/60000]\n",
      "loss: 0.698756  [ 6005/60000]\n",
      "loss: 0.505808  [ 6505/60000]\n",
      "loss: 0.468471  [ 7005/60000]\n",
      "loss: 0.031943  [ 7505/60000]\n",
      "loss: 0.088477  [ 8005/60000]\n",
      "loss: 0.030136  [ 8505/60000]\n",
      "loss: 0.043605  [ 9005/60000]\n",
      "loss: 0.295831  [ 9505/60000]\n",
      "loss: 0.131847  [10005/60000]\n",
      "loss: 0.040322  [10505/60000]\n",
      "loss: 0.149375  [11005/60000]\n",
      "loss: 0.037228  [11505/60000]\n",
      "loss: 0.295244  [12005/60000]\n",
      "loss: 0.325700  [12505/60000]\n",
      "loss: 0.129456  [13005/60000]\n",
      "loss: 0.262634  [13505/60000]\n",
      "loss: 0.017164  [14005/60000]\n",
      "loss: 0.095720  [14505/60000]\n",
      "loss: 0.022143  [15005/60000]\n",
      "loss: 0.201346  [15505/60000]\n",
      "loss: 2.118468  [16005/60000]\n",
      "loss: 0.032733  [16505/60000]\n",
      "loss: 0.029874  [17005/60000]\n",
      "loss: 0.115550  [17505/60000]\n",
      "loss: 0.888279  [18005/60000]\n",
      "loss: 0.017725  [18505/60000]\n",
      "loss: 0.494007  [19005/60000]\n",
      "loss: 0.335341  [19505/60000]\n",
      "loss: 0.064161  [20005/60000]\n",
      "loss: 0.160118  [20505/60000]\n",
      "loss: 0.018455  [21005/60000]\n",
      "loss: 0.299070  [21505/60000]\n",
      "loss: 0.306744  [22005/60000]\n",
      "loss: 0.632006  [22505/60000]\n",
      "loss: 0.128999  [23005/60000]\n",
      "loss: 0.151630  [23505/60000]\n",
      "loss: 0.354338  [24005/60000]\n",
      "loss: 0.026016  [24505/60000]\n",
      "loss: 0.378028  [25005/60000]\n",
      "loss: 1.050959  [25505/60000]\n",
      "loss: 0.120354  [26005/60000]\n",
      "loss: 0.312641  [26505/60000]\n",
      "loss: 0.037496  [27005/60000]\n",
      "loss: 0.272526  [27505/60000]\n",
      "loss: 0.466538  [28005/60000]\n",
      "loss: 0.445794  [28505/60000]\n",
      "loss: 0.097862  [29005/60000]\n",
      "loss: 0.808701  [29505/60000]\n",
      "loss: 0.276745  [30005/60000]\n",
      "loss: 0.032132  [30505/60000]\n",
      "loss: 0.088498  [31005/60000]\n",
      "loss: 0.390470  [31505/60000]\n",
      "loss: 0.177088  [32005/60000]\n",
      "loss: 0.083187  [32505/60000]\n",
      "loss: 0.008087  [33005/60000]\n",
      "loss: 0.796384  [33505/60000]\n",
      "loss: 0.011390  [34005/60000]\n",
      "loss: 0.036245  [34505/60000]\n",
      "loss: 0.211065  [35005/60000]\n",
      "loss: 0.200618  [35505/60000]\n",
      "loss: 0.055831  [36005/60000]\n",
      "loss: 0.508495  [36505/60000]\n",
      "loss: 0.035435  [37005/60000]\n",
      "loss: 0.406976  [37505/60000]\n",
      "loss: 0.428214  [38005/60000]\n",
      "loss: 0.012516  [38505/60000]\n",
      "loss: 0.871171  [39005/60000]\n",
      "loss: 0.054630  [39505/60000]\n",
      "loss: 0.565062  [40005/60000]\n",
      "loss: 0.040960  [40505/60000]\n",
      "loss: 0.054987  [41005/60000]\n",
      "loss: 0.041087  [41505/60000]\n",
      "loss: 0.103676  [42005/60000]\n",
      "loss: 0.220203  [42505/60000]\n",
      "loss: 0.195555  [43005/60000]\n",
      "loss: 0.119207  [43505/60000]\n",
      "loss: 0.175328  [44005/60000]\n",
      "loss: 0.186939  [44505/60000]\n",
      "loss: 0.578584  [45005/60000]\n",
      "loss: 0.097953  [45505/60000]\n",
      "loss: 0.349062  [46005/60000]\n",
      "loss: 0.036141  [46505/60000]\n",
      "loss: 0.132042  [47005/60000]\n",
      "loss: 0.480402  [47505/60000]\n",
      "loss: 0.023233  [48005/60000]\n",
      "loss: 0.104977  [48505/60000]\n",
      "loss: 0.360587  [49005/60000]\n",
      "loss: 0.071462  [49505/60000]\n",
      "loss: 0.183785  [50005/60000]\n",
      "loss: 0.072009  [50505/60000]\n",
      "loss: 0.553688  [51005/60000]\n",
      "loss: 0.037579  [51505/60000]\n",
      "loss: 0.071372  [52005/60000]\n",
      "loss: 0.491947  [52505/60000]\n",
      "loss: 0.626091  [53005/60000]\n",
      "loss: 0.404221  [53505/60000]\n",
      "loss: 0.163691  [54005/60000]\n",
      "loss: 0.017152  [54505/60000]\n",
      "loss: 0.543781  [55005/60000]\n",
      "loss: 0.018031  [55505/60000]\n",
      "loss: 0.075400  [56005/60000]\n",
      "loss: 0.205045  [56505/60000]\n",
      "loss: 0.862088  [57005/60000]\n",
      "loss: 0.073346  [57505/60000]\n",
      "loss: 0.032406  [58005/60000]\n",
      "loss: 0.027938  [58505/60000]\n",
      "loss: 0.210251  [59005/60000]\n",
      "loss: 0.415029  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.349918 \n",
      "\n",
      "time: 22.324163675308228\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "loss: 0.496411  [    5/60000]\n",
      "loss: 1.410029  [  505/60000]\n",
      "loss: 0.088252  [ 1005/60000]\n",
      "loss: 0.059632  [ 1505/60000]\n",
      "loss: 0.079465  [ 2005/60000]\n",
      "loss: 0.610513  [ 2505/60000]\n",
      "loss: 0.754554  [ 3005/60000]\n",
      "loss: 0.242781  [ 3505/60000]\n",
      "loss: 0.033406  [ 4005/60000]\n",
      "loss: 0.382299  [ 4505/60000]\n",
      "loss: 0.165846  [ 5005/60000]\n",
      "loss: 0.576289  [ 5505/60000]\n",
      "loss: 0.701994  [ 6005/60000]\n",
      "loss: 0.495643  [ 6505/60000]\n",
      "loss: 0.463506  [ 7005/60000]\n",
      "loss: 0.030751  [ 7505/60000]\n",
      "loss: 0.088811  [ 8005/60000]\n",
      "loss: 0.031132  [ 8505/60000]\n",
      "loss: 0.041731  [ 9005/60000]\n",
      "loss: 0.290619  [ 9505/60000]\n",
      "loss: 0.125906  [10005/60000]\n",
      "loss: 0.039218  [10505/60000]\n",
      "loss: 0.145781  [11005/60000]\n",
      "loss: 0.035248  [11505/60000]\n",
      "loss: 0.279324  [12005/60000]\n",
      "loss: 0.314521  [12505/60000]\n",
      "loss: 0.121734  [13005/60000]\n",
      "loss: 0.255568  [13505/60000]\n",
      "loss: 0.016728  [14005/60000]\n",
      "loss: 0.089267  [14505/60000]\n",
      "loss: 0.019852  [15005/60000]\n",
      "loss: 0.201432  [15505/60000]\n",
      "loss: 2.132589  [16005/60000]\n",
      "loss: 0.032288  [16505/60000]\n",
      "loss: 0.028610  [17005/60000]\n",
      "loss: 0.108900  [17505/60000]\n",
      "loss: 0.887841  [18005/60000]\n",
      "loss: 0.015547  [18505/60000]\n",
      "loss: 0.494273  [19005/60000]\n",
      "loss: 0.338621  [19505/60000]\n",
      "loss: 0.065961  [20005/60000]\n",
      "loss: 0.160391  [20505/60000]\n",
      "loss: 0.017238  [21005/60000]\n",
      "loss: 0.311297  [21505/60000]\n",
      "loss: 0.292611  [22005/60000]\n",
      "loss: 0.594562  [22505/60000]\n",
      "loss: 0.128871  [23005/60000]\n",
      "loss: 0.154653  [23505/60000]\n",
      "loss: 0.355298  [24005/60000]\n",
      "loss: 0.024954  [24505/60000]\n",
      "loss: 0.374686  [25005/60000]\n",
      "loss: 1.026405  [25505/60000]\n",
      "loss: 0.117076  [26005/60000]\n",
      "loss: 0.312319  [26505/60000]\n",
      "loss: 0.036609  [27005/60000]\n",
      "loss: 0.264194  [27505/60000]\n",
      "loss: 0.459710  [28005/60000]\n",
      "loss: 0.428171  [28505/60000]\n",
      "loss: 0.092644  [29005/60000]\n",
      "loss: 0.756462  [29505/60000]\n",
      "loss: 0.273163  [30005/60000]\n",
      "loss: 0.030478  [30505/60000]\n",
      "loss: 0.088907  [31005/60000]\n",
      "loss: 0.390282  [31505/60000]\n",
      "loss: 0.171258  [32005/60000]\n",
      "loss: 0.080841  [32505/60000]\n",
      "loss: 0.007608  [33005/60000]\n",
      "loss: 0.792451  [33505/60000]\n",
      "loss: 0.010901  [34005/60000]\n",
      "loss: 0.037143  [34505/60000]\n",
      "loss: 0.204854  [35005/60000]\n",
      "loss: 0.204493  [35505/60000]\n",
      "loss: 0.056766  [36005/60000]\n",
      "loss: 0.506055  [36505/60000]\n",
      "loss: 0.035050  [37005/60000]\n",
      "loss: 0.388384  [37505/60000]\n",
      "loss: 0.432121  [38005/60000]\n",
      "loss: 0.011461  [38505/60000]\n",
      "loss: 0.855010  [39005/60000]\n",
      "loss: 0.054767  [39505/60000]\n",
      "loss: 0.556340  [40005/60000]\n",
      "loss: 0.038960  [40505/60000]\n",
      "loss: 0.054411  [41005/60000]\n",
      "loss: 0.039793  [41505/60000]\n",
      "loss: 0.103041  [42005/60000]\n",
      "loss: 0.205371  [42505/60000]\n",
      "loss: 0.191109  [43005/60000]\n",
      "loss: 0.118786  [43505/60000]\n",
      "loss: 0.168222  [44005/60000]\n",
      "loss: 0.188505  [44505/60000]\n",
      "loss: 0.567385  [45005/60000]\n",
      "loss: 0.092901  [45505/60000]\n",
      "loss: 0.346950  [46005/60000]\n",
      "loss: 0.034734  [46505/60000]\n",
      "loss: 0.128573  [47005/60000]\n",
      "loss: 0.496039  [47505/60000]\n",
      "loss: 0.022138  [48005/60000]\n",
      "loss: 0.105363  [48505/60000]\n",
      "loss: 0.367669  [49005/60000]\n",
      "loss: 0.069769  [49505/60000]\n",
      "loss: 0.175018  [50005/60000]\n",
      "loss: 0.067029  [50505/60000]\n",
      "loss: 0.554014  [51005/60000]\n",
      "loss: 0.037751  [51505/60000]\n",
      "loss: 0.071271  [52005/60000]\n",
      "loss: 0.466379  [52505/60000]\n",
      "loss: 0.617766  [53005/60000]\n",
      "loss: 0.397987  [53505/60000]\n",
      "loss: 0.155838  [54005/60000]\n",
      "loss: 0.017291  [54505/60000]\n",
      "loss: 0.542642  [55005/60000]\n",
      "loss: 0.017185  [55505/60000]\n",
      "loss: 0.067379  [56005/60000]\n",
      "loss: 0.195415  [56505/60000]\n",
      "loss: 0.868043  [57005/60000]\n",
      "loss: 0.070233  [57505/60000]\n",
      "loss: 0.033070  [58005/60000]\n",
      "loss: 0.027032  [58505/60000]\n",
      "loss: 0.203957  [59005/60000]\n",
      "loss: 0.408369  [59505/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.347923 \n",
      "\n",
      "time: 22.171459436416626\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_dataloader, test_dataloader = data_loader(batch_size)\n",
    "corrects_5 = []\n",
    "test_losses_5 = []\n",
    "times_5 = []\n",
    "for t in range(epoches):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    start = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    end = time.time()\n",
    "    correct, test_loss = test(test_dataloader, model, loss_fn)\n",
    "    corrects_5.append(correct)\n",
    "    test_losses_5.append(test_loss)\n",
    "    times_5.append(end - start)\n",
    "    print(f\"time: {end - start}\\n\")\n",
    "    pass\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxcFCfHYMEaM"
   },
   "source": [
    "### batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpPDltTeDrvG",
    "outputId": "bec01780-90ce-4747-94d9-0fbd87d8a888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 0.226834  [   20/60000]\n",
      "loss: 0.351985  [ 2020/60000]\n",
      "loss: 0.102212  [ 4020/60000]\n",
      "loss: 0.296886  [ 6020/60000]\n",
      "loss: 0.539414  [ 8020/60000]\n",
      "loss: 0.152822  [10020/60000]\n",
      "loss: 0.222950  [12020/60000]\n",
      "loss: 0.322720  [14020/60000]\n",
      "loss: 0.628707  [16020/60000]\n",
      "loss: 0.566850  [18020/60000]\n",
      "loss: 0.105648  [20020/60000]\n",
      "loss: 0.418441  [22020/60000]\n",
      "loss: 0.369168  [24020/60000]\n",
      "loss: 0.175896  [26020/60000]\n",
      "loss: 0.537115  [28020/60000]\n",
      "loss: 0.108882  [30020/60000]\n",
      "loss: 0.277522  [32020/60000]\n",
      "loss: 0.350206  [34020/60000]\n",
      "loss: 0.192821  [36020/60000]\n",
      "loss: 0.318066  [38020/60000]\n",
      "loss: 0.492271  [40020/60000]\n",
      "loss: 0.781828  [42020/60000]\n",
      "loss: 0.132923  [44020/60000]\n",
      "loss: 0.370903  [46020/60000]\n",
      "loss: 0.299779  [48020/60000]\n",
      "loss: 0.502511  [50020/60000]\n",
      "loss: 0.079573  [52020/60000]\n",
      "loss: 0.237841  [54020/60000]\n",
      "loss: 0.125919  [56020/60000]\n",
      "loss: 0.318971  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.341361 \n",
      "\n",
      "time: 12.449963808059692\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 0.216291  [   20/60000]\n",
      "loss: 0.342162  [ 2020/60000]\n",
      "loss: 0.105733  [ 4020/60000]\n",
      "loss: 0.301049  [ 6020/60000]\n",
      "loss: 0.547093  [ 8020/60000]\n",
      "loss: 0.150979  [10020/60000]\n",
      "loss: 0.222935  [12020/60000]\n",
      "loss: 0.323849  [14020/60000]\n",
      "loss: 0.624013  [16020/60000]\n",
      "loss: 0.557369  [18020/60000]\n",
      "loss: 0.103881  [20020/60000]\n",
      "loss: 0.416025  [22020/60000]\n",
      "loss: 0.369107  [24020/60000]\n",
      "loss: 0.172276  [26020/60000]\n",
      "loss: 0.535190  [28020/60000]\n",
      "loss: 0.105348  [30020/60000]\n",
      "loss: 0.273945  [32020/60000]\n",
      "loss: 0.347217  [34020/60000]\n",
      "loss: 0.192328  [36020/60000]\n",
      "loss: 0.317185  [38020/60000]\n",
      "loss: 0.494975  [40020/60000]\n",
      "loss: 0.782982  [42020/60000]\n",
      "loss: 0.131794  [44020/60000]\n",
      "loss: 0.373651  [46020/60000]\n",
      "loss: 0.301304  [48020/60000]\n",
      "loss: 0.499944  [50020/60000]\n",
      "loss: 0.078692  [52020/60000]\n",
      "loss: 0.237754  [54020/60000]\n",
      "loss: 0.125274  [56020/60000]\n",
      "loss: 0.316439  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.340729 \n",
      "\n",
      "time: 12.063506126403809\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 0.213482  [   20/60000]\n",
      "loss: 0.340916  [ 2020/60000]\n",
      "loss: 0.106004  [ 4020/60000]\n",
      "loss: 0.302030  [ 6020/60000]\n",
      "loss: 0.549001  [ 8020/60000]\n",
      "loss: 0.149202  [10020/60000]\n",
      "loss: 0.221449  [12020/60000]\n",
      "loss: 0.322691  [14020/60000]\n",
      "loss: 0.624068  [16020/60000]\n",
      "loss: 0.554511  [18020/60000]\n",
      "loss: 0.103253  [20020/60000]\n",
      "loss: 0.412398  [22020/60000]\n",
      "loss: 0.368489  [24020/60000]\n",
      "loss: 0.170152  [26020/60000]\n",
      "loss: 0.534453  [28020/60000]\n",
      "loss: 0.103803  [30020/60000]\n",
      "loss: 0.273880  [32020/60000]\n",
      "loss: 0.346472  [34020/60000]\n",
      "loss: 0.191542  [36020/60000]\n",
      "loss: 0.319226  [38020/60000]\n",
      "loss: 0.493170  [40020/60000]\n",
      "loss: 0.783265  [42020/60000]\n",
      "loss: 0.130760  [44020/60000]\n",
      "loss: 0.374031  [46020/60000]\n",
      "loss: 0.302323  [48020/60000]\n",
      "loss: 0.497989  [50020/60000]\n",
      "loss: 0.079154  [52020/60000]\n",
      "loss: 0.237461  [54020/60000]\n",
      "loss: 0.124958  [56020/60000]\n",
      "loss: 0.317405  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.340012 \n",
      "\n",
      "time: 11.666417837142944\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 0.211942  [   20/60000]\n",
      "loss: 0.340235  [ 2020/60000]\n",
      "loss: 0.106392  [ 4020/60000]\n",
      "loss: 0.301949  [ 6020/60000]\n",
      "loss: 0.548664  [ 8020/60000]\n",
      "loss: 0.147775  [10020/60000]\n",
      "loss: 0.219796  [12020/60000]\n",
      "loss: 0.320343  [14020/60000]\n",
      "loss: 0.625320  [16020/60000]\n",
      "loss: 0.552659  [18020/60000]\n",
      "loss: 0.103003  [20020/60000]\n",
      "loss: 0.408912  [22020/60000]\n",
      "loss: 0.366355  [24020/60000]\n",
      "loss: 0.170373  [26020/60000]\n",
      "loss: 0.533211  [28020/60000]\n",
      "loss: 0.103072  [30020/60000]\n",
      "loss: 0.273242  [32020/60000]\n",
      "loss: 0.345937  [34020/60000]\n",
      "loss: 0.190948  [36020/60000]\n",
      "loss: 0.319698  [38020/60000]\n",
      "loss: 0.492377  [40020/60000]\n",
      "loss: 0.783380  [42020/60000]\n",
      "loss: 0.129953  [44020/60000]\n",
      "loss: 0.374837  [46020/60000]\n",
      "loss: 0.301576  [48020/60000]\n",
      "loss: 0.496974  [50020/60000]\n",
      "loss: 0.079052  [52020/60000]\n",
      "loss: 0.235064  [54020/60000]\n",
      "loss: 0.124633  [56020/60000]\n",
      "loss: 0.317770  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.339405 \n",
      "\n",
      "time: 11.741576910018921\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 0.210726  [   20/60000]\n",
      "loss: 0.339617  [ 2020/60000]\n",
      "loss: 0.106199  [ 4020/60000]\n",
      "loss: 0.300961  [ 6020/60000]\n",
      "loss: 0.549346  [ 8020/60000]\n",
      "loss: 0.146704  [10020/60000]\n",
      "loss: 0.218800  [12020/60000]\n",
      "loss: 0.319902  [14020/60000]\n",
      "loss: 0.625169  [16020/60000]\n",
      "loss: 0.550798  [18020/60000]\n",
      "loss: 0.102423  [20020/60000]\n",
      "loss: 0.406532  [22020/60000]\n",
      "loss: 0.365261  [24020/60000]\n",
      "loss: 0.170137  [26020/60000]\n",
      "loss: 0.531411  [28020/60000]\n",
      "loss: 0.102289  [30020/60000]\n",
      "loss: 0.272163  [32020/60000]\n",
      "loss: 0.344160  [34020/60000]\n",
      "loss: 0.189768  [36020/60000]\n",
      "loss: 0.320091  [38020/60000]\n",
      "loss: 0.492130  [40020/60000]\n",
      "loss: 0.784090  [42020/60000]\n",
      "loss: 0.129049  [44020/60000]\n",
      "loss: 0.375038  [46020/60000]\n",
      "loss: 0.301145  [48020/60000]\n",
      "loss: 0.495876  [50020/60000]\n",
      "loss: 0.078873  [52020/60000]\n",
      "loss: 0.233404  [54020/60000]\n",
      "loss: 0.124269  [56020/60000]\n",
      "loss: 0.316883  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.338792 \n",
      "\n",
      "time: 12.024200916290283\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 0.209571  [   20/60000]\n",
      "loss: 0.338920  [ 2020/60000]\n",
      "loss: 0.106083  [ 4020/60000]\n",
      "loss: 0.300569  [ 6020/60000]\n",
      "loss: 0.549870  [ 8020/60000]\n",
      "loss: 0.145309  [10020/60000]\n",
      "loss: 0.217293  [12020/60000]\n",
      "loss: 0.318447  [14020/60000]\n",
      "loss: 0.624414  [16020/60000]\n",
      "loss: 0.550281  [18020/60000]\n",
      "loss: 0.101656  [20020/60000]\n",
      "loss: 0.402654  [22020/60000]\n",
      "loss: 0.362501  [24020/60000]\n",
      "loss: 0.168729  [26020/60000]\n",
      "loss: 0.530264  [28020/60000]\n",
      "loss: 0.101495  [30020/60000]\n",
      "loss: 0.270729  [32020/60000]\n",
      "loss: 0.342923  [34020/60000]\n",
      "loss: 0.188633  [36020/60000]\n",
      "loss: 0.320505  [38020/60000]\n",
      "loss: 0.490723  [40020/60000]\n",
      "loss: 0.784507  [42020/60000]\n",
      "loss: 0.128109  [44020/60000]\n",
      "loss: 0.376540  [46020/60000]\n",
      "loss: 0.300761  [48020/60000]\n",
      "loss: 0.494400  [50020/60000]\n",
      "loss: 0.078951  [52020/60000]\n",
      "loss: 0.230755  [54020/60000]\n",
      "loss: 0.124130  [56020/60000]\n",
      "loss: 0.316882  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.338252 \n",
      "\n",
      "time: 11.640491962432861\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 0.208684  [   20/60000]\n",
      "loss: 0.338070  [ 2020/60000]\n",
      "loss: 0.105792  [ 4020/60000]\n",
      "loss: 0.299809  [ 6020/60000]\n",
      "loss: 0.550159  [ 8020/60000]\n",
      "loss: 0.143673  [10020/60000]\n",
      "loss: 0.216405  [12020/60000]\n",
      "loss: 0.318076  [14020/60000]\n",
      "loss: 0.624590  [16020/60000]\n",
      "loss: 0.547387  [18020/60000]\n",
      "loss: 0.101131  [20020/60000]\n",
      "loss: 0.400232  [22020/60000]\n",
      "loss: 0.360129  [24020/60000]\n",
      "loss: 0.168072  [26020/60000]\n",
      "loss: 0.528926  [28020/60000]\n",
      "loss: 0.100648  [30020/60000]\n",
      "loss: 0.269489  [32020/60000]\n",
      "loss: 0.342412  [34020/60000]\n",
      "loss: 0.187536  [36020/60000]\n",
      "loss: 0.320945  [38020/60000]\n",
      "loss: 0.488394  [40020/60000]\n",
      "loss: 0.784545  [42020/60000]\n",
      "loss: 0.127004  [44020/60000]\n",
      "loss: 0.376431  [46020/60000]\n",
      "loss: 0.299780  [48020/60000]\n",
      "loss: 0.493945  [50020/60000]\n",
      "loss: 0.079029  [52020/60000]\n",
      "loss: 0.229266  [54020/60000]\n",
      "loss: 0.124431  [56020/60000]\n",
      "loss: 0.316666  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.337670 \n",
      "\n",
      "time: 12.052232503890991\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 0.207906  [   20/60000]\n",
      "loss: 0.337586  [ 2020/60000]\n",
      "loss: 0.105564  [ 4020/60000]\n",
      "loss: 0.298739  [ 6020/60000]\n",
      "loss: 0.549247  [ 8020/60000]\n",
      "loss: 0.142683  [10020/60000]\n",
      "loss: 0.215765  [12020/60000]\n",
      "loss: 0.318404  [14020/60000]\n",
      "loss: 0.624982  [16020/60000]\n",
      "loss: 0.545082  [18020/60000]\n",
      "loss: 0.100846  [20020/60000]\n",
      "loss: 0.394888  [22020/60000]\n",
      "loss: 0.358838  [24020/60000]\n",
      "loss: 0.166993  [26020/60000]\n",
      "loss: 0.527516  [28020/60000]\n",
      "loss: 0.099646  [30020/60000]\n",
      "loss: 0.267962  [32020/60000]\n",
      "loss: 0.341527  [34020/60000]\n",
      "loss: 0.186363  [36020/60000]\n",
      "loss: 0.320818  [38020/60000]\n",
      "loss: 0.487343  [40020/60000]\n",
      "loss: 0.785648  [42020/60000]\n",
      "loss: 0.126103  [44020/60000]\n",
      "loss: 0.376526  [46020/60000]\n",
      "loss: 0.299243  [48020/60000]\n",
      "loss: 0.494457  [50020/60000]\n",
      "loss: 0.078662  [52020/60000]\n",
      "loss: 0.228062  [54020/60000]\n",
      "loss: 0.124207  [56020/60000]\n",
      "loss: 0.316137  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.337139 \n",
      "\n",
      "time: 11.656893491744995\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 0.207042  [   20/60000]\n",
      "loss: 0.337000  [ 2020/60000]\n",
      "loss: 0.105042  [ 4020/60000]\n",
      "loss: 0.297555  [ 6020/60000]\n",
      "loss: 0.549644  [ 8020/60000]\n",
      "loss: 0.141651  [10020/60000]\n",
      "loss: 0.214863  [12020/60000]\n",
      "loss: 0.318758  [14020/60000]\n",
      "loss: 0.624712  [16020/60000]\n",
      "loss: 0.542168  [18020/60000]\n",
      "loss: 0.100161  [20020/60000]\n",
      "loss: 0.391383  [22020/60000]\n",
      "loss: 0.356505  [24020/60000]\n",
      "loss: 0.166528  [26020/60000]\n",
      "loss: 0.526308  [28020/60000]\n",
      "loss: 0.098818  [30020/60000]\n",
      "loss: 0.267209  [32020/60000]\n",
      "loss: 0.339214  [34020/60000]\n",
      "loss: 0.184964  [36020/60000]\n",
      "loss: 0.321755  [38020/60000]\n",
      "loss: 0.485970  [40020/60000]\n",
      "loss: 0.786106  [42020/60000]\n",
      "loss: 0.124962  [44020/60000]\n",
      "loss: 0.376899  [46020/60000]\n",
      "loss: 0.299121  [48020/60000]\n",
      "loss: 0.493408  [50020/60000]\n",
      "loss: 0.078380  [52020/60000]\n",
      "loss: 0.225833  [54020/60000]\n",
      "loss: 0.124063  [56020/60000]\n",
      "loss: 0.315890  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.336608 \n",
      "\n",
      "time: 11.66609239578247\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 0.206188  [   20/60000]\n",
      "loss: 0.336464  [ 2020/60000]\n",
      "loss: 0.105134  [ 4020/60000]\n",
      "loss: 0.295790  [ 6020/60000]\n",
      "loss: 0.549193  [ 8020/60000]\n",
      "loss: 0.140415  [10020/60000]\n",
      "loss: 0.213944  [12020/60000]\n",
      "loss: 0.319323  [14020/60000]\n",
      "loss: 0.624354  [16020/60000]\n",
      "loss: 0.540613  [18020/60000]\n",
      "loss: 0.099476  [20020/60000]\n",
      "loss: 0.387021  [22020/60000]\n",
      "loss: 0.355169  [24020/60000]\n",
      "loss: 0.165681  [26020/60000]\n",
      "loss: 0.525005  [28020/60000]\n",
      "loss: 0.097856  [30020/60000]\n",
      "loss: 0.265657  [32020/60000]\n",
      "loss: 0.337354  [34020/60000]\n",
      "loss: 0.183854  [36020/60000]\n",
      "loss: 0.320856  [38020/60000]\n",
      "loss: 0.484868  [40020/60000]\n",
      "loss: 0.785407  [42020/60000]\n",
      "loss: 0.124113  [44020/60000]\n",
      "loss: 0.376310  [46020/60000]\n",
      "loss: 0.297781  [48020/60000]\n",
      "loss: 0.492752  [50020/60000]\n",
      "loss: 0.078064  [52020/60000]\n",
      "loss: 0.224103  [54020/60000]\n",
      "loss: 0.123665  [56020/60000]\n",
      "loss: 0.314745  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.336144 \n",
      "\n",
      "time: 11.643059492111206\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "loss: 0.205412  [   20/60000]\n",
      "loss: 0.335143  [ 2020/60000]\n",
      "loss: 0.104895  [ 4020/60000]\n",
      "loss: 0.294682  [ 6020/60000]\n",
      "loss: 0.548749  [ 8020/60000]\n",
      "loss: 0.139215  [10020/60000]\n",
      "loss: 0.213100  [12020/60000]\n",
      "loss: 0.319169  [14020/60000]\n",
      "loss: 0.624446  [16020/60000]\n",
      "loss: 0.537008  [18020/60000]\n",
      "loss: 0.098945  [20020/60000]\n",
      "loss: 0.384054  [22020/60000]\n",
      "loss: 0.352837  [24020/60000]\n",
      "loss: 0.164590  [26020/60000]\n",
      "loss: 0.524108  [28020/60000]\n",
      "loss: 0.097231  [30020/60000]\n",
      "loss: 0.264272  [32020/60000]\n",
      "loss: 0.336353  [34020/60000]\n",
      "loss: 0.182916  [36020/60000]\n",
      "loss: 0.321332  [38020/60000]\n",
      "loss: 0.483949  [40020/60000]\n",
      "loss: 0.785663  [42020/60000]\n",
      "loss: 0.123162  [44020/60000]\n",
      "loss: 0.375908  [46020/60000]\n",
      "loss: 0.296904  [48020/60000]\n",
      "loss: 0.491615  [50020/60000]\n",
      "loss: 0.077486  [52020/60000]\n",
      "loss: 0.222342  [54020/60000]\n",
      "loss: 0.123549  [56020/60000]\n",
      "loss: 0.315071  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.335597 \n",
      "\n",
      "time: 11.677424192428589\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "loss: 0.205037  [   20/60000]\n",
      "loss: 0.334205  [ 2020/60000]\n",
      "loss: 0.104786  [ 4020/60000]\n",
      "loss: 0.292857  [ 6020/60000]\n",
      "loss: 0.548710  [ 8020/60000]\n",
      "loss: 0.138112  [10020/60000]\n",
      "loss: 0.211826  [12020/60000]\n",
      "loss: 0.319291  [14020/60000]\n",
      "loss: 0.623891  [16020/60000]\n",
      "loss: 0.535587  [18020/60000]\n",
      "loss: 0.098577  [20020/60000]\n",
      "loss: 0.379546  [22020/60000]\n",
      "loss: 0.350882  [24020/60000]\n",
      "loss: 0.164532  [26020/60000]\n",
      "loss: 0.522529  [28020/60000]\n",
      "loss: 0.096159  [30020/60000]\n",
      "loss: 0.263142  [32020/60000]\n",
      "loss: 0.334617  [34020/60000]\n",
      "loss: 0.181850  [36020/60000]\n",
      "loss: 0.321314  [38020/60000]\n",
      "loss: 0.482307  [40020/60000]\n",
      "loss: 0.785119  [42020/60000]\n",
      "loss: 0.122279  [44020/60000]\n",
      "loss: 0.375023  [46020/60000]\n",
      "loss: 0.296214  [48020/60000]\n",
      "loss: 0.490621  [50020/60000]\n",
      "loss: 0.077251  [52020/60000]\n",
      "loss: 0.220344  [54020/60000]\n",
      "loss: 0.123247  [56020/60000]\n",
      "loss: 0.314505  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.335120 \n",
      "\n",
      "time: 11.804815530776978\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "loss: 0.204256  [   20/60000]\n",
      "loss: 0.333579  [ 2020/60000]\n",
      "loss: 0.104531  [ 4020/60000]\n",
      "loss: 0.291900  [ 6020/60000]\n",
      "loss: 0.549326  [ 8020/60000]\n",
      "loss: 0.137040  [10020/60000]\n",
      "loss: 0.210533  [12020/60000]\n",
      "loss: 0.318459  [14020/60000]\n",
      "loss: 0.623395  [16020/60000]\n",
      "loss: 0.533571  [18020/60000]\n",
      "loss: 0.098297  [20020/60000]\n",
      "loss: 0.374974  [22020/60000]\n",
      "loss: 0.348361  [24020/60000]\n",
      "loss: 0.163802  [26020/60000]\n",
      "loss: 0.522321  [28020/60000]\n",
      "loss: 0.095114  [30020/60000]\n",
      "loss: 0.261718  [32020/60000]\n",
      "loss: 0.333354  [34020/60000]\n",
      "loss: 0.180599  [36020/60000]\n",
      "loss: 0.320909  [38020/60000]\n",
      "loss: 0.482022  [40020/60000]\n",
      "loss: 0.786336  [42020/60000]\n",
      "loss: 0.121643  [44020/60000]\n",
      "loss: 0.374680  [46020/60000]\n",
      "loss: 0.295712  [48020/60000]\n",
      "loss: 0.489786  [50020/60000]\n",
      "loss: 0.076707  [52020/60000]\n",
      "loss: 0.219353  [54020/60000]\n",
      "loss: 0.123115  [56020/60000]\n",
      "loss: 0.314472  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.334640 \n",
      "\n",
      "time: 12.550715208053589\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "loss: 0.204162  [   20/60000]\n",
      "loss: 0.331997  [ 2020/60000]\n",
      "loss: 0.104109  [ 4020/60000]\n",
      "loss: 0.289981  [ 6020/60000]\n",
      "loss: 0.547900  [ 8020/60000]\n",
      "loss: 0.135935  [10020/60000]\n",
      "loss: 0.209111  [12020/60000]\n",
      "loss: 0.318622  [14020/60000]\n",
      "loss: 0.623599  [16020/60000]\n",
      "loss: 0.529831  [18020/60000]\n",
      "loss: 0.097962  [20020/60000]\n",
      "loss: 0.370308  [22020/60000]\n",
      "loss: 0.346302  [24020/60000]\n",
      "loss: 0.162975  [26020/60000]\n",
      "loss: 0.521415  [28020/60000]\n",
      "loss: 0.094160  [30020/60000]\n",
      "loss: 0.261275  [32020/60000]\n",
      "loss: 0.332024  [34020/60000]\n",
      "loss: 0.179835  [36020/60000]\n",
      "loss: 0.320910  [38020/60000]\n",
      "loss: 0.480505  [40020/60000]\n",
      "loss: 0.786444  [42020/60000]\n",
      "loss: 0.120806  [44020/60000]\n",
      "loss: 0.374104  [46020/60000]\n",
      "loss: 0.294405  [48020/60000]\n",
      "loss: 0.489301  [50020/60000]\n",
      "loss: 0.075769  [52020/60000]\n",
      "loss: 0.217665  [54020/60000]\n",
      "loss: 0.122796  [56020/60000]\n",
      "loss: 0.314447  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.334189 \n",
      "\n",
      "time: 11.775819778442383\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "loss: 0.203506  [   20/60000]\n",
      "loss: 0.331097  [ 2020/60000]\n",
      "loss: 0.103904  [ 4020/60000]\n",
      "loss: 0.288908  [ 6020/60000]\n",
      "loss: 0.548044  [ 8020/60000]\n",
      "loss: 0.134742  [10020/60000]\n",
      "loss: 0.207980  [12020/60000]\n",
      "loss: 0.317790  [14020/60000]\n",
      "loss: 0.623448  [16020/60000]\n",
      "loss: 0.527999  [18020/60000]\n",
      "loss: 0.097495  [20020/60000]\n",
      "loss: 0.367492  [22020/60000]\n",
      "loss: 0.344011  [24020/60000]\n",
      "loss: 0.162126  [26020/60000]\n",
      "loss: 0.520015  [28020/60000]\n",
      "loss: 0.093474  [30020/60000]\n",
      "loss: 0.259632  [32020/60000]\n",
      "loss: 0.330582  [34020/60000]\n",
      "loss: 0.178777  [36020/60000]\n",
      "loss: 0.320270  [38020/60000]\n",
      "loss: 0.478979  [40020/60000]\n",
      "loss: 0.785762  [42020/60000]\n",
      "loss: 0.119980  [44020/60000]\n",
      "loss: 0.373542  [46020/60000]\n",
      "loss: 0.293282  [48020/60000]\n",
      "loss: 0.488116  [50020/60000]\n",
      "loss: 0.075921  [52020/60000]\n",
      "loss: 0.215757  [54020/60000]\n",
      "loss: 0.122655  [56020/60000]\n",
      "loss: 0.313691  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.333733 \n",
      "\n",
      "time: 11.762111902236938\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "loss: 0.203180  [   20/60000]\n",
      "loss: 0.330192  [ 2020/60000]\n",
      "loss: 0.103484  [ 4020/60000]\n",
      "loss: 0.287205  [ 6020/60000]\n",
      "loss: 0.547584  [ 8020/60000]\n",
      "loss: 0.133594  [10020/60000]\n",
      "loss: 0.206939  [12020/60000]\n",
      "loss: 0.317923  [14020/60000]\n",
      "loss: 0.623256  [16020/60000]\n",
      "loss: 0.524753  [18020/60000]\n",
      "loss: 0.097041  [20020/60000]\n",
      "loss: 0.363989  [22020/60000]\n",
      "loss: 0.341823  [24020/60000]\n",
      "loss: 0.161225  [26020/60000]\n",
      "loss: 0.518504  [28020/60000]\n",
      "loss: 0.092424  [30020/60000]\n",
      "loss: 0.258622  [32020/60000]\n",
      "loss: 0.328757  [34020/60000]\n",
      "loss: 0.177735  [36020/60000]\n",
      "loss: 0.320478  [38020/60000]\n",
      "loss: 0.477214  [40020/60000]\n",
      "loss: 0.786336  [42020/60000]\n",
      "loss: 0.118942  [44020/60000]\n",
      "loss: 0.372363  [46020/60000]\n",
      "loss: 0.292184  [48020/60000]\n",
      "loss: 0.487045  [50020/60000]\n",
      "loss: 0.075250  [52020/60000]\n",
      "loss: 0.213543  [54020/60000]\n",
      "loss: 0.122599  [56020/60000]\n",
      "loss: 0.313603  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.333286 \n",
      "\n",
      "time: 12.002426385879517\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "loss: 0.202538  [   20/60000]\n",
      "loss: 0.329298  [ 2020/60000]\n",
      "loss: 0.103399  [ 4020/60000]\n",
      "loss: 0.286306  [ 6020/60000]\n",
      "loss: 0.548396  [ 8020/60000]\n",
      "loss: 0.132598  [10020/60000]\n",
      "loss: 0.206016  [12020/60000]\n",
      "loss: 0.318683  [14020/60000]\n",
      "loss: 0.623433  [16020/60000]\n",
      "loss: 0.521920  [18020/60000]\n",
      "loss: 0.096694  [20020/60000]\n",
      "loss: 0.359823  [22020/60000]\n",
      "loss: 0.338636  [24020/60000]\n",
      "loss: 0.160139  [26020/60000]\n",
      "loss: 0.517458  [28020/60000]\n",
      "loss: 0.091491  [30020/60000]\n",
      "loss: 0.257847  [32020/60000]\n",
      "loss: 0.326276  [34020/60000]\n",
      "loss: 0.177094  [36020/60000]\n",
      "loss: 0.320123  [38020/60000]\n",
      "loss: 0.475250  [40020/60000]\n",
      "loss: 0.786106  [42020/60000]\n",
      "loss: 0.118223  [44020/60000]\n",
      "loss: 0.372276  [46020/60000]\n",
      "loss: 0.291564  [48020/60000]\n",
      "loss: 0.486859  [50020/60000]\n",
      "loss: 0.074437  [52020/60000]\n",
      "loss: 0.211481  [54020/60000]\n",
      "loss: 0.122639  [56020/60000]\n",
      "loss: 0.313640  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.332883 \n",
      "\n",
      "time: 11.678451299667358\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "loss: 0.202066  [   20/60000]\n",
      "loss: 0.328327  [ 2020/60000]\n",
      "loss: 0.103046  [ 4020/60000]\n",
      "loss: 0.284637  [ 6020/60000]\n",
      "loss: 0.549095  [ 8020/60000]\n",
      "loss: 0.131743  [10020/60000]\n",
      "loss: 0.205166  [12020/60000]\n",
      "loss: 0.317974  [14020/60000]\n",
      "loss: 0.623629  [16020/60000]\n",
      "loss: 0.517387  [18020/60000]\n",
      "loss: 0.096165  [20020/60000]\n",
      "loss: 0.355547  [22020/60000]\n",
      "loss: 0.336451  [24020/60000]\n",
      "loss: 0.159344  [26020/60000]\n",
      "loss: 0.516534  [28020/60000]\n",
      "loss: 0.090547  [30020/60000]\n",
      "loss: 0.256750  [32020/60000]\n",
      "loss: 0.325575  [34020/60000]\n",
      "loss: 0.176076  [36020/60000]\n",
      "loss: 0.320038  [38020/60000]\n",
      "loss: 0.473539  [40020/60000]\n",
      "loss: 0.786723  [42020/60000]\n",
      "loss: 0.116725  [44020/60000]\n",
      "loss: 0.371074  [46020/60000]\n",
      "loss: 0.290244  [48020/60000]\n",
      "loss: 0.485218  [50020/60000]\n",
      "loss: 0.074189  [52020/60000]\n",
      "loss: 0.210385  [54020/60000]\n",
      "loss: 0.122470  [56020/60000]\n",
      "loss: 0.312518  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.332432 \n",
      "\n",
      "time: 11.812737226486206\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "loss: 0.201431  [   20/60000]\n",
      "loss: 0.328368  [ 2020/60000]\n",
      "loss: 0.102818  [ 4020/60000]\n",
      "loss: 0.283413  [ 6020/60000]\n",
      "loss: 0.548892  [ 8020/60000]\n",
      "loss: 0.130648  [10020/60000]\n",
      "loss: 0.203858  [12020/60000]\n",
      "loss: 0.317442  [14020/60000]\n",
      "loss: 0.623555  [16020/60000]\n",
      "loss: 0.515416  [18020/60000]\n",
      "loss: 0.095764  [20020/60000]\n",
      "loss: 0.352012  [22020/60000]\n",
      "loss: 0.333848  [24020/60000]\n",
      "loss: 0.158571  [26020/60000]\n",
      "loss: 0.515303  [28020/60000]\n",
      "loss: 0.089800  [30020/60000]\n",
      "loss: 0.254807  [32020/60000]\n",
      "loss: 0.324438  [34020/60000]\n",
      "loss: 0.174968  [36020/60000]\n",
      "loss: 0.319926  [38020/60000]\n",
      "loss: 0.471199  [40020/60000]\n",
      "loss: 0.786157  [42020/60000]\n",
      "loss: 0.115960  [44020/60000]\n",
      "loss: 0.370051  [46020/60000]\n",
      "loss: 0.289370  [48020/60000]\n",
      "loss: 0.484723  [50020/60000]\n",
      "loss: 0.074087  [52020/60000]\n",
      "loss: 0.208738  [54020/60000]\n",
      "loss: 0.122416  [56020/60000]\n",
      "loss: 0.311986  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.332045 \n",
      "\n",
      "time: 11.87527322769165\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "loss: 0.200864  [   20/60000]\n",
      "loss: 0.327173  [ 2020/60000]\n",
      "loss: 0.102501  [ 4020/60000]\n",
      "loss: 0.282315  [ 6020/60000]\n",
      "loss: 0.547839  [ 8020/60000]\n",
      "loss: 0.129599  [10020/60000]\n",
      "loss: 0.203034  [12020/60000]\n",
      "loss: 0.318288  [14020/60000]\n",
      "loss: 0.623545  [16020/60000]\n",
      "loss: 0.512791  [18020/60000]\n",
      "loss: 0.095349  [20020/60000]\n",
      "loss: 0.347110  [22020/60000]\n",
      "loss: 0.331534  [24020/60000]\n",
      "loss: 0.158374  [26020/60000]\n",
      "loss: 0.513928  [28020/60000]\n",
      "loss: 0.088121  [30020/60000]\n",
      "loss: 0.253913  [32020/60000]\n",
      "loss: 0.322529  [34020/60000]\n",
      "loss: 0.173627  [36020/60000]\n",
      "loss: 0.319774  [38020/60000]\n",
      "loss: 0.470555  [40020/60000]\n",
      "loss: 0.786335  [42020/60000]\n",
      "loss: 0.114961  [44020/60000]\n",
      "loss: 0.369348  [46020/60000]\n",
      "loss: 0.287620  [48020/60000]\n",
      "loss: 0.484178  [50020/60000]\n",
      "loss: 0.074420  [52020/60000]\n",
      "loss: 0.206866  [54020/60000]\n",
      "loss: 0.122111  [56020/60000]\n",
      "loss: 0.311744  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.331681 \n",
      "\n",
      "time: 11.93680214881897\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "loss: 0.200463  [   20/60000]\n",
      "loss: 0.326043  [ 2020/60000]\n",
      "loss: 0.102503  [ 4020/60000]\n",
      "loss: 0.281152  [ 6020/60000]\n",
      "loss: 0.548152  [ 8020/60000]\n",
      "loss: 0.128462  [10020/60000]\n",
      "loss: 0.202005  [12020/60000]\n",
      "loss: 0.318704  [14020/60000]\n",
      "loss: 0.622245  [16020/60000]\n",
      "loss: 0.511613  [18020/60000]\n",
      "loss: 0.094922  [20020/60000]\n",
      "loss: 0.342506  [22020/60000]\n",
      "loss: 0.329152  [24020/60000]\n",
      "loss: 0.157625  [26020/60000]\n",
      "loss: 0.513094  [28020/60000]\n",
      "loss: 0.086910  [30020/60000]\n",
      "loss: 0.252654  [32020/60000]\n",
      "loss: 0.320614  [34020/60000]\n",
      "loss: 0.172829  [36020/60000]\n",
      "loss: 0.319768  [38020/60000]\n",
      "loss: 0.468403  [40020/60000]\n",
      "loss: 0.786095  [42020/60000]\n",
      "loss: 0.113799  [44020/60000]\n",
      "loss: 0.368211  [46020/60000]\n",
      "loss: 0.286963  [48020/60000]\n",
      "loss: 0.482893  [50020/60000]\n",
      "loss: 0.074164  [52020/60000]\n",
      "loss: 0.204953  [54020/60000]\n",
      "loss: 0.121958  [56020/60000]\n",
      "loss: 0.311372  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.331284 \n",
      "\n",
      "time: 11.491702318191528\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "loss: 0.199538  [   20/60000]\n",
      "loss: 0.325313  [ 2020/60000]\n",
      "loss: 0.102177  [ 4020/60000]\n",
      "loss: 0.279900  [ 6020/60000]\n",
      "loss: 0.548379  [ 8020/60000]\n",
      "loss: 0.127324  [10020/60000]\n",
      "loss: 0.201091  [12020/60000]\n",
      "loss: 0.317940  [14020/60000]\n",
      "loss: 0.622831  [16020/60000]\n",
      "loss: 0.508883  [18020/60000]\n",
      "loss: 0.094524  [20020/60000]\n",
      "loss: 0.339297  [22020/60000]\n",
      "loss: 0.325579  [24020/60000]\n",
      "loss: 0.156912  [26020/60000]\n",
      "loss: 0.512403  [28020/60000]\n",
      "loss: 0.086752  [30020/60000]\n",
      "loss: 0.251174  [32020/60000]\n",
      "loss: 0.319706  [34020/60000]\n",
      "loss: 0.171451  [36020/60000]\n",
      "loss: 0.320118  [38020/60000]\n",
      "loss: 0.466702  [40020/60000]\n",
      "loss: 0.786008  [42020/60000]\n",
      "loss: 0.113229  [44020/60000]\n",
      "loss: 0.367022  [46020/60000]\n",
      "loss: 0.285844  [48020/60000]\n",
      "loss: 0.481779  [50020/60000]\n",
      "loss: 0.074159  [52020/60000]\n",
      "loss: 0.203627  [54020/60000]\n",
      "loss: 0.122052  [56020/60000]\n",
      "loss: 0.310047  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.330857 \n",
      "\n",
      "time: 11.499152183532715\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "loss: 0.199752  [   20/60000]\n",
      "loss: 0.324889  [ 2020/60000]\n",
      "loss: 0.102013  [ 4020/60000]\n",
      "loss: 0.278536  [ 6020/60000]\n",
      "loss: 0.548693  [ 8020/60000]\n",
      "loss: 0.126111  [10020/60000]\n",
      "loss: 0.200238  [12020/60000]\n",
      "loss: 0.318019  [14020/60000]\n",
      "loss: 0.623602  [16020/60000]\n",
      "loss: 0.506449  [18020/60000]\n",
      "loss: 0.094102  [20020/60000]\n",
      "loss: 0.334523  [22020/60000]\n",
      "loss: 0.323078  [24020/60000]\n",
      "loss: 0.156124  [26020/60000]\n",
      "loss: 0.510559  [28020/60000]\n",
      "loss: 0.085587  [30020/60000]\n",
      "loss: 0.250437  [32020/60000]\n",
      "loss: 0.317561  [34020/60000]\n",
      "loss: 0.170718  [36020/60000]\n",
      "loss: 0.319855  [38020/60000]\n",
      "loss: 0.466068  [40020/60000]\n",
      "loss: 0.785316  [42020/60000]\n",
      "loss: 0.112188  [44020/60000]\n",
      "loss: 0.365955  [46020/60000]\n",
      "loss: 0.284776  [48020/60000]\n",
      "loss: 0.480558  [50020/60000]\n",
      "loss: 0.073666  [52020/60000]\n",
      "loss: 0.201988  [54020/60000]\n",
      "loss: 0.121773  [56020/60000]\n",
      "loss: 0.308886  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.330448 \n",
      "\n",
      "time: 11.691824674606323\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "loss: 0.198243  [   20/60000]\n",
      "loss: 0.324181  [ 2020/60000]\n",
      "loss: 0.101731  [ 4020/60000]\n",
      "loss: 0.276691  [ 6020/60000]\n",
      "loss: 0.549188  [ 8020/60000]\n",
      "loss: 0.124940  [10020/60000]\n",
      "loss: 0.199470  [12020/60000]\n",
      "loss: 0.317971  [14020/60000]\n",
      "loss: 0.622315  [16020/60000]\n",
      "loss: 0.503484  [18020/60000]\n",
      "loss: 0.093700  [20020/60000]\n",
      "loss: 0.331479  [22020/60000]\n",
      "loss: 0.319567  [24020/60000]\n",
      "loss: 0.154964  [26020/60000]\n",
      "loss: 0.509297  [28020/60000]\n",
      "loss: 0.084822  [30020/60000]\n",
      "loss: 0.249739  [32020/60000]\n",
      "loss: 0.316215  [34020/60000]\n",
      "loss: 0.169885  [36020/60000]\n",
      "loss: 0.319777  [38020/60000]\n",
      "loss: 0.465662  [40020/60000]\n",
      "loss: 0.785492  [42020/60000]\n",
      "loss: 0.111289  [44020/60000]\n",
      "loss: 0.364984  [46020/60000]\n",
      "loss: 0.283329  [48020/60000]\n",
      "loss: 0.480832  [50020/60000]\n",
      "loss: 0.073210  [52020/60000]\n",
      "loss: 0.201015  [54020/60000]\n",
      "loss: 0.121638  [56020/60000]\n",
      "loss: 0.308796  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.330137 \n",
      "\n",
      "time: 11.649952173233032\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "loss: 0.197798  [   20/60000]\n",
      "loss: 0.323679  [ 2020/60000]\n",
      "loss: 0.101635  [ 4020/60000]\n",
      "loss: 0.276057  [ 6020/60000]\n",
      "loss: 0.548820  [ 8020/60000]\n",
      "loss: 0.123884  [10020/60000]\n",
      "loss: 0.198266  [12020/60000]\n",
      "loss: 0.317746  [14020/60000]\n",
      "loss: 0.622916  [16020/60000]\n",
      "loss: 0.501136  [18020/60000]\n",
      "loss: 0.093467  [20020/60000]\n",
      "loss: 0.327057  [22020/60000]\n",
      "loss: 0.316745  [24020/60000]\n",
      "loss: 0.154256  [26020/60000]\n",
      "loss: 0.508452  [28020/60000]\n",
      "loss: 0.083858  [30020/60000]\n",
      "loss: 0.248450  [32020/60000]\n",
      "loss: 0.314703  [34020/60000]\n",
      "loss: 0.168575  [36020/60000]\n",
      "loss: 0.319259  [38020/60000]\n",
      "loss: 0.464212  [40020/60000]\n",
      "loss: 0.784715  [42020/60000]\n",
      "loss: 0.110510  [44020/60000]\n",
      "loss: 0.363679  [46020/60000]\n",
      "loss: 0.281801  [48020/60000]\n",
      "loss: 0.481317  [50020/60000]\n",
      "loss: 0.072266  [52020/60000]\n",
      "loss: 0.199593  [54020/60000]\n",
      "loss: 0.121438  [56020/60000]\n",
      "loss: 0.308002  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.329732 \n",
      "\n",
      "time: 12.753268957138062\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "loss: 0.197126  [   20/60000]\n",
      "loss: 0.322282  [ 2020/60000]\n",
      "loss: 0.101744  [ 4020/60000]\n",
      "loss: 0.274285  [ 6020/60000]\n",
      "loss: 0.547056  [ 8020/60000]\n",
      "loss: 0.122878  [10020/60000]\n",
      "loss: 0.196975  [12020/60000]\n",
      "loss: 0.318686  [14020/60000]\n",
      "loss: 0.622484  [16020/60000]\n",
      "loss: 0.499129  [18020/60000]\n",
      "loss: 0.093239  [20020/60000]\n",
      "loss: 0.323497  [22020/60000]\n",
      "loss: 0.314630  [24020/60000]\n",
      "loss: 0.153595  [26020/60000]\n",
      "loss: 0.507861  [28020/60000]\n",
      "loss: 0.082909  [30020/60000]\n",
      "loss: 0.247274  [32020/60000]\n",
      "loss: 0.313445  [34020/60000]\n",
      "loss: 0.167503  [36020/60000]\n",
      "loss: 0.319738  [38020/60000]\n",
      "loss: 0.461732  [40020/60000]\n",
      "loss: 0.784760  [42020/60000]\n",
      "loss: 0.109618  [44020/60000]\n",
      "loss: 0.361993  [46020/60000]\n",
      "loss: 0.280056  [48020/60000]\n",
      "loss: 0.479688  [50020/60000]\n",
      "loss: 0.071920  [52020/60000]\n",
      "loss: 0.198299  [54020/60000]\n",
      "loss: 0.121165  [56020/60000]\n",
      "loss: 0.306806  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.329399 \n",
      "\n",
      "time: 11.706416606903076\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "loss: 0.196456  [   20/60000]\n",
      "loss: 0.321125  [ 2020/60000]\n",
      "loss: 0.101557  [ 4020/60000]\n",
      "loss: 0.273436  [ 6020/60000]\n",
      "loss: 0.546852  [ 8020/60000]\n",
      "loss: 0.121699  [10020/60000]\n",
      "loss: 0.195635  [12020/60000]\n",
      "loss: 0.318743  [14020/60000]\n",
      "loss: 0.621824  [16020/60000]\n",
      "loss: 0.497418  [18020/60000]\n",
      "loss: 0.092952  [20020/60000]\n",
      "loss: 0.319341  [22020/60000]\n",
      "loss: 0.311666  [24020/60000]\n",
      "loss: 0.153468  [26020/60000]\n",
      "loss: 0.506656  [28020/60000]\n",
      "loss: 0.082558  [30020/60000]\n",
      "loss: 0.245709  [32020/60000]\n",
      "loss: 0.312288  [34020/60000]\n",
      "loss: 0.166701  [36020/60000]\n",
      "loss: 0.318904  [38020/60000]\n",
      "loss: 0.460445  [40020/60000]\n",
      "loss: 0.784321  [42020/60000]\n",
      "loss: 0.108782  [44020/60000]\n",
      "loss: 0.360756  [46020/60000]\n",
      "loss: 0.277688  [48020/60000]\n",
      "loss: 0.480019  [50020/60000]\n",
      "loss: 0.071431  [52020/60000]\n",
      "loss: 0.196836  [54020/60000]\n",
      "loss: 0.120821  [56020/60000]\n",
      "loss: 0.306287  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.329056 \n",
      "\n",
      "time: 11.77627682685852\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "loss: 0.195651  [   20/60000]\n",
      "loss: 0.320485  [ 2020/60000]\n",
      "loss: 0.101514  [ 4020/60000]\n",
      "loss: 0.272051  [ 6020/60000]\n",
      "loss: 0.547104  [ 8020/60000]\n",
      "loss: 0.120697  [10020/60000]\n",
      "loss: 0.194915  [12020/60000]\n",
      "loss: 0.319256  [14020/60000]\n",
      "loss: 0.622532  [16020/60000]\n",
      "loss: 0.495437  [18020/60000]\n",
      "loss: 0.092647  [20020/60000]\n",
      "loss: 0.316056  [22020/60000]\n",
      "loss: 0.309018  [24020/60000]\n",
      "loss: 0.152723  [26020/60000]\n",
      "loss: 0.505499  [28020/60000]\n",
      "loss: 0.081780  [30020/60000]\n",
      "loss: 0.243849  [32020/60000]\n",
      "loss: 0.310387  [34020/60000]\n",
      "loss: 0.165910  [36020/60000]\n",
      "loss: 0.317896  [38020/60000]\n",
      "loss: 0.458706  [40020/60000]\n",
      "loss: 0.783845  [42020/60000]\n",
      "loss: 0.108032  [44020/60000]\n",
      "loss: 0.358833  [46020/60000]\n",
      "loss: 0.276613  [48020/60000]\n",
      "loss: 0.478616  [50020/60000]\n",
      "loss: 0.071344  [52020/60000]\n",
      "loss: 0.195420  [54020/60000]\n",
      "loss: 0.120704  [56020/60000]\n",
      "loss: 0.304372  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.328770 \n",
      "\n",
      "time: 11.678745746612549\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "loss: 0.196061  [   20/60000]\n",
      "loss: 0.319655  [ 2020/60000]\n",
      "loss: 0.101100  [ 4020/60000]\n",
      "loss: 0.270795  [ 6020/60000]\n",
      "loss: 0.546433  [ 8020/60000]\n",
      "loss: 0.119552  [10020/60000]\n",
      "loss: 0.193647  [12020/60000]\n",
      "loss: 0.319471  [14020/60000]\n",
      "loss: 0.622251  [16020/60000]\n",
      "loss: 0.492350  [18020/60000]\n",
      "loss: 0.092249  [20020/60000]\n",
      "loss: 0.312231  [22020/60000]\n",
      "loss: 0.305974  [24020/60000]\n",
      "loss: 0.152321  [26020/60000]\n",
      "loss: 0.504534  [28020/60000]\n",
      "loss: 0.081024  [30020/60000]\n",
      "loss: 0.243027  [32020/60000]\n",
      "loss: 0.309150  [34020/60000]\n",
      "loss: 0.164204  [36020/60000]\n",
      "loss: 0.317673  [38020/60000]\n",
      "loss: 0.456786  [40020/60000]\n",
      "loss: 0.783069  [42020/60000]\n",
      "loss: 0.107169  [44020/60000]\n",
      "loss: 0.356851  [46020/60000]\n",
      "loss: 0.276120  [48020/60000]\n",
      "loss: 0.477802  [50020/60000]\n",
      "loss: 0.070522  [52020/60000]\n",
      "loss: 0.194498  [54020/60000]\n",
      "loss: 0.120546  [56020/60000]\n",
      "loss: 0.304753  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.328493 \n",
      "\n",
      "time: 11.753333330154419\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "loss: 0.195190  [   20/60000]\n",
      "loss: 0.318328  [ 2020/60000]\n",
      "loss: 0.100894  [ 4020/60000]\n",
      "loss: 0.269541  [ 6020/60000]\n",
      "loss: 0.545984  [ 8020/60000]\n",
      "loss: 0.118744  [10020/60000]\n",
      "loss: 0.192515  [12020/60000]\n",
      "loss: 0.318886  [14020/60000]\n",
      "loss: 0.622100  [16020/60000]\n",
      "loss: 0.489995  [18020/60000]\n",
      "loss: 0.092130  [20020/60000]\n",
      "loss: 0.307792  [22020/60000]\n",
      "loss: 0.303388  [24020/60000]\n",
      "loss: 0.151270  [26020/60000]\n",
      "loss: 0.503357  [28020/60000]\n",
      "loss: 0.080765  [30020/60000]\n",
      "loss: 0.242238  [32020/60000]\n",
      "loss: 0.307446  [34020/60000]\n",
      "loss: 0.163271  [36020/60000]\n",
      "loss: 0.317350  [38020/60000]\n",
      "loss: 0.455333  [40020/60000]\n",
      "loss: 0.782466  [42020/60000]\n",
      "loss: 0.106124  [44020/60000]\n",
      "loss: 0.355590  [46020/60000]\n",
      "loss: 0.274893  [48020/60000]\n",
      "loss: 0.476884  [50020/60000]\n",
      "loss: 0.070061  [52020/60000]\n",
      "loss: 0.192923  [54020/60000]\n",
      "loss: 0.120556  [56020/60000]\n",
      "loss: 0.303738  [58020/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.328191 \n",
      "\n",
      "time: 11.742470502853394\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_dataloader, test_dataloader = data_loader(batch_size)\n",
    "corrects_20 = []\n",
    "test_losses_20 = []\n",
    "times_20 = []\n",
    "for t in range(epoches):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    start = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    end = time.time()\n",
    "    correct, test_loss = test(test_dataloader, model, loss_fn)\n",
    "    corrects_20.append(correct)\n",
    "    test_losses_20.append(test_loss)\n",
    "    times_20.append(end - start)\n",
    "    print(f\"time: {end - start}\\n\")\n",
    "    pass\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoguwBG5M-Yv"
   },
   "source": [
    "### batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUM84l47M8L5",
    "outputId": "317ba15f-9f59-4a1b-e16b-c42ca3844a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 0.150453  [  100/60000]\n",
      "loss: 0.185180  [10100/60000]\n",
      "loss: 0.214507  [20100/60000]\n",
      "loss: 0.229687  [30100/60000]\n",
      "loss: 0.394429  [40100/60000]\n",
      "loss: 0.300083  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324944 \n",
      "\n",
      "time: 8.053825616836548\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 0.149372  [  100/60000]\n",
      "loss: 0.183379  [10100/60000]\n",
      "loss: 0.212708  [20100/60000]\n",
      "loss: 0.230168  [30100/60000]\n",
      "loss: 0.394031  [40100/60000]\n",
      "loss: 0.299465  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324921 \n",
      "\n",
      "time: 7.821741104125977\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 0.149071  [  100/60000]\n",
      "loss: 0.182888  [10100/60000]\n",
      "loss: 0.212013  [20100/60000]\n",
      "loss: 0.230187  [30100/60000]\n",
      "loss: 0.393752  [40100/60000]\n",
      "loss: 0.298862  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324884 \n",
      "\n",
      "time: 7.439795255661011\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 0.149008  [  100/60000]\n",
      "loss: 0.182638  [10100/60000]\n",
      "loss: 0.211687  [20100/60000]\n",
      "loss: 0.230146  [30100/60000]\n",
      "loss: 0.393611  [40100/60000]\n",
      "loss: 0.298481  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324829 \n",
      "\n",
      "time: 7.073737621307373\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 0.148937  [  100/60000]\n",
      "loss: 0.182443  [10100/60000]\n",
      "loss: 0.211503  [20100/60000]\n",
      "loss: 0.229887  [30100/60000]\n",
      "loss: 0.393576  [40100/60000]\n",
      "loss: 0.298222  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324768 \n",
      "\n",
      "time: 7.8383684158325195\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 0.148813  [  100/60000]\n",
      "loss: 0.182159  [10100/60000]\n",
      "loss: 0.211333  [20100/60000]\n",
      "loss: 0.229685  [30100/60000]\n",
      "loss: 0.393311  [40100/60000]\n",
      "loss: 0.297893  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324706 \n",
      "\n",
      "time: 7.987921714782715\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 0.148700  [  100/60000]\n",
      "loss: 0.181933  [10100/60000]\n",
      "loss: 0.211215  [20100/60000]\n",
      "loss: 0.229516  [30100/60000]\n",
      "loss: 0.393036  [40100/60000]\n",
      "loss: 0.297598  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324640 \n",
      "\n",
      "time: 7.131230354309082\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 0.148540  [  100/60000]\n",
      "loss: 0.181606  [10100/60000]\n",
      "loss: 0.211105  [20100/60000]\n",
      "loss: 0.229328  [30100/60000]\n",
      "loss: 0.392708  [40100/60000]\n",
      "loss: 0.297270  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324571 \n",
      "\n",
      "time: 7.642822027206421\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 0.148389  [  100/60000]\n",
      "loss: 0.181411  [10100/60000]\n",
      "loss: 0.210926  [20100/60000]\n",
      "loss: 0.229070  [30100/60000]\n",
      "loss: 0.392429  [40100/60000]\n",
      "loss: 0.297063  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324493 \n",
      "\n",
      "time: 7.785693407058716\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 0.148260  [  100/60000]\n",
      "loss: 0.181110  [10100/60000]\n",
      "loss: 0.210798  [20100/60000]\n",
      "loss: 0.228819  [30100/60000]\n",
      "loss: 0.392137  [40100/60000]\n",
      "loss: 0.296880  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324424 \n",
      "\n",
      "time: 7.9559006690979\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "loss: 0.148194  [  100/60000]\n",
      "loss: 0.180873  [10100/60000]\n",
      "loss: 0.210727  [20100/60000]\n",
      "loss: 0.228579  [30100/60000]\n",
      "loss: 0.391899  [40100/60000]\n",
      "loss: 0.296630  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324343 \n",
      "\n",
      "time: 7.833494424819946\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "loss: 0.148079  [  100/60000]\n",
      "loss: 0.180627  [10100/60000]\n",
      "loss: 0.210589  [20100/60000]\n",
      "loss: 0.228351  [30100/60000]\n",
      "loss: 0.391562  [40100/60000]\n",
      "loss: 0.296572  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324266 \n",
      "\n",
      "time: 7.8856892585754395\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "loss: 0.147930  [  100/60000]\n",
      "loss: 0.180347  [10100/60000]\n",
      "loss: 0.210476  [20100/60000]\n",
      "loss: 0.228168  [30100/60000]\n",
      "loss: 0.391271  [40100/60000]\n",
      "loss: 0.296341  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324198 \n",
      "\n",
      "time: 8.05919075012207\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "loss: 0.147770  [  100/60000]\n",
      "loss: 0.180053  [10100/60000]\n",
      "loss: 0.210365  [20100/60000]\n",
      "loss: 0.227910  [30100/60000]\n",
      "loss: 0.391015  [40100/60000]\n",
      "loss: 0.296057  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324120 \n",
      "\n",
      "time: 7.904121160507202\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "loss: 0.147658  [  100/60000]\n",
      "loss: 0.179897  [10100/60000]\n",
      "loss: 0.210211  [20100/60000]\n",
      "loss: 0.227656  [30100/60000]\n",
      "loss: 0.390810  [40100/60000]\n",
      "loss: 0.295815  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.324048 \n",
      "\n",
      "time: 7.240717172622681\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "loss: 0.147517  [  100/60000]\n",
      "loss: 0.179598  [10100/60000]\n",
      "loss: 0.210091  [20100/60000]\n",
      "loss: 0.227359  [30100/60000]\n",
      "loss: 0.390509  [40100/60000]\n",
      "loss: 0.295557  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.323990 \n",
      "\n",
      "time: 7.8465776443481445\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "loss: 0.147423  [  100/60000]\n",
      "loss: 0.179227  [10100/60000]\n",
      "loss: 0.209968  [20100/60000]\n",
      "loss: 0.227128  [30100/60000]\n",
      "loss: 0.390251  [40100/60000]\n",
      "loss: 0.295305  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.323922 \n",
      "\n",
      "time: 7.901092052459717\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "loss: 0.147287  [  100/60000]\n",
      "loss: 0.179010  [10100/60000]\n",
      "loss: 0.209898  [20100/60000]\n",
      "loss: 0.226892  [30100/60000]\n",
      "loss: 0.389847  [40100/60000]\n",
      "loss: 0.295056  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.323855 \n",
      "\n",
      "time: 7.571467638015747\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "loss: 0.147147  [  100/60000]\n",
      "loss: 0.178705  [10100/60000]\n",
      "loss: 0.209746  [20100/60000]\n",
      "loss: 0.226545  [30100/60000]\n",
      "loss: 0.389567  [40100/60000]\n",
      "loss: 0.294815  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.323791 \n",
      "\n",
      "time: 7.341798305511475\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "loss: 0.147019  [  100/60000]\n",
      "loss: 0.178481  [10100/60000]\n",
      "loss: 0.209675  [20100/60000]\n",
      "loss: 0.226377  [30100/60000]\n",
      "loss: 0.389318  [40100/60000]\n",
      "loss: 0.294596  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323717 \n",
      "\n",
      "time: 7.98338770866394\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "loss: 0.146867  [  100/60000]\n",
      "loss: 0.178200  [10100/60000]\n",
      "loss: 0.209600  [20100/60000]\n",
      "loss: 0.226065  [30100/60000]\n",
      "loss: 0.389057  [40100/60000]\n",
      "loss: 0.294277  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323646 \n",
      "\n",
      "time: 7.804510116577148\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "loss: 0.146713  [  100/60000]\n",
      "loss: 0.177937  [10100/60000]\n",
      "loss: 0.209497  [20100/60000]\n",
      "loss: 0.225779  [30100/60000]\n",
      "loss: 0.388638  [40100/60000]\n",
      "loss: 0.294158  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323572 \n",
      "\n",
      "time: 7.075153589248657\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "loss: 0.146576  [  100/60000]\n",
      "loss: 0.177676  [10100/60000]\n",
      "loss: 0.209458  [20100/60000]\n",
      "loss: 0.225556  [30100/60000]\n",
      "loss: 0.388426  [40100/60000]\n",
      "loss: 0.293863  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323506 \n",
      "\n",
      "time: 7.5231781005859375\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "loss: 0.146434  [  100/60000]\n",
      "loss: 0.177426  [10100/60000]\n",
      "loss: 0.209396  [20100/60000]\n",
      "loss: 0.225328  [30100/60000]\n",
      "loss: 0.388238  [40100/60000]\n",
      "loss: 0.293618  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323447 \n",
      "\n",
      "time: 7.849211692810059\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "loss: 0.146285  [  100/60000]\n",
      "loss: 0.177164  [10100/60000]\n",
      "loss: 0.209239  [20100/60000]\n",
      "loss: 0.225058  [30100/60000]\n",
      "loss: 0.387887  [40100/60000]\n",
      "loss: 0.293384  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323377 \n",
      "\n",
      "time: 7.897567987442017\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "loss: 0.146100  [  100/60000]\n",
      "loss: 0.176877  [10100/60000]\n",
      "loss: 0.209122  [20100/60000]\n",
      "loss: 0.224763  [30100/60000]\n",
      "loss: 0.387580  [40100/60000]\n",
      "loss: 0.293143  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323313 \n",
      "\n",
      "time: 7.26100492477417\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "loss: 0.145985  [  100/60000]\n",
      "loss: 0.176666  [10100/60000]\n",
      "loss: 0.209028  [20100/60000]\n",
      "loss: 0.224467  [30100/60000]\n",
      "loss: 0.387371  [40100/60000]\n",
      "loss: 0.292963  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323250 \n",
      "\n",
      "time: 7.8606343269348145\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "loss: 0.145816  [  100/60000]\n",
      "loss: 0.176437  [10100/60000]\n",
      "loss: 0.208930  [20100/60000]\n",
      "loss: 0.224248  [30100/60000]\n",
      "loss: 0.387129  [40100/60000]\n",
      "loss: 0.292713  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323196 \n",
      "\n",
      "time: 7.851483345031738\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "loss: 0.145671  [  100/60000]\n",
      "loss: 0.176152  [10100/60000]\n",
      "loss: 0.208833  [20100/60000]\n",
      "loss: 0.224012  [30100/60000]\n",
      "loss: 0.386887  [40100/60000]\n",
      "loss: 0.292468  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323125 \n",
      "\n",
      "time: 7.475541591644287\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "loss: 0.145499  [  100/60000]\n",
      "loss: 0.175938  [10100/60000]\n",
      "loss: 0.208599  [20100/60000]\n",
      "loss: 0.223730  [30100/60000]\n",
      "loss: 0.386653  [40100/60000]\n",
      "loss: 0.292305  [50100/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323068 \n",
      "\n",
      "time: 7.843058824539185\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataloader, test_dataloader = data_loader(batch_size)\n",
    "corrects_100 = []\n",
    "test_losses_100 = []\n",
    "times_100 = []\n",
    "for t in range(epoches):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    start = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    end = time.time()\n",
    "    correct, test_loss = test(test_dataloader, model, loss_fn)\n",
    "    corrects_100.append(correct)\n",
    "    test_losses_100.append(test_loss)\n",
    "    times_100.append(end - start)\n",
    "    print(f\"time: {end - start}\\n\")\n",
    "    pass\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5MXNGZwN4ys"
   },
   "source": [
    "### batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBJewJ-ON0-c",
    "outputId": "fad9a444-0c89-4d4c-ebcc-117c426bb915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 0.228430  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.322832 \n",
      "\n",
      "time: 7.087778568267822\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 0.228014  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322716 \n",
      "\n",
      "time: 7.126408100128174\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 0.227758  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322651 \n",
      "\n",
      "time: 6.542964696884155\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 0.227580  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322609 \n",
      "\n",
      "time: 7.1930670738220215\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 0.227447  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322581 \n",
      "\n",
      "time: 7.030933141708374\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 0.227344  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322560 \n",
      "\n",
      "time: 6.413153648376465\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 0.227261  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322545 \n",
      "\n",
      "time: 7.011656761169434\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 0.227192  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322533 \n",
      "\n",
      "time: 7.135955095291138\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 0.227135  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322524 \n",
      "\n",
      "time: 6.338221549987793\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 0.227086  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322517 \n",
      "\n",
      "time: 7.125792026519775\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "loss: 0.227045  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322512 \n",
      "\n",
      "time: 7.248779773712158\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "loss: 0.227010  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322508 \n",
      "\n",
      "time: 6.544186592102051\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "loss: 0.226980  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322505 \n",
      "\n",
      "time: 7.189849376678467\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "loss: 0.226954  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322503 \n",
      "\n",
      "time: 7.196130990982056\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "loss: 0.226931  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322501 \n",
      "\n",
      "time: 6.507888555526733\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "loss: 0.226910  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322500 \n",
      "\n",
      "time: 7.476763010025024\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "loss: 0.226892  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322500 \n",
      "\n",
      "time: 7.46060037612915\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "loss: 0.226876  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322499 \n",
      "\n",
      "time: 6.801225662231445\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "loss: 0.226862  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322499 \n",
      "\n",
      "time: 7.218719959259033\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "loss: 0.226849  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322499 \n",
      "\n",
      "time: 8.217201471328735\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "loss: 0.226837  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322500 \n",
      "\n",
      "time: 7.195520401000977\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "loss: 0.226826  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322500 \n",
      "\n",
      "time: 6.6802825927734375\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "loss: 0.226816  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322500 \n",
      "\n",
      "time: 7.541878938674927\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "loss: 0.226807  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322501 \n",
      "\n",
      "time: 7.413151264190674\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "loss: 0.226798  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322502 \n",
      "\n",
      "time: 6.95745587348938\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "loss: 0.226791  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322503 \n",
      "\n",
      "time: 7.523040056228638\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "loss: 0.226783  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322503 \n",
      "\n",
      "time: 7.557542562484741\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "loss: 0.226776  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322504 \n",
      "\n",
      "time: 6.808806657791138\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "loss: 0.226769  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322505 \n",
      "\n",
      "time: 7.1269145011901855\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "loss: 0.226763  [10000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.322506 \n",
      "\n",
      "time: 7.320349931716919\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "train_dataloader, test_dataloader = data_loader(batch_size)\n",
    "corrects_10000 = []\n",
    "test_losses_10000 = []\n",
    "times_10000 = []\n",
    "for t in range(epoches):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    start = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    end = time.time()\n",
    "    correct, test_loss = test(test_dataloader, model, loss_fn)\n",
    "    corrects_10000.append(correct)\n",
    "    test_losses_10000.append(test_loss)\n",
    "    times_10000.append(end - start)\n",
    "    print(f\"time: {end - start}\\n\")\n",
    "    pass\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-Qw6oZCl8w8"
   },
   "source": [
    "准确率结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ihLh-RV8lmR-",
    "outputId": "332f8b45-a0e3-41a1-95d0-ce714cb2f4fe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_acc, ax_acc = plt.subplots()\n",
    "ax_acc.plot(range(1, epoches+1), corrects_5, c='red')\n",
    "ax_acc.plot(range(1, epoches+1), corrects_20, c='green')\n",
    "ax_acc.plot(range(1, epoches+1), corrects_100, c='orange')\n",
    "ax_acc.plot(range(1, epoches+1), corrects_10000, c='blue')\n",
    "fig_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhBiDvyKtbzr"
   },
   "source": [
    "loss 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "X_fX7MsqsxLk",
    "outputId": "33b66fe9-14d6-410e-a9ae-0a0f4e30101b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_loss, ax_loss = plt.subplots()\n",
    "ax_loss.plot(range(1, epoches+1), test_losses_5, c='red')\n",
    "ax_loss.plot(range(1, epoches+1), test_losses_20, c='green')\n",
    "ax_loss.plot(range(1, epoches+1), test_losses_100, c='orange')\n",
    "ax_loss.plot(range(1, epoches+1), test_losses_10000, c='blue')\n",
    "fig_loss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxSLc_NsteMa"
   },
   "source": [
    "每个epoch用时结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "_wB8PtvdtETG",
    "outputId": "e36c2618-1c3c-4d17-fc9b-613cf4228fe0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_time, ax_time = plt.subplots()\n",
    "ax_time.plot(range(1, epoches+1), times_5, c='red')\n",
    "ax_time.plot(range(1, epoches+1), times_20, c='green')\n",
    "ax_time.plot(range(1, epoches+1), times_100, c='orange')\n",
    "ax_time.plot(range(1, epoches+1), times_10000, c='blue')\n",
    "fig_time.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
